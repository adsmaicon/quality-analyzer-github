[{"url":"https://api.github.com/repos/scrapy/scrapy/issues/5461","repository_url":"https://api.github.com/repos/scrapy/scrapy","labels_url":"https://api.github.com/repos/scrapy/scrapy/issues/5461/labels{/name}","comments_url":"https://api.github.com/repos/scrapy/scrapy/issues/5461/comments","events_url":"https://api.github.com/repos/scrapy/scrapy/issues/5461/events","html_url":"https://github.com/scrapy/scrapy/pull/5461","id":1182164340,"node_id":"PR_kwDOAAgUXs41FzNq","number":5461,"title":"documentation update for multiple spiders","user":{"login":"yash-fn","id":76577754,"node_id":"MDQ6VXNlcjc2NTc3NzU0","avatar_url":"https://avatars.githubusercontent.com/u/76577754?v=4","gravatar_id":"","url":"https://api.github.com/users/yash-fn","html_url":"https://github.com/yash-fn","followers_url":"https://api.github.com/users/yash-fn/followers","following_url":"https://api.github.com/users/yash-fn/following{/other_user}","gists_url":"https://api.github.com/users/yash-fn/gists{/gist_id}","starred_url":"https://api.github.com/users/yash-fn/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/yash-fn/subscriptions","organizations_url":"https://api.github.com/users/yash-fn/orgs","repos_url":"https://api.github.com/users/yash-fn/repos","events_url":"https://api.github.com/users/yash-fn/events{/privacy}","received_events_url":"https://api.github.com/users/yash-fn/received_events","type":"User","site_admin":false},"labels":[],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":1,"created_at":"2022-03-26T23:17:16Z","updated_at":"2022-03-27T11:40:37Z","closed_at":null,"author_association":"NONE","active_lock_reason":null,"draft":false,"pull_request":{"url":"https://api.github.com/repos/scrapy/scrapy/pulls/5461","html_url":"https://github.com/scrapy/scrapy/pull/5461","diff_url":"https://github.com/scrapy/scrapy/pull/5461.diff","patch_url":"https://github.com/scrapy/scrapy/pull/5461.patch","merged_at":null},"body":"i noticed passing settings to configure logging function made weird output go away. checked documentation and it says first parameter is settings file. Is this correct?","reactions":{"url":"https://api.github.com/repos/scrapy/scrapy/issues/5461/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/scrapy/scrapy/issues/5461/timeline","performed_via_github_app":null},{"url":"https://api.github.com/repos/scrapy/scrapy/issues/5459","repository_url":"https://api.github.com/repos/scrapy/scrapy","labels_url":"https://api.github.com/repos/scrapy/scrapy/issues/5459/labels{/name}","comments_url":"https://api.github.com/repos/scrapy/scrapy/issues/5459/comments","events_url":"https://api.github.com/repos/scrapy/scrapy/issues/5459/events","html_url":"https://github.com/scrapy/scrapy/pull/5459","id":1180523826,"node_id":"PR_kwDOAAgUXs41AO7k","number":5459,"title":"Pin mitmproxy to < 8 for now.","user":{"login":"wRAR","id":241039,"node_id":"MDQ6VXNlcjI0MTAzOQ==","avatar_url":"https://avatars.githubusercontent.com/u/241039?v=4","gravatar_id":"","url":"https://api.github.com/users/wRAR","html_url":"https://github.com/wRAR","followers_url":"https://api.github.com/users/wRAR/followers","following_url":"https://api.github.com/users/wRAR/following{/other_user}","gists_url":"https://api.github.com/users/wRAR/gists{/gist_id}","starred_url":"https://api.github.com/users/wRAR/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/wRAR/subscriptions","organizations_url":"https://api.github.com/users/wRAR/orgs","repos_url":"https://api.github.com/users/wRAR/repos","events_url":"https://api.github.com/users/wRAR/events{/privacy}","received_events_url":"https://api.github.com/users/wRAR/received_events","type":"User","site_admin":false},"labels":[{"id":13907246,"node_id":"MDU6TGFiZWwxMzkwNzI0Ng==","url":"https://api.github.com/repos/scrapy/scrapy/labels/bug","name":"bug","color":"fc2929","default":true,"description":null},{"id":443053983,"node_id":"MDU6TGFiZWw0NDMwNTM5ODM=","url":"https://api.github.com/repos/scrapy/scrapy/labels/CI","name":"CI","color":"d4c5f9","default":false,"description":null}],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":3,"created_at":"2022-03-25T09:05:42Z","updated_at":"2022-03-26T06:47:35Z","closed_at":null,"author_association":"CONTRIBUTOR","active_lock_reason":null,"draft":false,"pull_request":{"url":"https://api.github.com/repos/scrapy/scrapy/pulls/5459","html_url":"https://github.com/scrapy/scrapy/pull/5459","diff_url":"https://github.com/scrapy/scrapy/pull/5459.diff","patch_url":"https://github.com/scrapy/scrapy/pull/5459.patch","merged_at":null},"body":"Related to #5454 \r\n\r\nShould this go into 2.6.2?","reactions":{"url":"https://api.github.com/repos/scrapy/scrapy/issues/5459/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/scrapy/scrapy/issues/5459/timeline","performed_via_github_app":null},{"url":"https://api.github.com/repos/scrapy/scrapy/issues/5458","repository_url":"https://api.github.com/repos/scrapy/scrapy","labels_url":"https://api.github.com/repos/scrapy/scrapy/issues/5458/labels{/name}","comments_url":"https://api.github.com/repos/scrapy/scrapy/issues/5458/comments","events_url":"https://api.github.com/repos/scrapy/scrapy/issues/5458/events","html_url":"https://github.com/scrapy/scrapy/pull/5458","id":1178376102,"node_id":"PR_kwDOAAgUXs405TzJ","number":5458,"title":"fix: return unique_list only when link_extractor.unique is True","user":{"login":"reidemeister94","id":28828348,"node_id":"MDQ6VXNlcjI4ODI4MzQ4","avatar_url":"https://avatars.githubusercontent.com/u/28828348?v=4","gravatar_id":"","url":"https://api.github.com/users/reidemeister94","html_url":"https://github.com/reidemeister94","followers_url":"https://api.github.com/users/reidemeister94/followers","following_url":"https://api.github.com/users/reidemeister94/following{/other_user}","gists_url":"https://api.github.com/users/reidemeister94/gists{/gist_id}","starred_url":"https://api.github.com/users/reidemeister94/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/reidemeister94/subscriptions","organizations_url":"https://api.github.com/users/reidemeister94/orgs","repos_url":"https://api.github.com/users/reidemeister94/repos","events_url":"https://api.github.com/users/reidemeister94/events{/privacy}","received_events_url":"https://api.github.com/users/reidemeister94/received_events","type":"User","site_admin":false},"labels":[],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":2,"created_at":"2022-03-23T16:39:17Z","updated_at":"2022-03-24T12:57:59Z","closed_at":null,"author_association":"NONE","active_lock_reason":null,"draft":false,"pull_request":{"url":"https://api.github.com/repos/scrapy/scrapy/pulls/5458","html_url":"https://github.com/scrapy/scrapy/pull/5458","diff_url":"https://github.com/scrapy/scrapy/pull/5458.diff","patch_url":"https://github.com/scrapy/scrapy/pull/5458.patch","merged_at":null},"body":"In the `lxmlhtml.py` script, the function `extract_links(self, response)` always returns `unique_list(all_links)`, even when the attribute `unique` of the `link_extractor` is equal to `False`. \r\n\r\nThe behavior is coherent with the `unique` attribute with this little change in the return line.","reactions":{"url":"https://api.github.com/repos/scrapy/scrapy/issues/5458/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/scrapy/scrapy/issues/5458/timeline","performed_via_github_app":null},{"url":"https://api.github.com/repos/scrapy/scrapy/issues/5454","repository_url":"https://api.github.com/repos/scrapy/scrapy","labels_url":"https://api.github.com/repos/scrapy/scrapy/issues/5454/labels{/name}","comments_url":"https://api.github.com/repos/scrapy/scrapy/issues/5454/comments","events_url":"https://api.github.com/repos/scrapy/scrapy/issues/5454/events","html_url":"https://github.com/scrapy/scrapy/issues/5454","id":1174934206,"node_id":"I_kwDOAAgUXs5GCBK-","number":5454,"title":"test_proxy_connect.py hangs with mitmproxy 8.0.0 (affecting 3.8 tests)","user":{"login":"wRAR","id":241039,"node_id":"MDQ6VXNlcjI0MTAzOQ==","avatar_url":"https://avatars.githubusercontent.com/u/241039?v=4","gravatar_id":"","url":"https://api.github.com/users/wRAR","html_url":"https://github.com/wRAR","followers_url":"https://api.github.com/users/wRAR/followers","following_url":"https://api.github.com/users/wRAR/following{/other_user}","gists_url":"https://api.github.com/users/wRAR/gists{/gist_id}","starred_url":"https://api.github.com/users/wRAR/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/wRAR/subscriptions","organizations_url":"https://api.github.com/users/wRAR/orgs","repos_url":"https://api.github.com/users/wRAR/repos","events_url":"https://api.github.com/users/wRAR/events{/privacy}","received_events_url":"https://api.github.com/users/wRAR/received_events","type":"User","site_admin":false},"labels":[{"id":13907246,"node_id":"MDU6TGFiZWwxMzkwNzI0Ng==","url":"https://api.github.com/repos/scrapy/scrapy/labels/bug","name":"bug","color":"fc2929","default":true,"description":null},{"id":443053983,"node_id":"MDU6TGFiZWw0NDMwNTM5ODM=","url":"https://api.github.com/repos/scrapy/scrapy/labels/CI","name":"CI","color":"d4c5f9","default":false,"description":null}],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":2,"created_at":"2022-03-21T06:16:36Z","updated_at":"2022-03-26T07:55:44Z","closed_at":null,"author_association":"CONTRIBUTOR","active_lock_reason":null,"body":"mitmproxy 8.0.0 was released 2 days ago, it's likely to be the cause of this.\r\n\r\nAlso it looks like we currently don't install mitmproxy at all for 3.9+.","reactions":{"url":"https://api.github.com/repos/scrapy/scrapy/issues/5454/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/scrapy/scrapy/issues/5454/timeline","performed_via_github_app":null},{"url":"https://api.github.com/repos/scrapy/scrapy/issues/5452","repository_url":"https://api.github.com/repos/scrapy/scrapy","labels_url":"https://api.github.com/repos/scrapy/scrapy/issues/5452/labels{/name}","comments_url":"https://api.github.com/repos/scrapy/scrapy/issues/5452/comments","events_url":"https://api.github.com/repos/scrapy/scrapy/issues/5452/events","html_url":"https://github.com/scrapy/scrapy/pull/5452","id":1174246801,"node_id":"PR_kwDOAAgUXs40sDxQ","number":5452,"title":"removed unnecessary parameters in _DictProxy.__iter__ following SonarCloud recommendations","user":{"login":"DrownedTales","id":79609963,"node_id":"MDQ6VXNlcjc5NjA5OTYz","avatar_url":"https://avatars.githubusercontent.com/u/79609963?v=4","gravatar_id":"","url":"https://api.github.com/users/DrownedTales","html_url":"https://github.com/DrownedTales","followers_url":"https://api.github.com/users/DrownedTales/followers","following_url":"https://api.github.com/users/DrownedTales/following{/other_user}","gists_url":"https://api.github.com/users/DrownedTales/gists{/gist_id}","starred_url":"https://api.github.com/users/DrownedTales/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/DrownedTales/subscriptions","organizations_url":"https://api.github.com/users/DrownedTales/orgs","repos_url":"https://api.github.com/users/DrownedTales/repos","events_url":"https://api.github.com/users/DrownedTales/events{/privacy}","received_events_url":"https://api.github.com/users/DrownedTales/received_events","type":"User","site_admin":false},"labels":[],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":3,"created_at":"2022-03-19T11:46:09Z","updated_at":"2022-03-20T14:39:34Z","closed_at":null,"author_association":"NONE","active_lock_reason":null,"draft":false,"pull_request":{"url":"https://api.github.com/repos/scrapy/scrapy/pulls/5452","html_url":"https://github.com/scrapy/scrapy/pull/5452","diff_url":"https://github.com/scrapy/scrapy/pull/5452.diff","patch_url":"https://github.com/scrapy/scrapy/pull/5452.patch","merged_at":null},"body":"I've noticed that the method _iter_(self, k, v) in _DictProxy (Scrapy/Settings/_init_.py) had three arguments. This method is called by python with only the self parameter so I think this was unnecesary. Correct me if I'm wrong.","reactions":{"url":"https://api.github.com/repos/scrapy/scrapy/issues/5452/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/scrapy/scrapy/issues/5452/timeline","performed_via_github_app":null},{"url":"https://api.github.com/repos/scrapy/scrapy/issues/5451","repository_url":"https://api.github.com/repos/scrapy/scrapy","labels_url":"https://api.github.com/repos/scrapy/scrapy/issues/5451/labels{/name}","comments_url":"https://api.github.com/repos/scrapy/scrapy/issues/5451/comments","events_url":"https://api.github.com/repos/scrapy/scrapy/issues/5451/events","html_url":"https://github.com/scrapy/scrapy/pull/5451","id":1172808804,"node_id":"PR_kwDOAAgUXs40nmvX","number":5451,"title":"HTML Conventions","user":{"login":"FJMonteroInformatica","id":49473282,"node_id":"MDQ6VXNlcjQ5NDczMjgy","avatar_url":"https://avatars.githubusercontent.com/u/49473282?v=4","gravatar_id":"","url":"https://api.github.com/users/FJMonteroInformatica","html_url":"https://github.com/FJMonteroInformatica","followers_url":"https://api.github.com/users/FJMonteroInformatica/followers","following_url":"https://api.github.com/users/FJMonteroInformatica/following{/other_user}","gists_url":"https://api.github.com/users/FJMonteroInformatica/gists{/gist_id}","starred_url":"https://api.github.com/users/FJMonteroInformatica/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/FJMonteroInformatica/subscriptions","organizations_url":"https://api.github.com/users/FJMonteroInformatica/orgs","repos_url":"https://api.github.com/users/FJMonteroInformatica/repos","events_url":"https://api.github.com/users/FJMonteroInformatica/events{/privacy}","received_events_url":"https://api.github.com/users/FJMonteroInformatica/received_events","type":"User","site_admin":false},"labels":[],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":4,"created_at":"2022-03-17T19:15:56Z","updated_at":"2022-03-17T21:25:32Z","closed_at":null,"author_association":"NONE","active_lock_reason":null,"draft":false,"pull_request":{"url":"https://api.github.com/repos/scrapy/scrapy/pulls/5451","html_url":"https://github.com/scrapy/scrapy/pull/5451","diff_url":"https://github.com/scrapy/scrapy/pull/5451.diff","patch_url":"https://github.com/scrapy/scrapy/pull/5451.patch","merged_at":null},"body":"I have updated the HTML files following SonarCloud's recommendations.","reactions":{"url":"https://api.github.com/repos/scrapy/scrapy/issues/5451/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/scrapy/scrapy/issues/5451/timeline","performed_via_github_app":null},{"url":"https://api.github.com/repos/scrapy/scrapy/issues/5450","repository_url":"https://api.github.com/repos/scrapy/scrapy","labels_url":"https://api.github.com/repos/scrapy/scrapy/issues/5450/labels{/name}","comments_url":"https://api.github.com/repos/scrapy/scrapy/issues/5450/comments","events_url":"https://api.github.com/repos/scrapy/scrapy/issues/5450/events","html_url":"https://github.com/scrapy/scrapy/pull/5450","id":1171103442,"node_id":"PR_kwDOAAgUXs40iYDJ","number":5450,"title":"fixed detection of extensions like \".tar.gz\" in URL","user":{"login":"kinoute","id":623763,"node_id":"MDQ6VXNlcjYyMzc2Mw==","avatar_url":"https://avatars.githubusercontent.com/u/623763?v=4","gravatar_id":"","url":"https://api.github.com/users/kinoute","html_url":"https://github.com/kinoute","followers_url":"https://api.github.com/users/kinoute/followers","following_url":"https://api.github.com/users/kinoute/following{/other_user}","gists_url":"https://api.github.com/users/kinoute/gists{/gist_id}","starred_url":"https://api.github.com/users/kinoute/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/kinoute/subscriptions","organizations_url":"https://api.github.com/users/kinoute/orgs","repos_url":"https://api.github.com/users/kinoute/repos","events_url":"https://api.github.com/users/kinoute/events{/privacy}","received_events_url":"https://api.github.com/users/kinoute/received_events","type":"User","site_admin":false},"labels":[],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":6,"created_at":"2022-03-16T14:29:15Z","updated_at":"2022-03-17T10:38:00Z","closed_at":null,"author_association":"NONE","active_lock_reason":null,"draft":false,"pull_request":{"url":"https://api.github.com/repos/scrapy/scrapy/pulls/5450","html_url":"https://github.com/scrapy/scrapy/pull/5450","diff_url":"https://github.com/scrapy/scrapy/pull/5450.diff","patch_url":"https://github.com/scrapy/scrapy/pull/5450.patch","merged_at":null},"body":"#4066 added more extensions to ignore by default such as `.rar` or `.zip`, which is great. \r\n\r\nUnfortunately, the addition of `.tar.gz` as an extension to exclude is not working because of the implementation of the function `url_has_any_extension` which can be found in `scrapy/utils/url.py`.\r\n\r\nThe actual function looks like this:\r\n\r\n```python3\r\ndef url_has_any_extension(url, extensions):\r\n    return posixpath.splitext(parse_url(url).path)[1].lower() in extensions\r\n```\r\n\r\nHere is an example that does not work:\r\n```python3\r\nimport posixpath\r\nfrom urllib.parse import urlparse\r\nfrom scrapy.utils.url import parse_url\r\n\r\nparsed_url = urlparse(\"https://dl.google.com/go/go1.7rc3.freebsd-amd64.tar.gz\")\r\n# ParseResult(scheme='https', netloc='dl.google.com', path='/go/go1.7rc3.freebsd-amd64.tar.gz', params='', query='', fragment='')\r\n\r\nurl_has_any_extension(parsed_url, [\".tar.gz\"])\r\n# False\r\n\r\n# Breaking down \"url_has_any_extension\" function\r\nposixpath.splitext(parse_url(parsed_url).path)[1].lower() in [\".tar.gz\"]\r\n# False\r\n\r\nposixpath.splitext(parse_url(parsed_url).path)[1].lower()\r\n# '.gz'\r\n\r\nposixpath.splitext(parse_url(parsed_url).path)\r\n# ('/go/go1.7rc3.freebsd-amd64.tar', '.gz')\r\n```\r\n\r\nWe can see that the function splits the path from the first dot encountered, starting from the right, which leads to a `.tar.gz` extension not detected.\r\n\r\nI fixed the function by making use of `any` and the standard Python method `endswith`.\r\n","reactions":{"url":"https://api.github.com/repos/scrapy/scrapy/issues/5450/reactions","total_count":1,"+1":1,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/scrapy/scrapy/issues/5450/timeline","performed_via_github_app":null},{"url":"https://api.github.com/repos/scrapy/scrapy/issues/5448","repository_url":"https://api.github.com/repos/scrapy/scrapy","labels_url":"https://api.github.com/repos/scrapy/scrapy/issues/5448/labels{/name}","comments_url":"https://api.github.com/repos/scrapy/scrapy/issues/5448/comments","events_url":"https://api.github.com/repos/scrapy/scrapy/issues/5448/events","html_url":"https://github.com/scrapy/scrapy/pull/5448","id":1169577346,"node_id":"PR_kwDOAAgUXs40dTfX","number":5448,"title":"Add release notes for 2.6.2","user":{"login":"Gallaecio","id":705211,"node_id":"MDQ6VXNlcjcwNTIxMQ==","avatar_url":"https://avatars.githubusercontent.com/u/705211?v=4","gravatar_id":"","url":"https://api.github.com/users/Gallaecio","html_url":"https://github.com/Gallaecio","followers_url":"https://api.github.com/users/Gallaecio/followers","following_url":"https://api.github.com/users/Gallaecio/following{/other_user}","gists_url":"https://api.github.com/users/Gallaecio/gists{/gist_id}","starred_url":"https://api.github.com/users/Gallaecio/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/Gallaecio/subscriptions","organizations_url":"https://api.github.com/users/Gallaecio/orgs","repos_url":"https://api.github.com/users/Gallaecio/repos","events_url":"https://api.github.com/users/Gallaecio/events{/privacy}","received_events_url":"https://api.github.com/users/Gallaecio/received_events","type":"User","site_admin":false},"labels":[],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":{"url":"https://api.github.com/repos/scrapy/scrapy/milestones/40","html_url":"https://github.com/scrapy/scrapy/milestone/40","labels_url":"https://api.github.com/repos/scrapy/scrapy/milestones/40/labels","id":7735764,"node_id":"MI_kwDOAAgUXs4AdgnU","number":40,"title":"2.6.2","description":"Address regressions introduced in 2.6.","creator":{"login":"Gallaecio","id":705211,"node_id":"MDQ6VXNlcjcwNTIxMQ==","avatar_url":"https://avatars.githubusercontent.com/u/705211?v=4","gravatar_id":"","url":"https://api.github.com/users/Gallaecio","html_url":"https://github.com/Gallaecio","followers_url":"https://api.github.com/users/Gallaecio/followers","following_url":"https://api.github.com/users/Gallaecio/following{/other_user}","gists_url":"https://api.github.com/users/Gallaecio/gists{/gist_id}","starred_url":"https://api.github.com/users/Gallaecio/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/Gallaecio/subscriptions","organizations_url":"https://api.github.com/users/Gallaecio/orgs","repos_url":"https://api.github.com/users/Gallaecio/repos","events_url":"https://api.github.com/users/Gallaecio/events{/privacy}","received_events_url":"https://api.github.com/users/Gallaecio/received_events","type":"User","site_admin":false},"open_issues":3,"closed_issues":1,"state":"open","created_at":"2022-03-04T09:09:55Z","updated_at":"2022-03-15T11:58:41Z","due_on":"2022-03-15T07:00:00Z","closed_at":null},"comments":1,"created_at":"2022-03-15T11:58:41Z","updated_at":"2022-03-15T13:24:12Z","closed_at":null,"author_association":"MEMBER","active_lock_reason":null,"draft":false,"pull_request":{"url":"https://api.github.com/repos/scrapy/scrapy/pulls/5448","html_url":"https://github.com/scrapy/scrapy/pull/5448","diff_url":"https://github.com/scrapy/scrapy/pull/5448.diff","patch_url":"https://github.com/scrapy/scrapy/pull/5448.patch","merged_at":null},"body":null,"reactions":{"url":"https://api.github.com/repos/scrapy/scrapy/issues/5448/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/scrapy/scrapy/issues/5448/timeline","performed_via_github_app":null},{"url":"https://api.github.com/repos/scrapy/scrapy/issues/5447","repository_url":"https://api.github.com/repos/scrapy/scrapy","labels_url":"https://api.github.com/repos/scrapy/scrapy/issues/5447/labels{/name}","comments_url":"https://api.github.com/repos/scrapy/scrapy/issues/5447/comments","events_url":"https://api.github.com/repos/scrapy/scrapy/issues/5447/events","html_url":"https://github.com/scrapy/scrapy/issues/5447","id":1167327424,"node_id":"I_kwDOAAgUXs5FlADA","number":5447,"title":"scrapy.shell.inspect_response breaks with the asyncio reactor on ipython","user":{"login":"elacuesta","id":1731933,"node_id":"MDQ6VXNlcjE3MzE5MzM=","avatar_url":"https://avatars.githubusercontent.com/u/1731933?v=4","gravatar_id":"","url":"https://api.github.com/users/elacuesta","html_url":"https://github.com/elacuesta","followers_url":"https://api.github.com/users/elacuesta/followers","following_url":"https://api.github.com/users/elacuesta/following{/other_user}","gists_url":"https://api.github.com/users/elacuesta/gists{/gist_id}","starred_url":"https://api.github.com/users/elacuesta/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/elacuesta/subscriptions","organizations_url":"https://api.github.com/users/elacuesta/orgs","repos_url":"https://api.github.com/users/elacuesta/repos","events_url":"https://api.github.com/users/elacuesta/events{/privacy}","received_events_url":"https://api.github.com/users/elacuesta/received_events","type":"User","site_admin":false},"labels":[{"id":13907246,"node_id":"MDU6TGFiZWwxMzkwNzI0Ng==","url":"https://api.github.com/repos/scrapy/scrapy/labels/bug","name":"bug","color":"fc2929","default":true,"description":null},{"id":3270783977,"node_id":"MDU6TGFiZWwzMjcwNzgzOTc3","url":"https://api.github.com/repos/scrapy/scrapy/labels/asyncio","name":"asyncio","color":"fef2c0","default":false,"description":""}],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":0,"created_at":"2022-03-12T16:05:59Z","updated_at":"2022-03-13T04:03:02Z","closed_at":null,"author_association":"MEMBER","active_lock_reason":null,"body":"### Description\r\n\r\n`scrapy.shell.inspect_response` does not work with the `asyncio` reactor when using the `ipython` shell\r\n\r\n### Steps to Reproduce\r\n\r\n1. Create a spider with the following contents:\r\n```python\r\n# test-spiders/inspect_response_asyncio.py\r\nimport scrapy\r\n\r\nclass InspectResponseSpider(scrapy.Spider):\r\n    name = \"inspect\"\r\n    custom_settings = {\r\n        \"TWISTED_REACTOR\": \"twisted.internet.asyncioreactor.AsyncioSelectorReactor\",\r\n    }\r\n    start_urls = [\"https://example.org\"]\r\n\r\n    def parse(self, response):\r\n        scrapy.shell.inspect_response(response, self)\r\n```\r\n\r\n3. Run the spider, explicitly indicating the `ipython` shell:\r\n`SCRAPY_PYTHON_SHELL=ipython scrapy runspider test-spiders/inspect_response_asyncio.py`\r\n\r\n**Expected behavior:** [What you expect to happen]\r\n\r\nI'd expect a \"regular\" Scrapy shell session with `ipython`, like the following:\r\n```bash\r\n$ scrapy shell https://example.org\r\n(...)\r\n2022-03-12 12:51:43 [scrapy.core.engine] INFO: Spider opened\r\n2022-03-12 12:51:43 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://example.org> (referer: None)\r\n2022-03-12 12:51:44 [asyncio] DEBUG: Using selector: EpollSelector\r\n[s] Available Scrapy objects:\r\n[s]   scrapy     scrapy module (contains scrapy.Request, scrapy.Selector, etc)\r\n[s]   crawler    <scrapy.crawler.Crawler object at 0x7f3867c30310>\r\n[s]   item       {}\r\n[s]   request    <GET https://example.org>\r\n[s]   response   <200 https://example.org>\r\n[s]   settings   <scrapy.settings.Settings object at 0x7f3867c30790>\r\n[s]   spider     <DefaultSpider 'default' at 0x7f38673d8910>\r\n[s] Useful shortcuts:\r\n[s]   fetch(url[, redirect=True]) Fetch URL and update local objects (by default, redirects are followed)\r\n[s]   fetch(req)                  Fetch a scrapy.Request and update local objects \r\n[s]   shelp()           Shell help (print this help)\r\n[s]   view(response)    View response in a browser\r\nIn [1]:\r\n```\r\n\r\n**Actual behavior:** [What actually happens]\r\n\r\nIt breaks right after printing the banner:\r\n\r\n```\r\n$ SCRAPY_PYTHON_SHELL=ipython scrapy runspider test-spiders/inspect_response_asyncio.py\r\n2022-03-12 12:17:27 [scrapy.utils.log] INFO: Scrapy 2.6.1 started (bot: scrapybot)\r\n2022-03-12 12:17:27 [scrapy.utils.log] INFO: Versions: lxml 4.7.1.0, libxml2 2.9.12, cssselect 1.1.0, parsel 1.6.0, w3lib 1.22.0, Twisted 21.7.0, Python 3.9.6 (default, Sep  6 2021, 10:09:19) - [GCC 7.5.0], pyOpenSSL 21.0.0 (OpenSSL 1.1.1m  14 Dec 2021), cryptography 36.0.1, Platform Linux-4.15.0-163-generic-x86_64-with-glibc2.27\r\n2022-03-12 12:17:27 [scrapy.crawler] INFO: Overridden settings:\r\n{'EDITOR': 'nano',\r\n 'SPIDER_LOADER_WARN_ONLY': True,\r\n 'TWISTED_REACTOR': 'twisted.internet.asyncioreactor.AsyncioSelectorReactor'}\r\n2022-03-12 12:17:27 [asyncio] DEBUG: Using selector: EpollSelector\r\n2022-03-12 12:17:27 [scrapy.utils.log] DEBUG: Using reactor: twisted.internet.asyncioreactor.AsyncioSelectorReactor\r\n2022-03-12 12:17:27 [scrapy.utils.log] DEBUG: Using asyncio event loop: asyncio.unix_events._UnixSelectorEventLoop\r\n2022-03-12 12:17:27 [scrapy.extensions.telnet] INFO: Telnet Password: 8a930ae69eba944b\r\n2022-03-12 12:17:27 [scrapy.middleware] INFO: Enabled extensions:\r\n['scrapy.extensions.corestats.CoreStats',\r\n 'scrapy.extensions.telnet.TelnetConsole',\r\n 'scrapy.extensions.memusage.MemoryUsage',\r\n 'scrapy.extensions.logstats.LogStats']\r\n2022-03-12 12:17:28 [scrapy.middleware] INFO: Enabled downloader middlewares:\r\n['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',\r\n 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',\r\n 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',\r\n 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',\r\n 'scrapy.downloadermiddlewares.retry.RetryMiddleware',\r\n 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',\r\n 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',\r\n 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',\r\n 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',\r\n 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',\r\n 'scrapy.downloadermiddlewares.stats.DownloaderStats']\r\n2022-03-12 12:17:28 [scrapy.middleware] INFO: Enabled spider middlewares:\r\n['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',\r\n 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',\r\n 'scrapy.spidermiddlewares.referer.RefererMiddleware',\r\n 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',\r\n 'scrapy.spidermiddlewares.depth.DepthMiddleware']\r\n2022-03-12 12:17:28 [scrapy.middleware] INFO: Enabled item pipelines:\r\n[]\r\n2022-03-12 12:17:28 [scrapy.core.engine] INFO: Spider opened\r\n2022-03-12 12:17:28 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\r\n2022-03-12 12:17:28 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023\r\n2022-03-12 12:17:28 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://example.org> (referer: None)\r\n2022-03-12 12:17:29 [asyncio] DEBUG: Using selector: EpollSelector\r\n[s] Available Scrapy objects:\r\n[s]   scrapy     scrapy module (contains scrapy.Request, scrapy.Selector, etc)\r\n[s]   crawler    <scrapy.crawler.Crawler object at 0x7f3f5dd72ac0>\r\n[s]   item       {}\r\n[s]   request    <GET https://example.org>\r\n[s]   response   <200 https://example.org>\r\n[s]   settings   <scrapy.settings.Settings object at 0x7f3f5dd727c0>\r\n[s]   spider     <InspectResponseSpider 'inspect' at 0x7f3f5da0c6d0>\r\n[s] Useful shortcuts:\r\n[s]   shelp()           Shell help (print this help)\r\n[s]   view(response)    View response in a browser\r\n2022-03-12 12:17:29 [scrapy.core.scraper] ERROR: Spider error processing <GET https://example.org> (referer: None)\r\nTraceback (most recent call last):\r\n  File \"/home/eugenio/zyte/scrapy/venv-scrapy/lib/python3.9/site-packages/twisted/internet/defer.py\", line 858, in _runCallbacks\r\n    current.result = callback(  # type: ignore[misc]\r\n  File \"/home/eugenio/zyte/scrapy/scrapy/spiders/__init__.py\", line 67, in _parse\r\n    return self.parse(response, **kwargs)\r\n  File \"/home/eugenio/zyte/scrapy/test-spiders/inspect_response_asyncio.py\", line 12, in parse\r\n    scrapy.shell.inspect_response(response, self)\r\n  File \"/home/eugenio/zyte/scrapy/scrapy/shell.py\", line 161, in inspect_response\r\n    Shell(spider.crawler).start(response=response, spider=spider)\r\n  File \"/home/eugenio/zyte/scrapy/scrapy/shell.py\", line 75, in start\r\n    start_python_console(self.vars, shells=shells,\r\n  File \"/home/eugenio/zyte/scrapy/scrapy/utils/console.py\", line 102, in start_python_console\r\n    shell(namespace=namespace, banner=banner)\r\n  File \"/home/eugenio/zyte/scrapy/scrapy/utils/console.py\", line 24, in wrapper\r\n    shell()\r\n  File \"/home/eugenio/zyte/scrapy/venv-scrapy/lib/python3.9/site-packages/IPython/terminal/embed.py\", line 245, in __call__\r\n    self.mainloop(\r\n  File \"/home/eugenio/zyte/scrapy/venv-scrapy/lib/python3.9/site-packages/IPython/terminal/embed.py\", line 337, in mainloop\r\n    self.interact()\r\n  File \"/home/eugenio/zyte/scrapy/venv-scrapy/lib/python3.9/site-packages/IPython/terminal/interactiveshell.py\", line 638, in interact\r\n    code = self.prompt_for_code()\r\n  File \"/home/eugenio/zyte/scrapy/venv-scrapy/lib/python3.9/site-packages/IPython/terminal/interactiveshell.py\", line 576, in prompt_for_code\r\n    text = self.pt_app.prompt(\r\n  File \"/home/eugenio/zyte/scrapy/venv-scrapy/lib/python3.9/site-packages/prompt_toolkit/shortcuts/prompt.py\", line 1033, in prompt\r\n    return self.app.run(\r\n  File \"/home/eugenio/zyte/scrapy/venv-scrapy/lib/python3.9/site-packages/prompt_toolkit/application/application.py\", line 937, in run\r\n    return loop.run_until_complete(\r\n  File \"/usr/local/lib/python3.9/asyncio/base_events.py\", line 618, in run_until_complete\r\n    self._check_running()\r\n  File \"/usr/local/lib/python3.9/asyncio/base_events.py\", line 578, in _check_running\r\n    raise RuntimeError('This event loop is already running')\r\nRuntimeError: This event loop is already running\r\n2022-03-12 12:17:29 [py.warnings] WARNING: /home/eugenio/zyte/scrapy/venv-scrapy/lib/python3.9/site-packages/twisted/internet/defer.py:858: RuntimeWarning: coroutine 'Application.run_async' was never awaited\r\n  current.result = callback(  # type: ignore[misc]\r\n\r\n2022-03-12 12:17:29 [scrapy.core.engine] INFO: Closing spider (finished)\r\n2022-03-12 12:17:29 [scrapy.statscollectors] INFO: Dumping Scrapy stats:\r\n{'downloader/request_bytes': 211,\r\n 'downloader/request_count': 1,\r\n 'downloader/request_method_count/GET': 1,\r\n 'downloader/response_bytes': 1022,\r\n 'downloader/response_count': 1,\r\n 'downloader/response_status_count/200': 1,\r\n 'elapsed_time_seconds': 1.227345,\r\n 'finish_reason': 'finished',\r\n 'finish_time': datetime.datetime(2022, 3, 12, 15, 17, 29, 364888),\r\n 'httpcompression/response_bytes': 1256,\r\n 'httpcompression/response_count': 1,\r\n 'log_count/DEBUG': 5,\r\n 'log_count/ERROR': 1,\r\n 'log_count/INFO': 10,\r\n 'log_count/WARNING': 1,\r\n 'memusage/max': 61014016,\r\n 'memusage/startup': 61014016,\r\n 'response_received_count': 1,\r\n 'scheduler/dequeued': 1,\r\n 'scheduler/dequeued/memory': 1,\r\n 'scheduler/enqueued': 1,\r\n 'scheduler/enqueued/memory': 1,\r\n 'spider_exceptions/RuntimeError': 1,\r\n 'start_time': datetime.datetime(2022, 3, 12, 15, 17, 28, 137543)}\r\n2022-03-12 12:17:29 [scrapy.core.engine] INFO: Spider closed (finished)\r\n```\r\n\r\n**Reproduces how often:** [What percentage of the time does it reproduce?]\r\n\r\n100% of the time\r\n\r\n### Versions\r\n\r\n```\r\nScrapy       : 2.6.1\r\nlxml         : 4.7.1.0\r\nlibxml2      : 2.9.12\r\ncssselect    : 1.1.0\r\nparsel       : 1.6.0\r\nw3lib        : 1.22.0\r\nTwisted      : 21.7.0\r\nPython       : 3.9.6 (default, Sep  6 2021, 10:09:19) - [GCC 7.5.0]\r\npyOpenSSL    : 21.0.0 (OpenSSL 1.1.1m  14 Dec 2021)\r\ncryptography : 36.0.1\r\nPlatform     : Linux-4.15.0-163-generic-x86_64-with-glibc2.27\r\n```\r\n\r\n```\r\nScrapy       : 2.6.1\r\nlxml         : 4.8.0.0\r\nlibxml2      : 2.9.12\r\ncssselect    : 1.1.0\r\nparsel       : 1.6.0\r\nw3lib        : 1.22.0\r\nTwisted      : 22.2.0\r\nPython       : 3.9.0 (v3.9.0:9cf6752276, Oct  5 2020, 11:29:23) - [Clang 6.0 (clang-600.0.57)]\r\npyOpenSSL    : 22.0.0 (OpenSSL 1.1.1m  14 Dec 2021)\r\ncryptography : 36.0.1\r\nPlatform     : macOS-10.16-x86_64-i386-64bit\r\n```\r\n\r\n### Additional context\r\n\r\nIt all works correctly with:\r\n* the `shell` command, i.e. `SCRAPY_PYTHON_SHELL=ipython scrapy shell https://example.org`\r\n* the `python` interpreter, i.e. `SCRAPY_PYTHON_SHELL=python scrapy runspider test-spiders/inspect_response_asyncio.py`\r\n\r\nGiven the traceback and the fact that it works correctly with the `python` interpreter, there's a chance this is actually an upstream problem with `ipython` or `prompt_toolkit`. I'm posting here before further research in case someone can think of a solution in the Scrapy side.","reactions":{"url":"https://api.github.com/repos/scrapy/scrapy/issues/5447/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/scrapy/scrapy/issues/5447/timeline","performed_via_github_app":null},{"url":"https://api.github.com/repos/scrapy/scrapy/issues/5441","repository_url":"https://api.github.com/repos/scrapy/scrapy","labels_url":"https://api.github.com/repos/scrapy/scrapy/issues/5441/labels{/name}","comments_url":"https://api.github.com/repos/scrapy/scrapy/issues/5441/comments","events_url":"https://api.github.com/repos/scrapy/scrapy/issues/5441/events","html_url":"https://github.com/scrapy/scrapy/issues/5441","id":1161453550,"node_id":"I_kwDOAAgUXs5FOl_u","number":5441,"title":"Scrapy engine hangs when no requests returned by a downloader middleware but empty download slot","user":{"login":"akinad3","id":7266950,"node_id":"MDQ6VXNlcjcyNjY5NTA=","avatar_url":"https://avatars.githubusercontent.com/u/7266950?v=4","gravatar_id":"","url":"https://api.github.com/users/akinad3","html_url":"https://github.com/akinad3","followers_url":"https://api.github.com/users/akinad3/followers","following_url":"https://api.github.com/users/akinad3/following{/other_user}","gists_url":"https://api.github.com/users/akinad3/gists{/gist_id}","starred_url":"https://api.github.com/users/akinad3/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/akinad3/subscriptions","organizations_url":"https://api.github.com/users/akinad3/orgs","repos_url":"https://api.github.com/users/akinad3/repos","events_url":"https://api.github.com/users/akinad3/events{/privacy}","received_events_url":"https://api.github.com/users/akinad3/received_events","type":"User","site_admin":false},"labels":[{"id":443680419,"node_id":"MDU6TGFiZWw0NDM2ODA0MTk=","url":"https://api.github.com/repos/scrapy/scrapy/labels/needs%20more%20info","name":"needs more info","color":"fef2c0","default":false,"description":null}],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":3,"created_at":"2022-03-07T13:57:02Z","updated_at":"2022-03-08T08:23:05Z","closed_at":null,"author_association":"NONE","active_lock_reason":null,"body":"### Description\r\nI'm writing a scraper which should hit given enqueued requests at a given time in the day. \r\nExample: req1 should be allowed to go out between 1-3PM, req2 between 2-4PM.\r\n\r\nI'm forcefully enqueuing all my requests in start_requests using `crawler.engine.slot.scheduler.enqueue_request(request)` rather then having a generator approach on enqueuing the start requests.\r\n\r\n\r\n### Steps to Reproduce\r\n\r\nI wrote a middleware which whenever called, check if the given request is allowed to go out or not (through attributes set on meta). If yes, returns None if not, reenqueues the request (return request).\r\nI set the queue type to be FIFO rather than LIFO, so the middleware isn't hanging on the same request over and over again.\r\n\r\n**Expected behavior:** [What you expect to happen]\r\nResponses are handled correctly, even when no requests left to be sent out\r\n\r\n**Actual behavior:** [What actually happens]\r\nsending out a request with an empty downloader slot seems to be taking precedence over processing whatever responses are ready. \r\n\r\nNote: this does happen when CONCURRENT_REQUESTS = 1 also.\r\n\r\n### Versions\r\n\r\nPlease paste here the output of executing `scrapy version --verbose` in the command line.\r\nScrapy 2.5.1, but also in Scrapy 2.6\r\n\r\nIs there any other solution which would help acomplish my goal described at the beginning?","reactions":{"url":"https://api.github.com/repos/scrapy/scrapy/issues/5441/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/scrapy/scrapy/issues/5441/timeline","performed_via_github_app":null},{"url":"https://api.github.com/repos/scrapy/scrapy/issues/5439","repository_url":"https://api.github.com/repos/scrapy/scrapy","labels_url":"https://api.github.com/repos/scrapy/scrapy/issues/5439/labels{/name}","comments_url":"https://api.github.com/repos/scrapy/scrapy/issues/5439/comments","events_url":"https://api.github.com/repos/scrapy/scrapy/issues/5439/events","html_url":"https://github.com/scrapy/scrapy/pull/5439","id":1160618015,"node_id":"PR_kwDOAAgUXs40AL1N","number":5439,"title":"Document the Scrapy component API","user":{"login":"OrestisKan","id":45314405,"node_id":"MDQ6VXNlcjQ1MzE0NDA1","avatar_url":"https://avatars.githubusercontent.com/u/45314405?v=4","gravatar_id":"","url":"https://api.github.com/users/OrestisKan","html_url":"https://github.com/OrestisKan","followers_url":"https://api.github.com/users/OrestisKan/followers","following_url":"https://api.github.com/users/OrestisKan/following{/other_user}","gists_url":"https://api.github.com/users/OrestisKan/gists{/gist_id}","starred_url":"https://api.github.com/users/OrestisKan/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/OrestisKan/subscriptions","organizations_url":"https://api.github.com/users/OrestisKan/orgs","repos_url":"https://api.github.com/users/OrestisKan/repos","events_url":"https://api.github.com/users/OrestisKan/events{/privacy}","received_events_url":"https://api.github.com/users/OrestisKan/received_events","type":"User","site_admin":false},"labels":[],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":4,"created_at":"2022-03-06T13:59:30Z","updated_at":"2022-03-27T15:34:40Z","closed_at":null,"author_association":"NONE","active_lock_reason":null,"draft":false,"pull_request":{"url":"https://api.github.com/repos/scrapy/scrapy/pulls/5439","html_url":"https://github.com/scrapy/scrapy/pull/5439","diff_url":"https://github.com/scrapy/scrapy/pull/5439.diff","patch_url":"https://github.com/scrapy/scrapy/pull/5439.patch","merged_at":null},"body":"Remove duplicate definition of from_crawler and from_settings method. Make them refer to the centralized definitions in class-methods.rst where the explanation of the methods is elaborated. Closes #5110 ","reactions":{"url":"https://api.github.com/repos/scrapy/scrapy/issues/5439/reactions","total_count":3,"+1":1,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":1,"rocket":1,"eyes":0},"timeline_url":"https://api.github.com/repos/scrapy/scrapy/issues/5439/timeline","performed_via_github_app":null},{"url":"https://api.github.com/repos/scrapy/scrapy/issues/5437","repository_url":"https://api.github.com/repos/scrapy/scrapy","labels_url":"https://api.github.com/repos/scrapy/scrapy/issues/5437/labels{/name}","comments_url":"https://api.github.com/repos/scrapy/scrapy/issues/5437/comments","events_url":"https://api.github.com/repos/scrapy/scrapy/issues/5437/events","html_url":"https://github.com/scrapy/scrapy/issues/5437","id":1158397886,"node_id":"I_kwDOAAgUXs5FC7--","number":5437,"title":"CLOSESPIDER_TIMEOUT problem.","user":{"login":"shahrom322","id":91263568,"node_id":"MDQ6VXNlcjkxMjYzNTY4","avatar_url":"https://avatars.githubusercontent.com/u/91263568?v=4","gravatar_id":"","url":"https://api.github.com/users/shahrom322","html_url":"https://github.com/shahrom322","followers_url":"https://api.github.com/users/shahrom322/followers","following_url":"https://api.github.com/users/shahrom322/following{/other_user}","gists_url":"https://api.github.com/users/shahrom322/gists{/gist_id}","starred_url":"https://api.github.com/users/shahrom322/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/shahrom322/subscriptions","organizations_url":"https://api.github.com/users/shahrom322/orgs","repos_url":"https://api.github.com/users/shahrom322/repos","events_url":"https://api.github.com/users/shahrom322/events{/privacy}","received_events_url":"https://api.github.com/users/shahrom322/received_events","type":"User","site_admin":false},"labels":[{"id":13907246,"node_id":"MDU6TGFiZWwxMzkwNzI0Ng==","url":"https://api.github.com/repos/scrapy/scrapy/labels/bug","name":"bug","color":"fc2929","default":true,"description":null}],"state":"open","locked":false,"assignee":{"login":"Gallaecio","id":705211,"node_id":"MDQ6VXNlcjcwNTIxMQ==","avatar_url":"https://avatars.githubusercontent.com/u/705211?v=4","gravatar_id":"","url":"https://api.github.com/users/Gallaecio","html_url":"https://github.com/Gallaecio","followers_url":"https://api.github.com/users/Gallaecio/followers","following_url":"https://api.github.com/users/Gallaecio/following{/other_user}","gists_url":"https://api.github.com/users/Gallaecio/gists{/gist_id}","starred_url":"https://api.github.com/users/Gallaecio/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/Gallaecio/subscriptions","organizations_url":"https://api.github.com/users/Gallaecio/orgs","repos_url":"https://api.github.com/users/Gallaecio/repos","events_url":"https://api.github.com/users/Gallaecio/events{/privacy}","received_events_url":"https://api.github.com/users/Gallaecio/received_events","type":"User","site_admin":false},"assignees":[{"login":"Gallaecio","id":705211,"node_id":"MDQ6VXNlcjcwNTIxMQ==","avatar_url":"https://avatars.githubusercontent.com/u/705211?v=4","gravatar_id":"","url":"https://api.github.com/users/Gallaecio","html_url":"https://github.com/Gallaecio","followers_url":"https://api.github.com/users/Gallaecio/followers","following_url":"https://api.github.com/users/Gallaecio/following{/other_user}","gists_url":"https://api.github.com/users/Gallaecio/gists{/gist_id}","starred_url":"https://api.github.com/users/Gallaecio/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/Gallaecio/subscriptions","organizations_url":"https://api.github.com/users/Gallaecio/orgs","repos_url":"https://api.github.com/users/Gallaecio/repos","events_url":"https://api.github.com/users/Gallaecio/events{/privacy}","received_events_url":"https://api.github.com/users/Gallaecio/received_events","type":"User","site_admin":false}],"milestone":{"url":"https://api.github.com/repos/scrapy/scrapy/milestones/40","html_url":"https://github.com/scrapy/scrapy/milestone/40","labels_url":"https://api.github.com/repos/scrapy/scrapy/milestones/40/labels","id":7735764,"node_id":"MI_kwDOAAgUXs4AdgnU","number":40,"title":"2.6.2","description":"Address regressions introduced in 2.6.","creator":{"login":"Gallaecio","id":705211,"node_id":"MDQ6VXNlcjcwNTIxMQ==","avatar_url":"https://avatars.githubusercontent.com/u/705211?v=4","gravatar_id":"","url":"https://api.github.com/users/Gallaecio","html_url":"https://github.com/Gallaecio","followers_url":"https://api.github.com/users/Gallaecio/followers","following_url":"https://api.github.com/users/Gallaecio/following{/other_user}","gists_url":"https://api.github.com/users/Gallaecio/gists{/gist_id}","starred_url":"https://api.github.com/users/Gallaecio/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/Gallaecio/subscriptions","organizations_url":"https://api.github.com/users/Gallaecio/orgs","repos_url":"https://api.github.com/users/Gallaecio/repos","events_url":"https://api.github.com/users/Gallaecio/events{/privacy}","received_events_url":"https://api.github.com/users/Gallaecio/received_events","type":"User","site_admin":false},"open_issues":3,"closed_issues":1,"state":"open","created_at":"2022-03-04T09:09:55Z","updated_at":"2022-03-15T11:58:41Z","due_on":"2022-03-15T07:00:00Z","closed_at":null},"comments":7,"created_at":"2022-03-03T13:16:39Z","updated_at":"2022-03-29T08:20:59Z","closed_at":null,"author_association":"NONE","active_lock_reason":null,"body":"### Description\r\n\r\nWhen switching from version 2.5.1 to 2.6.1, there was a problem with the parser terminating if the shutdown condition was CLOSESPIDER_TIMEOUT.\r\n\r\n### Steps to Reproduce\r\n\r\n1. pip install -U scrapy\r\n2. scrapy crawl --set 'CLOSESPIDER_TIMEOUT=1' some_crawler\r\n\r\n**Expected behavior:** [What you expect to happen]\r\n\r\n2022-03-03 16:09:30 [scrapy.core.engine] INFO: Spider closed (closespider_timeout)\r\n\r\n**Actual behavior:** [What actually happens]\r\n\r\n2022-03-03 16:09:30 [scrapy.core.engine] INFO: Spider closed (closespider_timeout)\r\n2022-03-03 16:09:30 [scrapy.core.engine] INFO: Error while scheduling new request\r\nTraceback (most recent call last):\r\n  File \"c:\\users\\onedrive\\python\\scrapy_test\\venv\\lib\\site-packages\\twisted\\internet\\task.py\", line 528, in _oneWorkUnit\r\n    result = next(self._iterator)\r\nStopIteration\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"c:\\users\\onedrive\\python\\scrapy_test\\venv\\lib\\site-packages\\twisted\\internet\\defer.py\", line 858, in _runCallbacks\r\n    current.result = callback(  # type: ignore[misc]\r\n  File \"c:\\users\\onedrive\\python\\scrapy_test\\venv\\lib\\site-packages\\scrapy\\core\\engine.py\", line 187, in <lambda>\r\n    d.addBoth(lambda _: self.slot.nextcall.schedule())\r\nAttributeError: 'NoneType' object has no attribute 'nextcall'\r\n\r\n**Reproduces how often:** [What percentage of the time does it reproduce?]\r\n\r\n\r\nIt always reproduce\r\n\r\n### Versions\r\n\r\nScrapy       : 2.6.1\r\nlxml         : 4.7.1.0\r\nlibxml2      : 2.9.12\r\ncssselect    : 1.1.0\r\nparsel       : 1.6.0\r\nw3lib        : 1.22.0\r\nTwisted      : 21.7.0\r\nPython       : 3.9.6 (tags/v3.9.6:db3ff76, Jun 28 2021, 15:26:21) [MSC v.1929 64 bit (AMD64)]\r\npyOpenSSL    : 22.0.0 (OpenSSL 1.1.1m  14 Dec 2021)\r\ncryptography : 36.0.1\r\nPlatform     : Windows-10-10.0.19044-SP0\r\n\r\n","reactions":{"url":"https://api.github.com/repos/scrapy/scrapy/issues/5437/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/scrapy/scrapy/issues/5437/timeline","performed_via_github_app":null},{"url":"https://api.github.com/repos/scrapy/scrapy/issues/5435","repository_url":"https://api.github.com/repos/scrapy/scrapy","labels_url":"https://api.github.com/repos/scrapy/scrapy/issues/5435/labels{/name}","comments_url":"https://api.github.com/repos/scrapy/scrapy/issues/5435/comments","events_url":"https://api.github.com/repos/scrapy/scrapy/issues/5435/events","html_url":"https://github.com/scrapy/scrapy/issues/5435","id":1156953839,"node_id":"I_kwDOAAgUXs5E9bbv","number":5435,"title":"2.6.0 breaks calling multiple Spider in CrawlerProcess()","user":{"login":"hideishi-m","id":92977868,"node_id":"U_kgDOBYq6zA","avatar_url":"https://avatars.githubusercontent.com/u/92977868?v=4","gravatar_id":"","url":"https://api.github.com/users/hideishi-m","html_url":"https://github.com/hideishi-m","followers_url":"https://api.github.com/users/hideishi-m/followers","following_url":"https://api.github.com/users/hideishi-m/following{/other_user}","gists_url":"https://api.github.com/users/hideishi-m/gists{/gist_id}","starred_url":"https://api.github.com/users/hideishi-m/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/hideishi-m/subscriptions","organizations_url":"https://api.github.com/users/hideishi-m/orgs","repos_url":"https://api.github.com/users/hideishi-m/repos","events_url":"https://api.github.com/users/hideishi-m/events{/privacy}","received_events_url":"https://api.github.com/users/hideishi-m/received_events","type":"User","site_admin":false},"labels":[{"id":13907246,"node_id":"MDU6TGFiZWwxMzkwNzI0Ng==","url":"https://api.github.com/repos/scrapy/scrapy/labels/bug","name":"bug","color":"fc2929","default":true,"description":null}],"state":"open","locked":false,"assignee":{"login":"Gallaecio","id":705211,"node_id":"MDQ6VXNlcjcwNTIxMQ==","avatar_url":"https://avatars.githubusercontent.com/u/705211?v=4","gravatar_id":"","url":"https://api.github.com/users/Gallaecio","html_url":"https://github.com/Gallaecio","followers_url":"https://api.github.com/users/Gallaecio/followers","following_url":"https://api.github.com/users/Gallaecio/following{/other_user}","gists_url":"https://api.github.com/users/Gallaecio/gists{/gist_id}","starred_url":"https://api.github.com/users/Gallaecio/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/Gallaecio/subscriptions","organizations_url":"https://api.github.com/users/Gallaecio/orgs","repos_url":"https://api.github.com/users/Gallaecio/repos","events_url":"https://api.github.com/users/Gallaecio/events{/privacy}","received_events_url":"https://api.github.com/users/Gallaecio/received_events","type":"User","site_admin":false},"assignees":[{"login":"Gallaecio","id":705211,"node_id":"MDQ6VXNlcjcwNTIxMQ==","avatar_url":"https://avatars.githubusercontent.com/u/705211?v=4","gravatar_id":"","url":"https://api.github.com/users/Gallaecio","html_url":"https://github.com/Gallaecio","followers_url":"https://api.github.com/users/Gallaecio/followers","following_url":"https://api.github.com/users/Gallaecio/following{/other_user}","gists_url":"https://api.github.com/users/Gallaecio/gists{/gist_id}","starred_url":"https://api.github.com/users/Gallaecio/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/Gallaecio/subscriptions","organizations_url":"https://api.github.com/users/Gallaecio/orgs","repos_url":"https://api.github.com/users/Gallaecio/repos","events_url":"https://api.github.com/users/Gallaecio/events{/privacy}","received_events_url":"https://api.github.com/users/Gallaecio/received_events","type":"User","site_admin":false}],"milestone":{"url":"https://api.github.com/repos/scrapy/scrapy/milestones/40","html_url":"https://github.com/scrapy/scrapy/milestone/40","labels_url":"https://api.github.com/repos/scrapy/scrapy/milestones/40/labels","id":7735764,"node_id":"MI_kwDOAAgUXs4AdgnU","number":40,"title":"2.6.2","description":"Address regressions introduced in 2.6.","creator":{"login":"Gallaecio","id":705211,"node_id":"MDQ6VXNlcjcwNTIxMQ==","avatar_url":"https://avatars.githubusercontent.com/u/705211?v=4","gravatar_id":"","url":"https://api.github.com/users/Gallaecio","html_url":"https://github.com/Gallaecio","followers_url":"https://api.github.com/users/Gallaecio/followers","following_url":"https://api.github.com/users/Gallaecio/following{/other_user}","gists_url":"https://api.github.com/users/Gallaecio/gists{/gist_id}","starred_url":"https://api.github.com/users/Gallaecio/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/Gallaecio/subscriptions","organizations_url":"https://api.github.com/users/Gallaecio/orgs","repos_url":"https://api.github.com/users/Gallaecio/repos","events_url":"https://api.github.com/users/Gallaecio/events{/privacy}","received_events_url":"https://api.github.com/users/Gallaecio/received_events","type":"User","site_admin":false},"open_issues":3,"closed_issues":1,"state":"open","created_at":"2022-03-04T09:09:55Z","updated_at":"2022-03-15T11:58:41Z","due_on":"2022-03-15T07:00:00Z","closed_at":null},"comments":3,"created_at":"2022-03-02T09:58:06Z","updated_at":"2022-03-04T09:10:47Z","closed_at":null,"author_association":"NONE","active_lock_reason":null,"body":"<!--\r\n\r\nThanks for taking an interest in Scrapy!\r\n\r\nIf you have a question that starts with \"How to...\", please see the Scrapy Community page: https://scrapy.org/community/.\r\nThe GitHub issue tracker's purpose is to deal with bug reports and feature requests for the project itself.\r\n\r\nKeep in mind that by filing an issue, you are expected to comply with Scrapy's Code of Conduct, including treating everyone with respect: https://github.com/scrapy/scrapy/blob/master/CODE_OF_CONDUCT.md\r\n\r\nThe following is a suggested template to structure your issue, you can find more guidelines at https://doc.scrapy.org/en/latest/contributing.html#reporting-bugs\r\n\r\n-->\r\n\r\n### Description\r\n\r\nSince 2.6.0, it breaks calling multiple Spiders from CrawlerProcess() as shown in the common practices \r\n\r\nhttps://docs.scrapy.org/en/latest/topics/practices.html#running-multiple-spiders-in-the-same-process\r\n\r\n### Steps to Reproduce\r\n\r\n1. use Scrapy=>2.6.0\r\n2. following is the code to reproduce\r\n\r\n```\r\nimport scrapy\r\nfrom scrapy.crawler import CrawlerProcess\r\nfrom scrapy.http import Request\r\n\r\n\r\nclass MySpider(scrapy.Spider):\r\n    name = 'MySpider'\r\n\r\n    def __init__(self, url, *args, **kwargs):\r\n        super().__init__(*args, **kwargs)\r\n        self.url = url\r\n\r\n    def start_requests(self):\r\n        yield Request(url=self.url, callback=self.parse)\r\n\r\n    def parse(self, response):\r\n        print(response.url)\r\n\r\n\r\nprocess = CrawlerProcess({\r\n    'DEPTH_LIMIT': 1,\r\n    'DEPTH_PRIORITY': 1\r\n})\r\nprocess.crawl(MySpider, url='https://www.google.com')\r\nprocess.crawl(MySpider, url='https://www.google.co.jp')\r\nprocess.start()\r\n```\r\n\r\n**Expected behavior:** [What you expect to happen]\r\n\r\nFollowing is the result from Scrapy 2.5.1\r\n\r\n```\r\n2022-03-02 18:49:45 [scrapy.utils.log] INFO: Scrapy 2.5.1 started (bot: scrapybot)\r\n2022-03-02 18:49:45 [scrapy.utils.log] INFO: Versions: lxml 4.8.0.0, libxml2 2.9.12, cssselect 1.1.0, parsel 1.6.0, w3lib 1.22.0, Twisted 22.1.0, Python 3.9.10 (main, Jan 17 2022, 08:36:28) - [GCC 11.2.1 20210728 (Red Hat 11.2.1-1)], pyOpenSSL 22.0.0 (OpenSSL 1.1.1m  14 Dec 2021), cryptography 36.0.1, Platform Linux-3.10.0-1160.59.1.el7.x86_64-x86_64-with-glibc2.17\r\n2022-03-02 18:49:45 [scrapy.utils.log] DEBUG: Using reactor: twisted.internet.epollreactor.EPollReactor\r\n2022-03-02 18:49:45 [scrapy.crawler] INFO: Overridden settings:\r\n{'DEPTH_LIMIT': 1, 'DEPTH_PRIORITY': 1}\r\n2022-03-02 18:49:45 [scrapy.extensions.telnet] INFO: Telnet Password: afe09d724aae9642\r\n2022-03-02 18:49:45 [scrapy.middleware] INFO: Enabled extensions:\r\n['scrapy.extensions.corestats.CoreStats',\r\n 'scrapy.extensions.telnet.TelnetConsole',\r\n 'scrapy.extensions.memusage.MemoryUsage',\r\n 'scrapy.extensions.logstats.LogStats']\r\n2022-03-02 18:49:45 [scrapy.middleware] INFO: Enabled downloader middlewares:\r\n['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',\r\n 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',\r\n 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',\r\n 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',\r\n 'scrapy.downloadermiddlewares.retry.RetryMiddleware',\r\n 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',\r\n 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',\r\n 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',\r\n 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',\r\n 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',\r\n 'scrapy.downloadermiddlewares.stats.DownloaderStats']\r\n2022-03-02 18:49:45 [scrapy.middleware] INFO: Enabled spider middlewares:\r\n['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',\r\n 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',\r\n 'scrapy.spidermiddlewares.referer.RefererMiddleware',\r\n 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',\r\n 'scrapy.spidermiddlewares.depth.DepthMiddleware']\r\n2022-03-02 18:49:45 [scrapy.middleware] INFO: Enabled item pipelines:\r\n[]\r\n2022-03-02 18:49:45 [scrapy.core.engine] INFO: Spider opened\r\n2022-03-02 18:49:45 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\r\n2022-03-02 18:49:45 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023\r\n2022-03-02 18:49:45 [scrapy.crawler] INFO: Overridden settings:\r\n{'DEPTH_LIMIT': 1, 'DEPTH_PRIORITY': 1}\r\n2022-03-02 18:49:45 [scrapy.extensions.telnet] INFO: Telnet Password: bd1670acfb7fb550\r\n2022-03-02 18:49:45 [scrapy.middleware] INFO: Enabled extensions:\r\n['scrapy.extensions.corestats.CoreStats',\r\n 'scrapy.extensions.telnet.TelnetConsole',\r\n 'scrapy.extensions.memusage.MemoryUsage',\r\n 'scrapy.extensions.logstats.LogStats']\r\n2022-03-02 18:49:45 [scrapy.middleware] INFO: Enabled downloader middlewares:\r\n['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',\r\n 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',\r\n 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',\r\n 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',\r\n 'scrapy.downloadermiddlewares.retry.RetryMiddleware',\r\n 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',\r\n 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',\r\n 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',\r\n 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',\r\n 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',\r\n 'scrapy.downloadermiddlewares.stats.DownloaderStats']\r\n2022-03-02 18:49:45 [scrapy.middleware] INFO: Enabled spider middlewares:\r\n['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',\r\n 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',\r\n 'scrapy.spidermiddlewares.referer.RefererMiddleware',\r\n 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',\r\n 'scrapy.spidermiddlewares.depth.DepthMiddleware']\r\n2022-03-02 18:49:45 [scrapy.middleware] INFO: Enabled item pipelines:\r\n[]\r\n2022-03-02 18:49:45 [scrapy.core.engine] INFO: Spider opened\r\n2022-03-02 18:49:45 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\r\n2022-03-02 18:49:45 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6024\r\n2022-03-02 18:49:45 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.google.com> (referer: None)\r\n2022-03-02 18:49:45 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.google.co.jp> (referer: None)\r\nhttps://www.google.com\r\nhttps://www.google.co.jp\r\n2022-03-02 18:49:46 [scrapy.core.engine] INFO: Closing spider (finished)\r\n2022-03-02 18:49:46 [scrapy.statscollectors] INFO: Dumping Scrapy stats:\r\n{'downloader/request_bytes': 214,\r\n 'downloader/request_count': 1,\r\n 'downloader/request_method_count/GET': 1,\r\n 'downloader/response_bytes': 7675,\r\n 'downloader/response_count': 1,\r\n 'downloader/response_status_count/200': 1,\r\n 'elapsed_time_seconds': 0.465932,\r\n 'finish_reason': 'finished',\r\n 'finish_time': datetime.datetime(2022, 3, 2, 9, 49, 46, 66477),\r\n 'httpcompression/response_bytes': 15980,\r\n 'httpcompression/response_count': 1,\r\n 'log_count/DEBUG': 2,\r\n 'log_count/INFO': 19,\r\n 'memusage/max': 47611904,\r\n 'memusage/startup': 47611904,\r\n 'response_received_count': 1,\r\n 'scheduler/dequeued': 1,\r\n 'scheduler/dequeued/memory': 1,\r\n 'scheduler/enqueued': 1,\r\n 'scheduler/enqueued/memory': 1,\r\n 'start_time': datetime.datetime(2022, 3, 2, 9, 49, 45, 600545)}\r\n2022-03-02 18:49:46 [scrapy.core.engine] INFO: Spider closed (finished)\r\n2022-03-02 18:49:46 [scrapy.core.engine] INFO: Closing spider (finished)\r\n2022-03-02 18:49:46 [scrapy.statscollectors] INFO: Dumping Scrapy stats:\r\n{'downloader/request_bytes': 216,\r\n 'downloader/request_count': 1,\r\n 'downloader/request_method_count/GET': 1,\r\n 'downloader/response_bytes': 7602,\r\n 'downloader/response_count': 1,\r\n 'downloader/response_status_count/200': 1,\r\n 'elapsed_time_seconds': 0.431935,\r\n 'finish_reason': 'finished',\r\n 'finish_time': datetime.datetime(2022, 3, 2, 9, 49, 46, 98011),\r\n 'httpcompression/response_bytes': 14794,\r\n 'httpcompression/response_count': 1,\r\n 'log_count/DEBUG': 2,\r\n 'log_count/INFO': 13,\r\n 'memusage/max': 47669248,\r\n 'memusage/startup': 47669248,\r\n 'response_received_count': 1,\r\n 'scheduler/dequeued': 1,\r\n 'scheduler/dequeued/memory': 1,\r\n 'scheduler/enqueued': 1,\r\n 'scheduler/enqueued/memory': 1,\r\n 'start_time': datetime.datetime(2022, 3, 2, 9, 49, 45, 666076)}\r\n2022-03-02 18:49:46 [scrapy.core.engine] INFO: Spider closed (finished)\r\n```\r\n\r\n**Actual behavior:** [What actually happens]\r\n\r\nSpider fails with twisted.internet.error.ReactorAlreadyInstalledError\r\n\r\n```\r\n2022-03-02 18:49:12 [scrapy.utils.log] INFO: Scrapy 2.6.1 started (bot: scrapybot)\r\n2022-03-02 18:49:12 [scrapy.utils.log] INFO: Versions: lxml 4.8.0.0, libxml2 2.9.12, cssselect 1.1.0, parsel 1.6.0, w3lib 1.22.0, Twisted 22.1.0, Python 3.9.10 (main, Jan 17 2022, 08:36:28) - [GCC 11.2.1 20210728 (Red Hat 11.2.1-1)], pyOpenSSL 22.0.0 (OpenSSL 1.1.1m  14 Dec 2021), cryptography 36.0.1, Platform Linux-3.10.0-1160.59.1.el7.x86_64-x86_64-with-glibc2.17\r\n2022-03-02 18:49:12 [scrapy.crawler] INFO: Overridden settings:\r\n{'DEPTH_LIMIT': 1, 'DEPTH_PRIORITY': 1}\r\n2022-03-02 18:49:12 [scrapy.utils.log] DEBUG: Using reactor: twisted.internet.epollreactor.EPollReactor\r\n2022-03-02 18:49:12 [scrapy.extensions.telnet] INFO: Telnet Password: ce57e6aa863bb786\r\n2022-03-02 18:49:12 [scrapy.middleware] INFO: Enabled extensions:\r\n['scrapy.extensions.corestats.CoreStats',\r\n 'scrapy.extensions.telnet.TelnetConsole',\r\n 'scrapy.extensions.memusage.MemoryUsage',\r\n 'scrapy.extensions.logstats.LogStats']\r\n2022-03-02 18:49:13 [scrapy.middleware] INFO: Enabled downloader middlewares:\r\n['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',\r\n 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',\r\n 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',\r\n 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',\r\n 'scrapy.downloadermiddlewares.retry.RetryMiddleware',\r\n 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',\r\n 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',\r\n 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',\r\n 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',\r\n 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',\r\n 'scrapy.downloadermiddlewares.stats.DownloaderStats']\r\n2022-03-02 18:49:13 [scrapy.middleware] INFO: Enabled spider middlewares:\r\n['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',\r\n 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',\r\n 'scrapy.spidermiddlewares.referer.RefererMiddleware',\r\n 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',\r\n 'scrapy.spidermiddlewares.depth.DepthMiddleware']\r\n2022-03-02 18:49:13 [scrapy.middleware] INFO: Enabled item pipelines:\r\n[]\r\n2022-03-02 18:49:13 [scrapy.core.engine] INFO: Spider opened\r\n2022-03-02 18:49:13 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\r\n2022-03-02 18:49:13 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023\r\n2022-03-02 18:49:13 [scrapy.crawler] INFO: Overridden settings:\r\n{'DEPTH_LIMIT': 1, 'DEPTH_PRIORITY': 1}\r\nTraceback (most recent call last):\r\n  File \"/home/kusanagi/work/scrapy/test.py\", line 25, in <module>\r\n    process.crawl(MySpider, url='https://www.google.co.jp')\r\n  File \"/usr/local/lib/python3.9/site-packages/scrapy/crawler.py\", line 205, in crawl\r\n    crawler = self.create_crawler(crawler_or_spidercls)\r\n  File \"/usr/local/lib/python3.9/site-packages/scrapy/crawler.py\", line 238, in create_crawler\r\n    return self._create_crawler(crawler_or_spidercls)\r\n  File \"/usr/local/lib/python3.9/site-packages/scrapy/crawler.py\", line 313, in _create_crawler\r\n    return Crawler(spidercls, self.settings, init_reactor=True)\r\n  File \"/usr/local/lib/python3.9/site-packages/scrapy/crawler.py\", line 82, in __init__\r\n    default.install()\r\n  File \"/usr/local/lib/python3.9/site-packages/twisted/internet/epollreactor.py\", line 256, in install\r\n    installReactor(p)\r\n  File \"/usr/local/lib/python3.9/site-packages/twisted/internet/main.py\", line 32, in installReactor\r\n    raise error.ReactorAlreadyInstalledError(\"reactor already installed\")\r\ntwisted.internet.error.ReactorAlreadyInstalledError: reactor already installed\r\n```\r\n\r\n**Reproduces how often:** [What percentage of the time does it reproduce?]\r\n\r\nAlways.\r\n\r\n### Versions\r\n\r\nScrapy       : 2.6.1\r\nlxml         : 4.8.0.0\r\nlibxml2      : 2.9.12\r\ncssselect    : 1.1.0\r\nparsel       : 1.6.0\r\nw3lib        : 1.22.0\r\nTwisted      : 22.1.0\r\nPython       : 3.9.10 (main, Jan 17 2022, 08:36:28) - [GCC 11.2.1 20210728 (Red Hat 11.2.1-1)]\r\npyOpenSSL    : 22.0.0 (OpenSSL 1.1.1m  14 Dec 2021)\r\ncryptography : 36.0.1\r\nPlatform     : Linux-3.10.0-1160.59.1.el7.x86_64-x86_64-with-glibc2.17\r\n\r\n### Additional context\r\n\r\nThe intension of using the same MySpider but from CrawlerProcess is to call Scrapy programatically using different initial url and some tweaks to parser depending on the initial url.\r\n\r\nI think this is very fair usage and was working fine before 2.6.0.","reactions":{"url":"https://api.github.com/repos/scrapy/scrapy/issues/5435/reactions","total_count":2,"+1":2,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/scrapy/scrapy/issues/5435/timeline","performed_via_github_app":null},{"url":"https://api.github.com/repos/scrapy/scrapy/issues/5431","repository_url":"https://api.github.com/repos/scrapy/scrapy","labels_url":"https://api.github.com/repos/scrapy/scrapy/issues/5431/labels{/name}","comments_url":"https://api.github.com/repos/scrapy/scrapy/issues/5431/comments","events_url":"https://api.github.com/repos/scrapy/scrapy/issues/5431/events","html_url":"https://github.com/scrapy/scrapy/issues/5431","id":1155719536,"node_id":"I_kwDOAAgUXs5E4uFw","number":5431,"title":"Improve cookie handling","user":{"login":"Gallaecio","id":705211,"node_id":"MDQ6VXNlcjcwNTIxMQ==","avatar_url":"https://avatars.githubusercontent.com/u/705211?v=4","gravatar_id":"","url":"https://api.github.com/users/Gallaecio","html_url":"https://github.com/Gallaecio","followers_url":"https://api.github.com/users/Gallaecio/followers","following_url":"https://api.github.com/users/Gallaecio/following{/other_user}","gists_url":"https://api.github.com/users/Gallaecio/gists{/gist_id}","starred_url":"https://api.github.com/users/Gallaecio/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/Gallaecio/subscriptions","organizations_url":"https://api.github.com/users/Gallaecio/orgs","repos_url":"https://api.github.com/users/Gallaecio/repos","events_url":"https://api.github.com/users/Gallaecio/events{/privacy}","received_events_url":"https://api.github.com/users/Gallaecio/received_events","type":"User","site_admin":false},"labels":[{"id":14483092,"node_id":"MDU6TGFiZWwxNDQ4MzA5Mg==","url":"https://api.github.com/repos/scrapy/scrapy/labels/enhancement","name":"enhancement","color":"84b6eb","default":true,"description":null},{"id":168174314,"node_id":"MDU6TGFiZWwxNjgxNzQzMTQ=","url":"https://api.github.com/repos/scrapy/scrapy/labels/gsoc-candidate","name":"gsoc-candidate","color":"006b75","default":false,"description":null}],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":7,"created_at":"2022-03-01T19:21:58Z","updated_at":"2022-03-15T10:22:55Z","closed_at":null,"author_association":"MEMBER","active_lock_reason":null,"body":"There are different aspects of cookie handling in Scrapy that we should improve. This issue aims to centralize a set of improvements that could be addressed as part of a Google Summer of Code project.\r\n\r\n-   Implement the latest standard of cookies, the one web browsers use.\r\n\r\n    This cannot be done with the Python standard library, as its cookie implementation does not comply with the latest standards of cookies. We should look for Python libraries that do or, if none fit the bill, build our own Python library for modern cookie handling.\r\n\r\n    As part of this implementation, we should build a comprehensive set of tests covering all aspects of cookie handling. [There is a draft pull request of initial work on this front](https://github.com/scrapy/scrapy/pull/5430).\r\n\r\n    Related standards:\r\n\r\n    - https://datatracker.ietf.org/doc/html/rfc6265\r\n    - https://datatracker.ietf.org/doc/html/draft-ietf-httpbis-rfc6265bis/\r\n    - https://datatracker.ietf.org/doc/html/draft-ietf-httpbis-cookie-same-site-00\r\n    - https://datatracker.ietf.org/doc/html/draft-ietf-httpbis-cookie-alone-01\r\n\r\n-   Let the `Cookie` header of a request be processed just the same as the `cookies` parameter of `Request`.\r\n\r\n    Reported at https://github.com/scrapy/scrapy/issues/1992, which we [fixed](https://github.com/scrapy/scrapy/pull/2400), but we had to [revert the fix](https://github.com/scrapy/scrapy/pull/4823) due to [undesired side effects](https://github.com/scrapy/scrapy/issues/4717); there is however [a new draft pull request to address the issue for good](https://github.com/scrapy/scrapy/pull/4812).\r\n\r\n-   Allow users to decide, for a request and its response, any combination of the following:\r\n    \r\n    -   Whether or not cookies defined by the user in the request itself (be it through the `cookies` parameter of `Request` or the `Cookie` header) should be included in the request cookiejar.\r\n    -   Whether or not cookies from the cookiejar should be included into the request.\r\n    -   Whether or not cookies received in the response should be included into the cookiejar.\r\n\r\n    Some of the combinations are already possible, e.g. by the use of the `dont_merge_cookies` and `cookiejar` request metadata keys. We should extend support to the rest of scenarios, and make sure we document all scenarios properly.\r\n\r\n    Related issues: #2124, #3769\r\n\r\n-   Provide a user-friendly API to interact with cookiejars.\r\n\r\n    Related issues: https://github.com/scrapy/scrapy/issues/1448, https://github.com/scrapy/scrapy/issues/1878, https://github.com/scrapy/scrapy/pull/2986\r\n\r\n-   Allow cookie storage\r\n\r\n    A single setting to define the storage path may be enough.\r\n\r\n    Related plugin: https://github.com/scrapedia/scrapy-cookies\r\n\r\n**Note:** A Google Summer of Code project around this idea does not need to cover everything here. Choose a subset of the work that fits the time you plan to spend on Google Summer of Code and your estimations for the work. It is better to overestimate the time it will take you to complete tasks, and to have stretch goals to spend the extra time after project completion, than to underestimate and fail to achieve your goals on time.","reactions":{"url":"https://api.github.com/repos/scrapy/scrapy/issues/5431/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/scrapy/scrapy/issues/5431/timeline","performed_via_github_app":null},{"url":"https://api.github.com/repos/scrapy/scrapy/issues/5430","repository_url":"https://api.github.com/repos/scrapy/scrapy","labels_url":"https://api.github.com/repos/scrapy/scrapy/issues/5430/labels{/name}","comments_url":"https://api.github.com/repos/scrapy/scrapy/issues/5430/comments","events_url":"https://api.github.com/repos/scrapy/scrapy/issues/5430/events","html_url":"https://github.com/scrapy/scrapy/pull/5430","id":1155716361,"node_id":"PR_kwDOAAgUXs4zv48r","number":5430,"title":"Comprehensive cookie domain test cases","user":{"login":"Gallaecio","id":705211,"node_id":"MDQ6VXNlcjcwNTIxMQ==","avatar_url":"https://avatars.githubusercontent.com/u/705211?v=4","gravatar_id":"","url":"https://api.github.com/users/Gallaecio","html_url":"https://github.com/Gallaecio","followers_url":"https://api.github.com/users/Gallaecio/followers","following_url":"https://api.github.com/users/Gallaecio/following{/other_user}","gists_url":"https://api.github.com/users/Gallaecio/gists{/gist_id}","starred_url":"https://api.github.com/users/Gallaecio/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/Gallaecio/subscriptions","organizations_url":"https://api.github.com/users/Gallaecio/orgs","repos_url":"https://api.github.com/users/Gallaecio/repos","events_url":"https://api.github.com/users/Gallaecio/events{/privacy}","received_events_url":"https://api.github.com/users/Gallaecio/received_events","type":"User","site_admin":false},"labels":[],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":0,"created_at":"2022-03-01T19:18:32Z","updated_at":"2022-03-01T19:18:32Z","closed_at":null,"author_association":"MEMBER","active_lock_reason":null,"draft":true,"pull_request":{"url":"https://api.github.com/repos/scrapy/scrapy/pulls/5430","html_url":"https://github.com/scrapy/scrapy/pull/5430","diff_url":"https://github.com/scrapy/scrapy/pull/5430.diff","patch_url":"https://github.com/scrapy/scrapy/pull/5430.patch","merged_at":null},"body":"Uncomplete work on improvements for cookies. The code itself is probably not very useful, but the hundreds of test scenarios could be useful for any work aiming to improve cookie handling in Scrapy.","reactions":{"url":"https://api.github.com/repos/scrapy/scrapy/issues/5430/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/scrapy/scrapy/issues/5430/timeline","performed_via_github_app":null},{"url":"https://api.github.com/repos/scrapy/scrapy/issues/5426","repository_url":"https://api.github.com/repos/scrapy/scrapy","labels_url":"https://api.github.com/repos/scrapy/scrapy/issues/5426/labels{/name}","comments_url":"https://api.github.com/repos/scrapy/scrapy/issues/5426/comments","events_url":"https://api.github.com/repos/scrapy/scrapy/issues/5426/events","html_url":"https://github.com/scrapy/scrapy/issues/5426","id":1147090354,"node_id":"I_kwDOAAgUXs5EXzWy","number":5426,"title":"Exception handling for start_requests","user":{"login":"wRAR","id":241039,"node_id":"MDQ6VXNlcjI0MTAzOQ==","avatar_url":"https://avatars.githubusercontent.com/u/241039?v=4","gravatar_id":"","url":"https://api.github.com/users/wRAR","html_url":"https://github.com/wRAR","followers_url":"https://api.github.com/users/wRAR/followers","following_url":"https://api.github.com/users/wRAR/following{/other_user}","gists_url":"https://api.github.com/users/wRAR/gists{/gist_id}","starred_url":"https://api.github.com/users/wRAR/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/wRAR/subscriptions","organizations_url":"https://api.github.com/users/wRAR/orgs","repos_url":"https://api.github.com/users/wRAR/repos","events_url":"https://api.github.com/users/wRAR/events{/privacy}","received_events_url":"https://api.github.com/users/wRAR/received_events","type":"User","site_admin":false},"labels":[{"id":14483092,"node_id":"MDU6TGFiZWwxNDQ4MzA5Mg==","url":"https://api.github.com/repos/scrapy/scrapy/labels/enhancement","name":"enhancement","color":"84b6eb","default":true,"description":null},{"id":280218717,"node_id":"MDU6TGFiZWwyODAyMTg3MTc=","url":"https://api.github.com/repos/scrapy/scrapy/labels/discuss","name":"discuss","color":"cc317c","default":false,"description":null}],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":1,"created_at":"2022-02-22T16:02:03Z","updated_at":"2022-03-17T05:00:17Z","closed_at":null,"author_association":"CONTRIBUTOR","active_lock_reason":null,"body":"While working on #4467 (which involves extracting code that calls `start_requests()` from [`Crawler.crawl()`](https://github.com/scrapy/scrapy/blob/3b42ccfebadd72d9b455f6526ab63835b72b1558/scrapy/crawler.py#L95)) I decided to improve handling of exceptions raised when calling `start_requests()` and also explicitly check the return value of it. But I failed to design the flow in a good way so I'm writing this here in case somebody has ideas.\r\n\r\nThe current behavior:\r\n\r\n1. If an exception is raised when *iterating* start_requests(), if it's a generator, that exception is [logged correctly](https://github.com/scrapy/scrapy/blob/3b42ccfebadd72d9b455f6526ab63835b72b1558/scrapy/core/engine.py#L155) and the spider finishes (though it finishes with the \"finished\" status and the exit code 0 which may be wrong).\r\n2. If an exception is raised when *calling* start_requests(), for scrapy crawl you get \"Unhandled error in Deferred\", the exit code 1 and no stats. #2171 is related to this but only covers `scrapy crawl`, catching the exception in the command. For e.g. CrawlerRunner the exception will cause the errback of the crawl() result to run, which depending on the user code will be either handled correctly or cause the same unhandled error exception.\r\n3. If start_requests() returns a bad result, a TypeError \"object is not iterable\" will be raised with the same behavior as in the previous case.\r\n\r\nI would like all these cases to be handled better, with proper \"Error calling start_requests()\" and \"start_requests() returned an object of a wrong type\" log messages, but I'm not sure what behavior do we want. Initially I saw it as \"if an exception happened here, we cannot continue so print a good message and return\" but this is hard to implement considering the \"catch-shutdown the engine-reraise\" flow in crawl() and also may be not the best way to handle this in the context of CrawlerRunner. Or we could always raise some exception with a good message and add catching it in the crawl and runspider commands.","reactions":{"url":"https://api.github.com/repos/scrapy/scrapy/issues/5426/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/scrapy/scrapy/issues/5426/timeline","performed_via_github_app":null},{"url":"https://api.github.com/repos/scrapy/scrapy/issues/5424","repository_url":"https://api.github.com/repos/scrapy/scrapy","labels_url":"https://api.github.com/repos/scrapy/scrapy/issues/5424/labels{/name}","comments_url":"https://api.github.com/repos/scrapy/scrapy/issues/5424/comments","events_url":"https://api.github.com/repos/scrapy/scrapy/issues/5424/events","html_url":"https://github.com/scrapy/scrapy/issues/5424","id":1145631777,"node_id":"I_kwDOAAgUXs5ESPQh","number":5424,"title":"scrapy parse doesn't support async callbacks","user":{"login":"wRAR","id":241039,"node_id":"MDQ6VXNlcjI0MTAzOQ==","avatar_url":"https://avatars.githubusercontent.com/u/241039?v=4","gravatar_id":"","url":"https://api.github.com/users/wRAR","html_url":"https://github.com/wRAR","followers_url":"https://api.github.com/users/wRAR/followers","following_url":"https://api.github.com/users/wRAR/following{/other_user}","gists_url":"https://api.github.com/users/wRAR/gists{/gist_id}","starred_url":"https://api.github.com/users/wRAR/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/wRAR/subscriptions","organizations_url":"https://api.github.com/users/wRAR/orgs","repos_url":"https://api.github.com/users/wRAR/repos","events_url":"https://api.github.com/users/wRAR/events{/privacy}","received_events_url":"https://api.github.com/users/wRAR/received_events","type":"User","site_admin":false},"labels":[{"id":13907246,"node_id":"MDU6TGFiZWwxMzkwNzI0Ng==","url":"https://api.github.com/repos/scrapy/scrapy/labels/bug","name":"bug","color":"fc2929","default":true,"description":null},{"id":3270783977,"node_id":"MDU6TGFiZWwzMjcwNzgzOTc3","url":"https://api.github.com/repos/scrapy/scrapy/labels/asyncio","name":"asyncio","color":"fef2c0","default":false,"description":""}],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":0,"created_at":"2022-02-21T10:50:47Z","updated_at":"2022-02-21T10:51:06Z","closed_at":null,"author_association":"CONTRIBUTOR","active_lock_reason":null,"body":"In master when running `scrapy parse` for a spider with `async def parse` the page is downloaded but then the spider hangs. In #4978  it instead raises `TypeError: 'async_generator' object is not iterable`. Both problems happen because the parse command calls `iterate_spider_output` and doesn't expect a Deferred or an async iterator. ","reactions":{"url":"https://api.github.com/repos/scrapy/scrapy/issues/5424/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/scrapy/scrapy/issues/5424/timeline","performed_via_github_app":null},{"url":"https://api.github.com/repos/scrapy/scrapy/issues/5409","repository_url":"https://api.github.com/repos/scrapy/scrapy","labels_url":"https://api.github.com/repos/scrapy/scrapy/issues/5409/labels{/name}","comments_url":"https://api.github.com/repos/scrapy/scrapy/issues/5409/comments","events_url":"https://api.github.com/repos/scrapy/scrapy/issues/5409/events","html_url":"https://github.com/scrapy/scrapy/issues/5409","id":1130343186,"node_id":"I_kwDOAAgUXs5DX6sS","number":5409,"title":"Stop making network requests in tests","user":{"login":"wRAR","id":241039,"node_id":"MDQ6VXNlcjI0MTAzOQ==","avatar_url":"https://avatars.githubusercontent.com/u/241039?v=4","gravatar_id":"","url":"https://api.github.com/users/wRAR","html_url":"https://github.com/wRAR","followers_url":"https://api.github.com/users/wRAR/followers","following_url":"https://api.github.com/users/wRAR/following{/other_user}","gists_url":"https://api.github.com/users/wRAR/gists{/gist_id}","starred_url":"https://api.github.com/users/wRAR/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/wRAR/subscriptions","organizations_url":"https://api.github.com/users/wRAR/orgs","repos_url":"https://api.github.com/users/wRAR/repos","events_url":"https://api.github.com/users/wRAR/events{/privacy}","received_events_url":"https://api.github.com/users/wRAR/received_events","type":"User","site_admin":false},"labels":[{"id":14483092,"node_id":"MDU6TGFiZWwxNDQ4MzA5Mg==","url":"https://api.github.com/repos/scrapy/scrapy/labels/enhancement","name":"enhancement","color":"84b6eb","default":true,"description":null},{"id":443053983,"node_id":"MDU6TGFiZWw0NDMwNTM5ODM=","url":"https://api.github.com/repos/scrapy/scrapy/labels/CI","name":"CI","color":"d4c5f9","default":false,"description":null}],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":0,"created_at":"2022-02-10T15:46:18Z","updated_at":"2022-02-10T15:46:18Z","closed_at":null,"author_association":"CONTRIBUTOR","active_lock_reason":null,"body":"I ran the test suite as `bwrap --bind / / --dev /dev --unshare-net -- .env/bin/pytest tests`. This allows localhost access for mockserver but not other network access. Only 6 tests in tests/test_command_check.py failed.","reactions":{"url":"https://api.github.com/repos/scrapy/scrapy/issues/5409/reactions","total_count":1,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":1,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/scrapy/scrapy/issues/5409/timeline","performed_via_github_app":null},{"url":"https://api.github.com/repos/scrapy/scrapy/issues/5408","repository_url":"https://api.github.com/repos/scrapy/scrapy","labels_url":"https://api.github.com/repos/scrapy/scrapy/issues/5408/labels{/name}","comments_url":"https://api.github.com/repos/scrapy/scrapy/issues/5408/comments","events_url":"https://api.github.com/repos/scrapy/scrapy/issues/5408/events","html_url":"https://github.com/scrapy/scrapy/issues/5408","id":1129806717,"node_id":"I_kwDOAAgUXs5DV3t9","number":5408,"title":"Google Summer of Code 2022","user":{"login":"Gallaecio","id":705211,"node_id":"MDQ6VXNlcjcwNTIxMQ==","avatar_url":"https://avatars.githubusercontent.com/u/705211?v=4","gravatar_id":"","url":"https://api.github.com/users/Gallaecio","html_url":"https://github.com/Gallaecio","followers_url":"https://api.github.com/users/Gallaecio/followers","following_url":"https://api.github.com/users/Gallaecio/following{/other_user}","gists_url":"https://api.github.com/users/Gallaecio/gists{/gist_id}","starred_url":"https://api.github.com/users/Gallaecio/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/Gallaecio/subscriptions","organizations_url":"https://api.github.com/users/Gallaecio/orgs","repos_url":"https://api.github.com/users/Gallaecio/repos","events_url":"https://api.github.com/users/Gallaecio/events{/privacy}","received_events_url":"https://api.github.com/users/Gallaecio/received_events","type":"User","site_admin":false},"labels":[],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":4,"created_at":"2022-02-10T10:40:12Z","updated_at":"2022-03-07T09:01:36Z","closed_at":null,"author_association":"MEMBER","active_lock_reason":null,"body":"This is a GitHub issue to handle student questions and feedback regarding the 2022 edition of Google Summer of Code, where Scrapy participates again this year: https://gsoc2022.zyte.com/\r\n\r\nPlease, check our [Google Summer of Code communication guidelines](https://gsoc2022.zyte.com/participate#communication) before commenting on this issue.","reactions":{"url":"https://api.github.com/repos/scrapy/scrapy/issues/5408/reactions","total_count":1,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":1,"eyes":0},"timeline_url":"https://api.github.com/repos/scrapy/scrapy/issues/5408/timeline","performed_via_github_app":null},{"url":"https://api.github.com/repos/scrapy/scrapy/issues/5406","repository_url":"https://api.github.com/repos/scrapy/scrapy","labels_url":"https://api.github.com/repos/scrapy/scrapy/issues/5406/labels{/name}","comments_url":"https://api.github.com/repos/scrapy/scrapy/issues/5406/comments","events_url":"https://api.github.com/repos/scrapy/scrapy/issues/5406/events","html_url":"https://github.com/scrapy/scrapy/pull/5406","id":1128987152,"node_id":"PR_kwDOAAgUXs4yUuHI","number":5406,"title":"Fix SMTP STARTTLS for Twisted >= 21.2.0 (5386)","user":{"login":"TobiMayr","id":16239618,"node_id":"MDQ6VXNlcjE2MjM5NjE4","avatar_url":"https://avatars.githubusercontent.com/u/16239618?v=4","gravatar_id":"","url":"https://api.github.com/users/TobiMayr","html_url":"https://github.com/TobiMayr","followers_url":"https://api.github.com/users/TobiMayr/followers","following_url":"https://api.github.com/users/TobiMayr/following{/other_user}","gists_url":"https://api.github.com/users/TobiMayr/gists{/gist_id}","starred_url":"https://api.github.com/users/TobiMayr/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/TobiMayr/subscriptions","organizations_url":"https://api.github.com/users/TobiMayr/orgs","repos_url":"https://api.github.com/users/TobiMayr/repos","events_url":"https://api.github.com/users/TobiMayr/events{/privacy}","received_events_url":"https://api.github.com/users/TobiMayr/received_events","type":"User","site_admin":false},"labels":[],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":20,"created_at":"2022-02-09T20:24:48Z","updated_at":"2022-03-24T13:06:40Z","closed_at":null,"author_association":"NONE","active_lock_reason":null,"draft":false,"pull_request":{"url":"https://api.github.com/repos/scrapy/scrapy/pulls/5406","html_url":"https://github.com/scrapy/scrapy/pull/5406","diff_url":"https://github.com/scrapy/scrapy/pull/5406.diff","patch_url":"https://github.com/scrapy/scrapy/pull/5406.patch","merged_at":null},"body":"Pass hostname to the ESMTPSenderFactory when the twisted version is higher than 21.2.0 to use STARTTLS.\r\n\r\nFixes #5386","reactions":{"url":"https://api.github.com/repos/scrapy/scrapy/issues/5406/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/scrapy/scrapy/issues/5406/timeline","performed_via_github_app":null},{"url":"https://api.github.com/repos/scrapy/scrapy/issues/5403","repository_url":"https://api.github.com/repos/scrapy/scrapy","labels_url":"https://api.github.com/repos/scrapy/scrapy/issues/5403/labels{/name}","comments_url":"https://api.github.com/repos/scrapy/scrapy/issues/5403/comments","events_url":"https://api.github.com/repos/scrapy/scrapy/issues/5403/events","html_url":"https://github.com/scrapy/scrapy/issues/5403","id":1127567732,"node_id":"I_kwDOAAgUXs5DNVF0","number":5403,"title":"Run tox -e docs-links","user":{"login":"Gallaecio","id":705211,"node_id":"MDQ6VXNlcjcwNTIxMQ==","avatar_url":"https://avatars.githubusercontent.com/u/705211?v=4","gravatar_id":"","url":"https://api.github.com/users/Gallaecio","html_url":"https://github.com/Gallaecio","followers_url":"https://api.github.com/users/Gallaecio/followers","following_url":"https://api.github.com/users/Gallaecio/following{/other_user}","gists_url":"https://api.github.com/users/Gallaecio/gists{/gist_id}","starred_url":"https://api.github.com/users/Gallaecio/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/Gallaecio/subscriptions","organizations_url":"https://api.github.com/users/Gallaecio/orgs","repos_url":"https://api.github.com/users/Gallaecio/repos","events_url":"https://api.github.com/users/Gallaecio/events{/privacy}","received_events_url":"https://api.github.com/users/Gallaecio/received_events","type":"User","site_admin":false},"labels":[{"id":14483092,"node_id":"MDU6TGFiZWwxNDQ4MzA5Mg==","url":"https://api.github.com/repos/scrapy/scrapy/labels/enhancement","name":"enhancement","color":"84b6eb","default":true,"description":null},{"id":183224248,"node_id":"MDU6TGFiZWwxODMyMjQyNDg=","url":"https://api.github.com/repos/scrapy/scrapy/labels/docs","name":"docs","color":"bfdadc","default":false,"description":null}],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":0,"created_at":"2022-02-08T17:46:26Z","updated_at":"2022-02-08T17:46:26Z","closed_at":null,"author_association":"MEMBER","active_lock_reason":null,"body":"This Tox environment is meant to detect broken external links in the documentation. We do not run it in CI because it depends on the Internet ti be reliable, so it can fail easily, but we should run it manually now and then to keep track of broken links, and I think we have been postponing the next check for too long.","reactions":{"url":"https://api.github.com/repos/scrapy/scrapy/issues/5403/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/scrapy/scrapy/issues/5403/timeline","performed_via_github_app":null},{"url":"https://api.github.com/repos/scrapy/scrapy/issues/5402","repository_url":"https://api.github.com/repos/scrapy/scrapy","labels_url":"https://api.github.com/repos/scrapy/scrapy/issues/5402/labels{/name}","comments_url":"https://api.github.com/repos/scrapy/scrapy/issues/5402/comments","events_url":"https://api.github.com/repos/scrapy/scrapy/issues/5402/events","html_url":"https://github.com/scrapy/scrapy/issues/5402","id":1127550947,"node_id":"I_kwDOAAgUXs5DNQ_j","number":5402,"title":"API documentation links seem broken","user":{"login":"Gallaecio","id":705211,"node_id":"MDQ6VXNlcjcwNTIxMQ==","avatar_url":"https://avatars.githubusercontent.com/u/705211?v=4","gravatar_id":"","url":"https://api.github.com/users/Gallaecio","html_url":"https://github.com/Gallaecio","followers_url":"https://api.github.com/users/Gallaecio/followers","following_url":"https://api.github.com/users/Gallaecio/following{/other_user}","gists_url":"https://api.github.com/users/Gallaecio/gists{/gist_id}","starred_url":"https://api.github.com/users/Gallaecio/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/Gallaecio/subscriptions","organizations_url":"https://api.github.com/users/Gallaecio/orgs","repos_url":"https://api.github.com/users/Gallaecio/repos","events_url":"https://api.github.com/users/Gallaecio/events{/privacy}","received_events_url":"https://api.github.com/users/Gallaecio/received_events","type":"User","site_admin":false},"labels":[{"id":13907246,"node_id":"MDU6TGFiZWwxMzkwNzI0Ng==","url":"https://api.github.com/repos/scrapy/scrapy/labels/bug","name":"bug","color":"fc2929","default":true,"description":null},{"id":183224248,"node_id":"MDU6TGFiZWwxODMyMjQyNDg=","url":"https://api.github.com/repos/scrapy/scrapy/labels/docs","name":"docs","color":"bfdadc","default":false,"description":null}],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":0,"created_at":"2022-02-08T17:29:58Z","updated_at":"2022-02-08T17:29:58Z","closed_at":null,"author_association":"MEMBER","active_lock_reason":null,"body":"It seems the use of the Sphinx `module` and `currentmodule` directives is corrupting the work made at #5099. If `.. module:: scrapy.http` is followed by `.. class:: scrapy.http.request.form.FormRequest`, link-wise the signature of that class is actually `scrapy.http.scrapy.http.request.form.FormRequest`.\r\n\r\nI am hoping there is a simple way to fix this, but I suspect no. I suspect the only way forward is to remove the module directives, requiring us to use absolute paths for every API reference.","reactions":{"url":"https://api.github.com/repos/scrapy/scrapy/issues/5402/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/scrapy/scrapy/issues/5402/timeline","performed_via_github_app":null},{"url":"https://api.github.com/repos/scrapy/scrapy/issues/5386","repository_url":"https://api.github.com/repos/scrapy/scrapy","labels_url":"https://api.github.com/repos/scrapy/scrapy/issues/5386/labels{/name}","comments_url":"https://api.github.com/repos/scrapy/scrapy/issues/5386/comments","events_url":"https://api.github.com/repos/scrapy/scrapy/issues/5386/events","html_url":"https://github.com/scrapy/scrapy/issues/5386","id":1120895972,"node_id":"I_kwDOAAgUXs5Cz4Pk","number":5386,"title":"Fix SMTP STARTTLS for Twisted >= 21.2.0","user":{"login":"TobiMayr","id":16239618,"node_id":"MDQ6VXNlcjE2MjM5NjE4","avatar_url":"https://avatars.githubusercontent.com/u/16239618?v=4","gravatar_id":"","url":"https://api.github.com/users/TobiMayr","html_url":"https://github.com/TobiMayr","followers_url":"https://api.github.com/users/TobiMayr/followers","following_url":"https://api.github.com/users/TobiMayr/following{/other_user}","gists_url":"https://api.github.com/users/TobiMayr/gists{/gist_id}","starred_url":"https://api.github.com/users/TobiMayr/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/TobiMayr/subscriptions","organizations_url":"https://api.github.com/users/TobiMayr/orgs","repos_url":"https://api.github.com/users/TobiMayr/repos","events_url":"https://api.github.com/users/TobiMayr/events{/privacy}","received_events_url":"https://api.github.com/users/TobiMayr/received_events","type":"User","site_admin":false},"labels":[{"id":13907246,"node_id":"MDU6TGFiZWwxMzkwNzI0Ng==","url":"https://api.github.com/repos/scrapy/scrapy/labels/bug","name":"bug","color":"fc2929","default":true,"description":null},{"id":80417179,"node_id":"MDU6TGFiZWw4MDQxNzE3OQ==","url":"https://api.github.com/repos/scrapy/scrapy/labels/good%20first%20issue","name":"good first issue","color":"bfe5bf","default":true,"description":null}],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":12,"created_at":"2022-02-01T16:09:00Z","updated_at":"2022-03-24T06:02:01Z","closed_at":null,"author_association":"NONE","active_lock_reason":null,"body":"## Summary\r\n\r\nThe [Mail settings](https://docs.scrapy.org/en/latest/topics/email.html#topics-email-settings) don't have an option to choose a TLS version. Only to enforce upgrading connections to use SSL/TLS.\r\nMail servers like smtp.office365.com dropped support for TLS1.0 and TLS1.1 and now require TLS1.2: https://techcommunity.microsoft.com/t5/exchange-team-blog/new-opt-in-endpoint-available-for-smtp-auth-clients-still/ba-p/2659652 \r\n\r\nIt seems that scrapy mail doesn't support TLS1.2. The error message (with `MAIL_TLS = True`):\r\n\r\n`[scrapy.mail] Unable to send mail: To=['user@gmail.com'] Cc=[] Subject=\"Test\" Attachs=0- 421 b'4.7.66 TLS 1.0 and 1.1 are not supported. Please upgrade/update your client to support TLS 1.2. Visit https://aka.ms/smtp_auth_tls. [AM6P194CA0047.EURP194.PROD.OUTLOOK.COM]'` \r\n\r\n## Motivation\r\n\r\nWithout TLS1.2 it's not possible anymore to send mails via smtp.office365.com. An option to use TLS1.2 would fix this issue\r\n","reactions":{"url":"https://api.github.com/repos/scrapy/scrapy/issues/5386/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/scrapy/scrapy/issues/5386/timeline","performed_via_github_app":null},{"url":"https://api.github.com/repos/scrapy/scrapy/issues/5381","repository_url":"https://api.github.com/repos/scrapy/scrapy","labels_url":"https://api.github.com/repos/scrapy/scrapy/issues/5381/labels{/name}","comments_url":"https://api.github.com/repos/scrapy/scrapy/issues/5381/comments","events_url":"https://api.github.com/repos/scrapy/scrapy/issues/5381/events","html_url":"https://github.com/scrapy/scrapy/issues/5381","id":1117450315,"node_id":"I_kwDOAAgUXs5CmvBL","number":5381,"title":"Possible import name clash between project directory and installed *.py executables","user":{"login":"woho","id":829649,"node_id":"MDQ6VXNlcjgyOTY0OQ==","avatar_url":"https://avatars.githubusercontent.com/u/829649?v=4","gravatar_id":"","url":"https://api.github.com/users/woho","html_url":"https://github.com/woho","followers_url":"https://api.github.com/users/woho/followers","following_url":"https://api.github.com/users/woho/following{/other_user}","gists_url":"https://api.github.com/users/woho/gists{/gist_id}","starred_url":"https://api.github.com/users/woho/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/woho/subscriptions","organizations_url":"https://api.github.com/users/woho/orgs","repos_url":"https://api.github.com/users/woho/repos","events_url":"https://api.github.com/users/woho/events{/privacy}","received_events_url":"https://api.github.com/users/woho/received_events","type":"User","site_admin":false},"labels":[{"id":443680419,"node_id":"MDU6TGFiZWw0NDM2ODA0MTk=","url":"https://api.github.com/repos/scrapy/scrapy/labels/needs%20more%20info","name":"needs more info","color":"fef2c0","default":false,"description":null}],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":7,"created_at":"2022-01-28T14:10:21Z","updated_at":"2022-02-06T23:30:20Z","closed_at":null,"author_association":"NONE","active_lock_reason":null,"body":"Python's import system has one weird aspect that make it vulnerable to import name clashes between modules in the working directory and in the directory of the main entry point. This affects Scrapy's import mechanism.\r\n\r\nMinimal example:\r\n```bash\r\ntouch $(dirname `which scrapy`)/myproject.py   # Could be some file from another program\r\n\r\nscrapy startproject myproject\r\ncd myproject\r\nscrapy genspider example example.com\r\nscrapy crawl example\r\n```\r\n... fails with: `ModuleNotFoundError: No module named 'myproject.settings'; 'myproject' is not a package`\r\n\r\n## Possible solution\r\n\r\nJust remove the problematic folder from `sys.path`, i.e. add `sys.path.remove(os.path.dirname(__file__))` to the Scrapy executable (usually `/usr/bin/scrapy` or `/usr/local/bin/scrapy`).","reactions":{"url":"https://api.github.com/repos/scrapy/scrapy/issues/5381/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/scrapy/scrapy/issues/5381/timeline","performed_via_github_app":null},{"url":"https://api.github.com/repos/scrapy/scrapy/issues/5379","repository_url":"https://api.github.com/repos/scrapy/scrapy","labels_url":"https://api.github.com/repos/scrapy/scrapy/issues/5379/labels{/name}","comments_url":"https://api.github.com/repos/scrapy/scrapy/issues/5379/comments","events_url":"https://api.github.com/repos/scrapy/scrapy/issues/5379/events","html_url":"https://github.com/scrapy/scrapy/issues/5379","id":1114695727,"node_id":"I_kwDOAAgUXs5CcOgv","number":5379,"title":"Proxy + invalid domain makes Downloader stuck","user":{"login":"vryazanov","id":2764703,"node_id":"MDQ6VXNlcjI3NjQ3MDM=","avatar_url":"https://avatars.githubusercontent.com/u/2764703?v=4","gravatar_id":"","url":"https://api.github.com/users/vryazanov","html_url":"https://github.com/vryazanov","followers_url":"https://api.github.com/users/vryazanov/followers","following_url":"https://api.github.com/users/vryazanov/following{/other_user}","gists_url":"https://api.github.com/users/vryazanov/gists{/gist_id}","starred_url":"https://api.github.com/users/vryazanov/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/vryazanov/subscriptions","organizations_url":"https://api.github.com/users/vryazanov/orgs","repos_url":"https://api.github.com/users/vryazanov/repos","events_url":"https://api.github.com/users/vryazanov/events{/privacy}","received_events_url":"https://api.github.com/users/vryazanov/received_events","type":"User","site_admin":false},"labels":[{"id":13907246,"node_id":"MDU6TGFiZWwxMzkwNzI0Ng==","url":"https://api.github.com/repos/scrapy/scrapy/labels/bug","name":"bug","color":"fc2929","default":true,"description":null}],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":0,"created_at":"2022-01-26T07:06:54Z","updated_at":"2022-01-26T12:57:26Z","closed_at":null,"author_association":"NONE","active_lock_reason":null,"body":"<!--\r\n\r\nThanks for taking an interest in Scrapy!\r\n\r\nIf you have a question that starts with \"How to...\", please see the Scrapy Community page: https://scrapy.org/community/.\r\nThe GitHub issue tracker's purpose is to deal with bug reports and feature requests for the project itself.\r\n\r\nKeep in mind that by filing an issue, you are expected to comply with Scrapy's Code of Conduct, including treating everyone with respect: https://github.com/scrapy/scrapy/blob/master/CODE_OF_CONDUCT.md\r\n\r\nThe following is a suggested template to structure your issue, you can find more guidelines at https://doc.scrapy.org/en/latest/contributing.html#reporting-bugs\r\n\r\n-->\r\n\r\n### Description\r\n\r\nDownloader gets stuck when trying to download a url having not valid domain. It works good, but without proxy.\r\n\r\n### Steps to Reproduce\r\n\r\n1. Set proxy\r\n2. Try to crawl any invalid domain, for example https://text_example.scrapy.com\r\n\r\n**Expected behavior:** a request leaves downloader\r\n**Actual behavior:** a request do not leave downloader and it gets stuck\r\n**Reproduces how often:** 100%\r\n### Versions\r\n\r\nScrapy       : 2.5.0\r\nlxml         : 4.6.3.0\r\nlibxml2      : 2.9.10\r\ncssselect    : 1.1.0\r\nparsel       : 1.6.0\r\nw3lib        : 1.22.0\r\nTwisted      : 21.2.0\r\nPython       : 3.8.9 (default, Apr 10 2021, 15:55:09) - [GCC 8.3.0]\r\npyOpenSSL    : 20.0.1 (OpenSSL 1.1.1k  25 Mar 2021)\r\ncryptography : 3.4.7\r\nPlatform     : Linux-5.10.15-200.fc33.x86_64-x86_64-with-glibc2.2.5\r\n\r\n### Additional context\r\nThe problem is in this part from `scrapy.core.downloader.handlers.http11TunnelingTCP4ClientEndpoint`. `creatorForNetloc` raises an exception which is not handled by twisted.\r\n```python\r\n    def processProxyResponse(self, rcvd_bytes):\r\n        \"\"\"Processes the response from the proxy. If the tunnel is successfully\r\n        created, notifies the client that we are ready to send requests. If not\r\n        raises a TunnelError.\r\n        \"\"\"\r\n        self._connectBuffer += rcvd_bytes\r\n        # make sure that enough (all) bytes are consumed\r\n        # and that we've got all HTTP headers (ending with a blank line)\r\n        # from the proxy so that we don't send those bytes to the TLS layer\r\n        #\r\n        # see https://github.com/scrapy/scrapy/issues/2491\r\n        if b'\\r\\n\\r\\n' not in self._connectBuffer:\r\n            return\r\n        self._protocol.dataReceived = self._protocolDataReceived\r\n        respm = TunnelingTCP4ClientEndpoint._responseMatcher.match(self._connectBuffer)\r\n        if respm and int(respm.group('status')) == 200:\r\n            # set proper Server Name Indication extension\r\n            sslOptions = self._contextFactory.creatorForNetloc(self._tunneledHost, self._tunneledPort)\r\n            self._protocol.transport.startTLS(sslOptions, self._protocolFactory)\r\n            self._tunnelReadyDeferred.callback(self._protocol)\r\n```\r\n\r\n\r\n### Traceback\r\n```python\r\nUnhandled Error\r\nTraceback (most recent call last):\r\n  File \"/usr/local/lib/python3.8/site-packages/twisted/python/log.py\", line 101, in callWithLogger\r\n    return callWithContext({\"system\": lp}, func, *args, **kw)\r\n  File \"/usr/local/lib/python3.8/site-packages/twisted/python/log.py\", line 85, in callWithContext\r\n    return context.call({ILogContext: newCtx}, func, *args, **kw)\r\n  File \"/usr/local/lib/python3.8/site-packages/twisted/python/context.py\", line 118, in callWithContext\r\n    return self.currentContext().callWithContext(ctx, func, *args, **kw)\r\n  File \"/usr/local/lib/python3.8/site-packages/twisted/python/context.py\", line 83, in callWithContext\r\n    return func(*args, **kw)\r\n--- <exception caught here> ---\r\n  File \"/usr/local/lib/python3.8/site-packages/twisted/internet/posixbase.py\", line 687, in _doReadOrWrite\r\n    why = selectable.doRead()\r\n  File \"/usr/local/lib/python3.8/site-packages/twisted/internet/tcp.py\", line 246, in doRead\r\n    return self._dataReceived(data)\r\n  File \"/usr/local/lib/python3.8/site-packages/twisted/internet/tcp.py\", line 251, in _dataReceived\r\n    rval = self.protocol.dataReceived(data)\r\n  File \"/usr/local/lib/python3.8/site-packages/twisted/internet/endpoints.py\", line 149, in dataReceived\r\n    return self._wrappedProtocol.dataReceived(data)\r\n  File \"/usr/local/lib/python3.8/site-packages/scrapy/core/downloader/handlers/http11.py\", line 139, in processProxyResponse\r\n    sslOptions = self._contextFactory.creatorForNetloc(self._tunneledHost, self._tunneledPort)\r\n  File \"/application/crawler/contextfactory.py\", line 53, in creatorForNetloc\r\n    return super().creatorForNetloc(hostname, port)\r\n  File \"/usr/local/lib/python3.8/site-packages/scrapy/core/downloader/contextfactory.py\", line 67, in creatorForNetloc\r\n    return ScrapyClientTLSOptions(hostname.decode(\"ascii\"), self.getContext(),\r\n  File \"/usr/local/lib/python3.8/site-packages/scrapy/core/downloader/tls.py\", line 42, in __init__\r\n    super().__init__(hostname, ctx)\r\n  File \"/usr/local/lib/python3.8/site-packages/twisted/internet/_sslverify.py\", line 1130, in __init__\r\n    self._hostnameBytes = _idnaBytes(hostname)\r\n  File \"/usr/local/lib/python3.8/site-packages/twisted/internet/_idna.py\", line 31, in _idnaBytes\r\n    return idna.encode(text)\r\n  File \"/usr/local/lib/python3.8/site-packages/idna/core.py\", line 362, in encode\r\n    s = alabel(label)\r\n  File \"/usr/local/lib/python3.8/site-packages/idna/core.py\", line 270, in alabel\r\n    ulabel(label)\r\n  File \"/usr/local/lib/python3.8/site-packages/idna/core.py\", line 308, in ulabel\r\n    check_label(label)\r\n  File \"/usr/local/lib/python3.8/site-packages/idna/core.py\", line 261, in check_label\r\n    raise InvalidCodepoint('Codepoint {0} at position {1} of {2} not allowed'.format(_unot(cp_value), pos+1, repr(label)))\r\nidna.core.InvalidCodepoint: Codepoint U+005F at position 10 of 'resilient_test' not allowed\r\n\r\n```","reactions":{"url":"https://api.github.com/repos/scrapy/scrapy/issues/5379/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/scrapy/scrapy/issues/5379/timeline","performed_via_github_app":null},{"url":"https://api.github.com/repos/scrapy/scrapy/issues/5376","repository_url":"https://api.github.com/repos/scrapy/scrapy","labels_url":"https://api.github.com/repos/scrapy/scrapy/issues/5376/labels{/name}","comments_url":"https://api.github.com/repos/scrapy/scrapy/issues/5376/comments","events_url":"https://api.github.com/repos/scrapy/scrapy/issues/5376/events","html_url":"https://github.com/scrapy/scrapy/pull/5376","id":1111369854,"node_id":"PR_kwDOAAgUXs4xboYK","number":5376,"title":"Fix command parse unhandled error :AttributeError: 'NoneType' object has no attribute 'start_requests'(#3264)","user":{"login":"rahul-sambyal","id":24474979,"node_id":"MDQ6VXNlcjI0NDc0OTc5","avatar_url":"https://avatars.githubusercontent.com/u/24474979?v=4","gravatar_id":"","url":"https://api.github.com/users/rahul-sambyal","html_url":"https://github.com/rahul-sambyal","followers_url":"https://api.github.com/users/rahul-sambyal/followers","following_url":"https://api.github.com/users/rahul-sambyal/following{/other_user}","gists_url":"https://api.github.com/users/rahul-sambyal/gists{/gist_id}","starred_url":"https://api.github.com/users/rahul-sambyal/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/rahul-sambyal/subscriptions","organizations_url":"https://api.github.com/users/rahul-sambyal/orgs","repos_url":"https://api.github.com/users/rahul-sambyal/repos","events_url":"https://api.github.com/users/rahul-sambyal/events{/privacy}","received_events_url":"https://api.github.com/users/rahul-sambyal/received_events","type":"User","site_admin":false},"labels":[],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":4,"created_at":"2022-01-22T09:12:15Z","updated_at":"2022-02-15T08:35:30Z","closed_at":null,"author_association":"NONE","active_lock_reason":null,"draft":false,"pull_request":{"url":"https://api.github.com/repos/scrapy/scrapy/pulls/5376","html_url":"https://github.com/scrapy/scrapy/pull/5376","diff_url":"https://github.com/scrapy/scrapy/pull/5376.diff","patch_url":"https://github.com/scrapy/scrapy/pull/5376.patch","merged_at":null},"body":"Reopening @wangrenlei's PR \r\nFixes #3264","reactions":{"url":"https://api.github.com/repos/scrapy/scrapy/issues/5376/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/scrapy/scrapy/issues/5376/timeline","performed_via_github_app":null},{"url":"https://api.github.com/repos/scrapy/scrapy/issues/5372","repository_url":"https://api.github.com/repos/scrapy/scrapy","labels_url":"https://api.github.com/repos/scrapy/scrapy/issues/5372/labels{/name}","comments_url":"https://api.github.com/repos/scrapy/scrapy/issues/5372/comments","events_url":"https://api.github.com/repos/scrapy/scrapy/issues/5372/events","html_url":"https://github.com/scrapy/scrapy/pull/5372","id":1110849286,"node_id":"PR_kwDOAAgUXs4xZ1D9","number":5372,"title":"deprecate scrapy.pipelines.images.NoimagesDrop","user":{"login":"GhostViz","id":1735406,"node_id":"MDQ6VXNlcjE3MzU0MDY=","avatar_url":"https://avatars.githubusercontent.com/u/1735406?v=4","gravatar_id":"","url":"https://api.github.com/users/GhostViz","html_url":"https://github.com/GhostViz","followers_url":"https://api.github.com/users/GhostViz/followers","following_url":"https://api.github.com/users/GhostViz/following{/other_user}","gists_url":"https://api.github.com/users/GhostViz/gists{/gist_id}","starred_url":"https://api.github.com/users/GhostViz/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/GhostViz/subscriptions","organizations_url":"https://api.github.com/users/GhostViz/orgs","repos_url":"https://api.github.com/users/GhostViz/repos","events_url":"https://api.github.com/users/GhostViz/events{/privacy}","received_events_url":"https://api.github.com/users/GhostViz/received_events","type":"User","site_admin":false},"labels":[],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":1,"created_at":"2022-01-21T19:54:22Z","updated_at":"2022-02-18T09:32:42Z","closed_at":null,"author_association":"NONE","active_lock_reason":null,"draft":false,"pull_request":{"url":"https://api.github.com/repos/scrapy/scrapy/pulls/5372","html_url":"https://github.com/scrapy/scrapy/pull/5372","diff_url":"https://github.com/scrapy/scrapy/pull/5372.diff","patch_url":"https://github.com/scrapy/scrapy/pull/5372.patch","merged_at":null},"body":"Deprecate scrapy.pipelines.images.NoimagesDrop \r\n\r\nResolved #5368 ","reactions":{"url":"https://api.github.com/repos/scrapy/scrapy/issues/5372/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/scrapy/scrapy/issues/5372/timeline","performed_via_github_app":null},{"url":"https://api.github.com/repos/scrapy/scrapy/issues/5368","repository_url":"https://api.github.com/repos/scrapy/scrapy","labels_url":"https://api.github.com/repos/scrapy/scrapy/issues/5368/labels{/name}","comments_url":"https://api.github.com/repos/scrapy/scrapy/issues/5368/comments","events_url":"https://api.github.com/repos/scrapy/scrapy/issues/5368/events","html_url":"https://github.com/scrapy/scrapy/issues/5368","id":1110180159,"node_id":"I_kwDOAAgUXs5CLAE_","number":5368,"title":"Deprecate scrapy.pipelines.images.NoimagesDrop","user":{"login":"wRAR","id":241039,"node_id":"MDQ6VXNlcjI0MTAzOQ==","avatar_url":"https://avatars.githubusercontent.com/u/241039?v=4","gravatar_id":"","url":"https://api.github.com/users/wRAR","html_url":"https://github.com/wRAR","followers_url":"https://api.github.com/users/wRAR/followers","following_url":"https://api.github.com/users/wRAR/following{/other_user}","gists_url":"https://api.github.com/users/wRAR/gists{/gist_id}","starred_url":"https://api.github.com/users/wRAR/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/wRAR/subscriptions","organizations_url":"https://api.github.com/users/wRAR/orgs","repos_url":"https://api.github.com/users/wRAR/repos","events_url":"https://api.github.com/users/wRAR/events{/privacy}","received_events_url":"https://api.github.com/users/wRAR/received_events","type":"User","site_admin":false},"labels":[{"id":80417179,"node_id":"MDU6TGFiZWw4MDQxNzE3OQ==","url":"https://api.github.com/repos/scrapy/scrapy/labels/good%20first%20issue","name":"good first issue","color":"bfe5bf","default":true,"description":null},{"id":545314542,"node_id":"MDU6TGFiZWw1NDUzMTQ1NDI=","url":"https://api.github.com/repos/scrapy/scrapy/labels/cleanup","name":"cleanup","color":"1d76db","default":false,"description":null}],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":0,"created_at":"2022-01-21T08:16:59Z","updated_at":"2022-01-21T08:16:59Z","closed_at":null,"author_association":"CONTRIBUTOR","active_lock_reason":null,"body":"As noticed in #4910, `scrapy.pipelines.images.NoimagesDrop` is not used in the Scrapy code so we should deprecate (and later remove) it.","reactions":{"url":"https://api.github.com/repos/scrapy/scrapy/issues/5368/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/scrapy/scrapy/issues/5368/timeline","performed_via_github_app":null},{"url":"https://api.github.com/repos/scrapy/scrapy/issues/5365","repository_url":"https://api.github.com/repos/scrapy/scrapy","labels_url":"https://api.github.com/repos/scrapy/scrapy/issues/5365/labels{/name}","comments_url":"https://api.github.com/repos/scrapy/scrapy/issues/5365/comments","events_url":"https://api.github.com/repos/scrapy/scrapy/issues/5365/events","html_url":"https://github.com/scrapy/scrapy/pull/5365","id":1095982820,"node_id":"PR_kwDOAAgUXs4wpI1B","number":5365,"title":"Create SECURITY.md","user":{"login":"JamieSlome","id":55323451,"node_id":"MDQ6VXNlcjU1MzIzNDUx","avatar_url":"https://avatars.githubusercontent.com/u/55323451?v=4","gravatar_id":"","url":"https://api.github.com/users/JamieSlome","html_url":"https://github.com/JamieSlome","followers_url":"https://api.github.com/users/JamieSlome/followers","following_url":"https://api.github.com/users/JamieSlome/following{/other_user}","gists_url":"https://api.github.com/users/JamieSlome/gists{/gist_id}","starred_url":"https://api.github.com/users/JamieSlome/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/JamieSlome/subscriptions","organizations_url":"https://api.github.com/users/JamieSlome/orgs","repos_url":"https://api.github.com/users/JamieSlome/repos","events_url":"https://api.github.com/users/JamieSlome/events{/privacy}","received_events_url":"https://api.github.com/users/JamieSlome/received_events","type":"User","site_admin":false},"labels":[],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":2,"created_at":"2022-01-07T04:52:18Z","updated_at":"2022-03-17T12:55:34Z","closed_at":null,"author_association":"NONE","active_lock_reason":null,"draft":false,"pull_request":{"url":"https://api.github.com/repos/scrapy/scrapy/pulls/5365","html_url":"https://github.com/scrapy/scrapy/pull/5365","diff_url":"https://github.com/scrapy/scrapy/pull/5365.diff","patch_url":"https://github.com/scrapy/scrapy/pull/5365.patch","merged_at":null},"body":"To let the repository confirm scrapy-security@googlegroups.com as its security contact.\r\n\r\nResolves #5364","reactions":{"url":"https://api.github.com/repos/scrapy/scrapy/issues/5365/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/scrapy/scrapy/issues/5365/timeline","performed_via_github_app":null},{"url":"https://api.github.com/repos/scrapy/scrapy/issues/5364","repository_url":"https://api.github.com/repos/scrapy/scrapy","labels_url":"https://api.github.com/repos/scrapy/scrapy/issues/5364/labels{/name}","comments_url":"https://api.github.com/repos/scrapy/scrapy/issues/5364/comments","events_url":"https://api.github.com/repos/scrapy/scrapy/issues/5364/events","html_url":"https://github.com/scrapy/scrapy/issues/5364","id":1095194036,"node_id":"I_kwDOAAgUXs5BR1W0","number":5364,"title":"Add SECURITY.md","user":{"login":"JamieSlome","id":55323451,"node_id":"MDQ6VXNlcjU1MzIzNDUx","avatar_url":"https://avatars.githubusercontent.com/u/55323451?v=4","gravatar_id":"","url":"https://api.github.com/users/JamieSlome","html_url":"https://github.com/JamieSlome","followers_url":"https://api.github.com/users/JamieSlome/followers","following_url":"https://api.github.com/users/JamieSlome/following{/other_user}","gists_url":"https://api.github.com/users/JamieSlome/gists{/gist_id}","starred_url":"https://api.github.com/users/JamieSlome/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/JamieSlome/subscriptions","organizations_url":"https://api.github.com/users/JamieSlome/orgs","repos_url":"https://api.github.com/users/JamieSlome/repos","events_url":"https://api.github.com/users/JamieSlome/events{/privacy}","received_events_url":"https://api.github.com/users/JamieSlome/received_events","type":"User","site_admin":false},"labels":[],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":2,"created_at":"2022-01-06T10:47:02Z","updated_at":"2022-01-07T14:14:02Z","closed_at":null,"author_association":"NONE","active_lock_reason":null,"body":"Hey there!\n\nI belong to an open source security research community, and a member (@ranjit-git) has found an issue, but doesn’t know the best way to disclose it.\n\nIf not a hassle, might you kindly add a `SECURITY.md` file with an email, or another contact method? GitHub [recommends](https://docs.github.com/en/code-security/getting-started/adding-a-security-policy-to-your-repository) this best practice to ensure security issues are responsibly disclosed, and it would serve as a simple instruction for security researchers in the future.\n\nThank you for your consideration, and I look forward to hearing from you!\n\n(cc @huntr-helper)","reactions":{"url":"https://api.github.com/repos/scrapy/scrapy/issues/5364/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/scrapy/scrapy/issues/5364/timeline","performed_via_github_app":null}]