[{"url":"https://api.github.com/repos/google-research/bert/issues/1302","repository_url":"https://api.github.com/repos/google-research/bert","labels_url":"https://api.github.com/repos/google-research/bert/issues/1302/labels{/name}","comments_url":"https://api.github.com/repos/google-research/bert/issues/1302/comments","events_url":"https://api.github.com/repos/google-research/bert/issues/1302/events","html_url":"https://github.com/google-research/bert/issues/1302","id":1171847101,"node_id":"I_kwDOCTlCuc5F2Pe9","number":1302,"title":"Degraded bookcorpus link at README.md ","user":{"login":"teohsinyee","id":56106630,"node_id":"MDQ6VXNlcjU2MTA2NjMw","avatar_url":"https://avatars.githubusercontent.com/u/56106630?v=4","gravatar_id":"","url":"https://api.github.com/users/teohsinyee","html_url":"https://github.com/teohsinyee","followers_url":"https://api.github.com/users/teohsinyee/followers","following_url":"https://api.github.com/users/teohsinyee/following{/other_user}","gists_url":"https://api.github.com/users/teohsinyee/gists{/gist_id}","starred_url":"https://api.github.com/users/teohsinyee/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/teohsinyee/subscriptions","organizations_url":"https://api.github.com/users/teohsinyee/orgs","repos_url":"https://api.github.com/users/teohsinyee/repos","events_url":"https://api.github.com/users/teohsinyee/events{/privacy}","received_events_url":"https://api.github.com/users/teohsinyee/received_events","type":"User","site_admin":false},"labels":[],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":1,"created_at":"2022-03-17T02:15:08Z","updated_at":"2022-03-26T16:14:30Z","closed_at":null,"author_association":"NONE","active_lock_reason":null,"body":"Refer to README.md, search for BookCorpus & click on the link. It will redirect you to [here](https://yknzhu.wixsite.com/mbweb) & you will notice BookCorpus is no longer available from the original authors.\r\n<img width=\"674\" alt=\"image\" src=\"https://user-images.githubusercontent.com/56106630/158722360-596527d4-f0b6-415a-ad1a-65611acc14b2.png\">\r\n","reactions":{"url":"https://api.github.com/repos/google-research/bert/issues/1302/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/google-research/bert/issues/1302/timeline","performed_via_github_app":null},{"url":"https://api.github.com/repos/google-research/bert/issues/1301","repository_url":"https://api.github.com/repos/google-research/bert","labels_url":"https://api.github.com/repos/google-research/bert/issues/1301/labels{/name}","comments_url":"https://api.github.com/repos/google-research/bert/issues/1301/comments","events_url":"https://api.github.com/repos/google-research/bert/issues/1301/events","html_url":"https://github.com/google-research/bert/pull/1301","id":1151253042,"node_id":"PR_kwDOCTlCuc4zgccE","number":1301,"title":"Update run_squad.py","user":{"login":"sherlcok314159","id":76043326,"node_id":"MDQ6VXNlcjc2MDQzMzI2","avatar_url":"https://avatars.githubusercontent.com/u/76043326?v=4","gravatar_id":"","url":"https://api.github.com/users/sherlcok314159","html_url":"https://github.com/sherlcok314159","followers_url":"https://api.github.com/users/sherlcok314159/followers","following_url":"https://api.github.com/users/sherlcok314159/following{/other_user}","gists_url":"https://api.github.com/users/sherlcok314159/gists{/gist_id}","starred_url":"https://api.github.com/users/sherlcok314159/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/sherlcok314159/subscriptions","organizations_url":"https://api.github.com/users/sherlcok314159/orgs","repos_url":"https://api.github.com/users/sherlcok314159/repos","events_url":"https://api.github.com/users/sherlcok314159/events{/privacy}","received_events_url":"https://api.github.com/users/sherlcok314159/received_events","type":"User","site_admin":false},"labels":[],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":1,"created_at":"2022-02-26T05:40:24Z","updated_at":"2022-02-26T05:40:28Z","closed_at":null,"author_association":"NONE","active_lock_reason":null,"draft":false,"pull_request":{"url":"https://api.github.com/repos/google-research/bert/pulls/1301","html_url":"https://github.com/google-research/bert/pull/1301","diff_url":"https://github.com/google-research/bert/pull/1301.diff","patch_url":"https://github.com/google-research/bert/pull/1301.patch","merged_at":null},"body":"The __repr___() method seems to repeat the statement ` if self.start_position`.","reactions":{"url":"https://api.github.com/repos/google-research/bert/issues/1301/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/google-research/bert/issues/1301/timeline","performed_via_github_app":null},{"url":"https://api.github.com/repos/google-research/bert/issues/1300","repository_url":"https://api.github.com/repos/google-research/bert","labels_url":"https://api.github.com/repos/google-research/bert/issues/1300/labels{/name}","comments_url":"https://api.github.com/repos/google-research/bert/issues/1300/comments","events_url":"https://api.github.com/repos/google-research/bert/issues/1300/events","html_url":"https://github.com/google-research/bert/pull/1300","id":1145769812,"node_id":"PR_kwDOCTlCuc4zOh3B","number":1300,"title":"I corrected some grammatically incorrect and ambiguous statements.","user":{"login":"AkashKumarSingh11032001","id":55061520,"node_id":"MDQ6VXNlcjU1MDYxNTIw","avatar_url":"https://avatars.githubusercontent.com/u/55061520?v=4","gravatar_id":"","url":"https://api.github.com/users/AkashKumarSingh11032001","html_url":"https://github.com/AkashKumarSingh11032001","followers_url":"https://api.github.com/users/AkashKumarSingh11032001/followers","following_url":"https://api.github.com/users/AkashKumarSingh11032001/following{/other_user}","gists_url":"https://api.github.com/users/AkashKumarSingh11032001/gists{/gist_id}","starred_url":"https://api.github.com/users/AkashKumarSingh11032001/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/AkashKumarSingh11032001/subscriptions","organizations_url":"https://api.github.com/users/AkashKumarSingh11032001/orgs","repos_url":"https://api.github.com/users/AkashKumarSingh11032001/repos","events_url":"https://api.github.com/users/AkashKumarSingh11032001/events{/privacy}","received_events_url":"https://api.github.com/users/AkashKumarSingh11032001/received_events","type":"User","site_admin":false},"labels":[],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":0,"created_at":"2022-02-21T13:02:02Z","updated_at":"2022-02-21T13:02:02Z","closed_at":null,"author_association":"NONE","active_lock_reason":null,"draft":false,"pull_request":{"url":"https://api.github.com/repos/google-research/bert/pulls/1300","html_url":"https://github.com/google-research/bert/pull/1300","diff_url":"https://github.com/google-research/bert/pull/1300.diff","patch_url":"https://github.com/google-research/bert/pull/1300.patch","merged_at":null},"body":null,"reactions":{"url":"https://api.github.com/repos/google-research/bert/issues/1300/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/google-research/bert/issues/1300/timeline","performed_via_github_app":null},{"url":"https://api.github.com/repos/google-research/bert/issues/1299","repository_url":"https://api.github.com/repos/google-research/bert","labels_url":"https://api.github.com/repos/google-research/bert/issues/1299/labels{/name}","comments_url":"https://api.github.com/repos/google-research/bert/issues/1299/comments","events_url":"https://api.github.com/repos/google-research/bert/issues/1299/events","html_url":"https://github.com/google-research/bert/issues/1299","id":1145321076,"node_id":"I_kwDOCTlCuc5ERDZ0","number":1299,"title":"Does LSTM better than BERT ? ","user":{"login":"SamMohel","id":29579092,"node_id":"MDQ6VXNlcjI5NTc5MDky","avatar_url":"https://avatars.githubusercontent.com/u/29579092?v=4","gravatar_id":"","url":"https://api.github.com/users/SamMohel","html_url":"https://github.com/SamMohel","followers_url":"https://api.github.com/users/SamMohel/followers","following_url":"https://api.github.com/users/SamMohel/following{/other_user}","gists_url":"https://api.github.com/users/SamMohel/gists{/gist_id}","starred_url":"https://api.github.com/users/SamMohel/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/SamMohel/subscriptions","organizations_url":"https://api.github.com/users/SamMohel/orgs","repos_url":"https://api.github.com/users/SamMohel/repos","events_url":"https://api.github.com/users/SamMohel/events{/privacy}","received_events_url":"https://api.github.com/users/SamMohel/received_events","type":"User","site_admin":false},"labels":[],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":1,"created_at":"2022-02-21T04:53:19Z","updated_at":"2022-02-27T10:13:21Z","closed_at":null,"author_association":"NONE","active_lock_reason":null,"body":"does LSTM is better than BERT ? as i run the BERT and LSTM with the same data but got accuracy with LSTM is better than BERT. does the dataset size make effect and if s, what is the range if dataset that make BERT better than LSTM ","reactions":{"url":"https://api.github.com/repos/google-research/bert/issues/1299/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/google-research/bert/issues/1299/timeline","performed_via_github_app":null},{"url":"https://api.github.com/repos/google-research/bert/issues/1296","repository_url":"https://api.github.com/repos/google-research/bert","labels_url":"https://api.github.com/repos/google-research/bert/issues/1296/labels{/name}","comments_url":"https://api.github.com/repos/google-research/bert/issues/1296/comments","events_url":"https://api.github.com/repos/google-research/bert/issues/1296/events","html_url":"https://github.com/google-research/bert/issues/1296","id":1141438774,"node_id":"I_kwDOCTlCuc5ECPk2","number":1296,"title":"multilingual bert-training size","user":{"login":"IreneSucameli","id":48584598,"node_id":"MDQ6VXNlcjQ4NTg0NTk4","avatar_url":"https://avatars.githubusercontent.com/u/48584598?v=4","gravatar_id":"","url":"https://api.github.com/users/IreneSucameli","html_url":"https://github.com/IreneSucameli","followers_url":"https://api.github.com/users/IreneSucameli/followers","following_url":"https://api.github.com/users/IreneSucameli/following{/other_user}","gists_url":"https://api.github.com/users/IreneSucameli/gists{/gist_id}","starred_url":"https://api.github.com/users/IreneSucameli/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/IreneSucameli/subscriptions","organizations_url":"https://api.github.com/users/IreneSucameli/orgs","repos_url":"https://api.github.com/users/IreneSucameli/repos","events_url":"https://api.github.com/users/IreneSucameli/events{/privacy}","received_events_url":"https://api.github.com/users/IreneSucameli/received_events","type":"User","site_admin":false},"labels":[],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":0,"created_at":"2022-02-17T14:49:59Z","updated_at":"2022-02-17T14:49:59Z","closed_at":null,"author_association":"NONE","active_lock_reason":null,"body":"Hi, could you please specify how large is the training size of the multilingual version of BERT, in terms of GB?\r\nThanks","reactions":{"url":"https://api.github.com/repos/google-research/bert/issues/1296/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/google-research/bert/issues/1296/timeline","performed_via_github_app":null},{"url":"https://api.github.com/repos/google-research/bert/issues/1295","repository_url":"https://api.github.com/repos/google-research/bert","labels_url":"https://api.github.com/repos/google-research/bert/issues/1295/labels{/name}","comments_url":"https://api.github.com/repos/google-research/bert/issues/1295/comments","events_url":"https://api.github.com/repos/google-research/bert/issues/1295/events","html_url":"https://github.com/google-research/bert/issues/1295","id":1133623902,"node_id":"I_kwDOCTlCuc5Dkbpe","number":1295,"title":"valueError: Could not find matching function to call loaded from the SavedModel. Got:   Positional arguments (2 total):     * False     * None   Keyword arguments: {'as_dict': True, 'signature': 'tokenization_info'","user":{"login":"kusumlata123","id":39075908,"node_id":"MDQ6VXNlcjM5MDc1OTA4","avatar_url":"https://avatars.githubusercontent.com/u/39075908?v=4","gravatar_id":"","url":"https://api.github.com/users/kusumlata123","html_url":"https://github.com/kusumlata123","followers_url":"https://api.github.com/users/kusumlata123/followers","following_url":"https://api.github.com/users/kusumlata123/following{/other_user}","gists_url":"https://api.github.com/users/kusumlata123/gists{/gist_id}","starred_url":"https://api.github.com/users/kusumlata123/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/kusumlata123/subscriptions","organizations_url":"https://api.github.com/users/kusumlata123/orgs","repos_url":"https://api.github.com/users/kusumlata123/repos","events_url":"https://api.github.com/users/kusumlata123/events{/privacy}","received_events_url":"https://api.github.com/users/kusumlata123/received_events","type":"User","site_admin":false},"labels":[],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":3,"created_at":"2022-02-12T05:36:02Z","updated_at":"2022-02-16T09:18:13Z","closed_at":null,"author_association":"NONE","active_lock_reason":null,"body":" I am having this code to call Bert model and  getting error:\r\nFile \"/home/dr/Desktop/dali-md-master/biaffine_md.py\", line 26, in add_model_specific_valuables\r\n    self.bert_tokenizer = self.load_bert_vocab()\r\n  File \"/home/dr/Desktop/dali-md-master/biaffine_md.py\", line 53, in load_bert_vocab\r\n    vocab_info = bert_model(signature=\"tokenization_info\",as_dict=True)\r\n  File \"/home/dr/anaconda3/envs/hcoref/lib/python2.7/site-packages/tensorflow_core/python/saved_model/load.py\", line 438, in _call_attribute\r\n    return instance.__call__(*args, **kwargs)\r\n  File \"/home/dr/anaconda3/envs/hcoref/lib/python2.7/site-packages/tensorflow_core/python/eager/def_function.py\", line 449, in __call__\r\n    self._initialize(args, kwds, add_initializers_to=initializer_map)\r\n  File \"/home/dr/anaconda3/envs/hcoref/lib/python2.7/site-packages/tensorflow_core/python/eager/def_function.py\", line 392, in _initialize\r\n    *args, **kwds))\r\n  File \"/home/dr/anaconda3/envs/hcoref/lib/python2.7/site-packages/tensorflow_core/python/eager/function.py\", line 1847, in _get_concrete_function_internal_garbage_collected\r\n    graph_function, _, _ = self._maybe_define_function(args, kwargs)\r\n  File \"/home/dr/anaconda3/envs/hcoref/lib/python2.7/site-packages/tensorflow_core/python/eager/function.py\", line 2147, in _maybe_define_function\r\n    graph_function = self._create_graph_function(args, kwargs)\r\n  File \"/home/dr/anaconda3/envs/hcoref/lib/python2.7/site-packages/tensorflow_core/python/eager/function.py\", line 2038, in _create_graph_function\r\n    capture_by_value=self._capture_by_value),\r\n  File \"/home/dr/anaconda3/envs/hcoref/lib/python2.7/site-packages/tensorflow_core/python/framework/func_graph.py\", line 915, in func_graph_from_py_func\r\n    func_outputs = python_func(*func_args, **func_kwargs)\r\n  File \"/home/dr/anaconda3/envs/hcoref/lib/python2.7/site-packages/tensorflow_core/python/eager/def_function.py\", line 335, in wrapped_fn\r\n    return weak_wrapped_fn().__wrapped__(*args, **kwds)\r\n  File \"/home/dr/anaconda3/envs/hcoref/lib/python2.7/site-packages/tensorflow_core/python/saved_model/function_deserialization.py\", line 262, in restored_function_body\r\n    \"\\n\\n\".join(signature_descriptions)))\r\nValueError: Could not find matching function to call loaded from the SavedModel. Got:\r\n  Positional arguments (2 total):\r\n    * False\r\n    * None\r\n  Keyword arguments: {'as_dict': True, 'signature': 'tokenization_info'}\r\n\r\nExpected these arguments to match one of the following 4 option(s):\r\n\r\nOption 1:\r\n  Positional arguments (3 total):\r\n    * {u'input_word_ids': TensorSpec(shape=(?, ?), dtype=tf.int32, name=u'input_word_ids'), u'input_mask': TensorSpec(shape=(?, ?), dtype=tf.int32, name=u'input_mask'), u'input_type_ids': TensorSpec(shape=(?, ?), dtype=tf.int32, name=u'input_type_ids')}\r\n    * False\r\n    * None\r\n  Keyword arguments: {}\r\n\r\nOption 2:\r\n  Positional arguments (3 total):\r\n    * {u'input_word_ids': TensorSpec(shape=(?, ?), dtype=tf.int32, name=u'inputs/input_word_ids'), u'input_mask': TensorSpec(shape=(?, ?), dtype=tf.int32, name=u'inputs/input_mask'), u'input_type_ids': TensorSpec(shape=(?, ?), dtype=tf.int32, name=u'inputs/input_type_ids')}\r\n    * False\r\n    * None\r\n  Keyword arguments: {}\r\n\r\nOption 3:\r\n  Positional arguments (3 total):\r\n    * {u'input_word_ids': TensorSpec(shape=(?, ?), dtype=tf.int32, name=u'inputs/input_word_ids'), u'input_mask': TensorSpec(shape=(?, ?), dtype=tf.int32, name=u'inputs/input_mask'), u'input_type_ids': TensorSpec(shape=(?, ?), dtype=tf.int32, name=u'inputs/input_type_ids')}\r\n    * True\r\n    * None\r\n  Keyword arguments: {}\r\n\r\nOption 4:\r\n  Positional arguments (3 total):\r\n    * {u'input_word_ids': TensorSpec(shape=(?, ?), dtype=tf.int32, name=u'input_word_ids'), u'input_mask': TensorSpec(shape=(?, ?), dtype=tf.int32, name=u'input_mask'), u'input_type_ids': TensorSpec(shape=(?, ?), dtype=tf.int32, name=u'input_type_ids')}\r\n    * True\r\n    * None\r\n  Keyword arguments: {}\r\n below is code:\r\n\r\ndef load_bert_vocab(self):\r\n    with tf.Graph().as_default():\r\n      # bert_model = hub.Module(self.bert_url)\r\n      bert_model = hub.load(self.bert_url)\r\n      vocab_info = bert_model(signature=\"tokenization_info\",as_dict=True)\r\n      \r\n      with tf.Session() as sess:\r\n        vocab_file, do_lower_case = sess.run([vocab_info[\"vocab_file\"],vocab_info[\"do_lower_case\"]])\r\n\r\n    return bert_tokenization.FullTokenizer(vocab_file=vocab_file,do_lower_case=do_lower_case)\r\n ","reactions":{"url":"https://api.github.com/repos/google-research/bert/issues/1295/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/google-research/bert/issues/1295/timeline","performed_via_github_app":null},{"url":"https://api.github.com/repos/google-research/bert/issues/1293","repository_url":"https://api.github.com/repos/google-research/bert","labels_url":"https://api.github.com/repos/google-research/bert/issues/1293/labels{/name}","comments_url":"https://api.github.com/repos/google-research/bert/issues/1293/comments","events_url":"https://api.github.com/repos/google-research/bert/issues/1293/events","html_url":"https://github.com/google-research/bert/issues/1293","id":1126756758,"node_id":"I_kwDOCTlCuc5DKPGW","number":1293,"title":"Should first global step be initialized to 1 in wam up schedule?","user":{"login":"tiandiweizun","id":13550295,"node_id":"MDQ6VXNlcjEzNTUwMjk1","avatar_url":"https://avatars.githubusercontent.com/u/13550295?v=4","gravatar_id":"","url":"https://api.github.com/users/tiandiweizun","html_url":"https://github.com/tiandiweizun","followers_url":"https://api.github.com/users/tiandiweizun/followers","following_url":"https://api.github.com/users/tiandiweizun/following{/other_user}","gists_url":"https://api.github.com/users/tiandiweizun/gists{/gist_id}","starred_url":"https://api.github.com/users/tiandiweizun/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/tiandiweizun/subscriptions","organizations_url":"https://api.github.com/users/tiandiweizun/orgs","repos_url":"https://api.github.com/users/tiandiweizun/repos","events_url":"https://api.github.com/users/tiandiweizun/events{/privacy}","received_events_url":"https://api.github.com/users/tiandiweizun/received_events","type":"User","site_admin":false},"labels":[],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":0,"created_at":"2022-02-08T03:44:41Z","updated_at":"2022-02-08T03:44:41Z","closed_at":null,"author_association":"NONE","active_lock_reason":null,"body":"In linear warmup, the first global step is **zero**,and the learing rate is **zero** too(lr=global_step/num_warmup_steps * init_lr),then nothing will be updated except momentum(m and v in Adam), especially when optimizer has no momentum, the first step will do nothing, so I think  the global step iteration(add one) sholud be done before learning rate updated, thank you for your answer!","reactions":{"url":"https://api.github.com/repos/google-research/bert/issues/1293/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/google-research/bert/issues/1293/timeline","performed_via_github_app":null},{"url":"https://api.github.com/repos/google-research/bert/issues/1292","repository_url":"https://api.github.com/repos/google-research/bert","labels_url":"https://api.github.com/repos/google-research/bert/issues/1292/labels{/name}","comments_url":"https://api.github.com/repos/google-research/bert/issues/1292/comments","events_url":"https://api.github.com/repos/google-research/bert/issues/1292/events","html_url":"https://github.com/google-research/bert/pull/1292","id":1126735219,"node_id":"PR_kwDOCTlCuc4yNXdJ","number":1292,"title":"When the count of train_examples is not divisible by the train_batch_size,the tail batch of last epoch can't be trained","user":{"login":"tiandiweizun","id":13550295,"node_id":"MDQ6VXNlcjEzNTUwMjk1","avatar_url":"https://avatars.githubusercontent.com/u/13550295?v=4","gravatar_id":"","url":"https://api.github.com/users/tiandiweizun","html_url":"https://github.com/tiandiweizun","followers_url":"https://api.github.com/users/tiandiweizun/followers","following_url":"https://api.github.com/users/tiandiweizun/following{/other_user}","gists_url":"https://api.github.com/users/tiandiweizun/gists{/gist_id}","starred_url":"https://api.github.com/users/tiandiweizun/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/tiandiweizun/subscriptions","organizations_url":"https://api.github.com/users/tiandiweizun/orgs","repos_url":"https://api.github.com/users/tiandiweizun/repos","events_url":"https://api.github.com/users/tiandiweizun/events{/privacy}","received_events_url":"https://api.github.com/users/tiandiweizun/received_events","type":"User","site_admin":false},"labels":[],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":0,"created_at":"2022-02-08T03:04:55Z","updated_at":"2022-02-08T03:04:55Z","closed_at":null,"author_association":"NONE","active_lock_reason":null,"draft":false,"pull_request":{"url":"https://api.github.com/repos/google-research/bert/pulls/1292","html_url":"https://github.com/google-research/bert/pull/1292","diff_url":"https://github.com/google-research/bert/pull/1292.diff","patch_url":"https://github.com/google-research/bert/pull/1292.patch","merged_at":null},"body":null,"reactions":{"url":"https://api.github.com/repos/google-research/bert/issues/1292/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/google-research/bert/issues/1292/timeline","performed_via_github_app":null},{"url":"https://api.github.com/repos/google-research/bert/issues/1290","repository_url":"https://api.github.com/repos/google-research/bert","labels_url":"https://api.github.com/repos/google-research/bert/issues/1290/labels{/name}","comments_url":"https://api.github.com/repos/google-research/bert/issues/1290/comments","events_url":"https://api.github.com/repos/google-research/bert/issues/1290/events","html_url":"https://github.com/google-research/bert/issues/1290","id":1123792441,"node_id":"I_kwDOCTlCuc5C-7Y5","number":1290,"title":"No ckpt.meta file in the release of 24 smaller BERT models","user":{"login":"sauravpr","id":28605330,"node_id":"MDQ6VXNlcjI4NjA1MzMw","avatar_url":"https://avatars.githubusercontent.com/u/28605330?v=4","gravatar_id":"","url":"https://api.github.com/users/sauravpr","html_url":"https://github.com/sauravpr","followers_url":"https://api.github.com/users/sauravpr/followers","following_url":"https://api.github.com/users/sauravpr/following{/other_user}","gists_url":"https://api.github.com/users/sauravpr/gists{/gist_id}","starred_url":"https://api.github.com/users/sauravpr/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/sauravpr/subscriptions","organizations_url":"https://api.github.com/users/sauravpr/orgs","repos_url":"https://api.github.com/users/sauravpr/repos","events_url":"https://api.github.com/users/sauravpr/events{/privacy}","received_events_url":"https://api.github.com/users/sauravpr/received_events","type":"User","site_admin":false},"labels":[],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":0,"created_at":"2022-02-04T04:18:50Z","updated_at":"2022-02-04T04:20:33Z","closed_at":null,"author_association":"NONE","active_lock_reason":null,"body":".meta component of the TF checkpoint is missing in each of the smaller BERT models provided here \r\nhttps://github.com/google-research/bert \r\n\r\nFor example, this link only has .index and .data components https://storage.googleapis.com/bert_models/2020_02_20/uncased_L-10_H-768_A-12.zip\r\n\r\nThis issue has been raised earlier as well, but hasn't been resolved\r\nhttps://github.com/google-research/bert/issues/1172 ","reactions":{"url":"https://api.github.com/repos/google-research/bert/issues/1290/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/google-research/bert/issues/1290/timeline","performed_via_github_app":null},{"url":"https://api.github.com/repos/google-research/bert/issues/1289","repository_url":"https://api.github.com/repos/google-research/bert","labels_url":"https://api.github.com/repos/google-research/bert/issues/1289/labels{/name}","comments_url":"https://api.github.com/repos/google-research/bert/issues/1289/comments","events_url":"https://api.github.com/repos/google-research/bert/issues/1289/events","html_url":"https://github.com/google-research/bert/issues/1289","id":1097280270,"node_id":"I_kwDOCTlCuc5BZysO","number":1289,"title":"Saving a model after pretraining on custom data.","user":{"login":"theProcrastinatr","id":51825289,"node_id":"MDQ6VXNlcjUxODI1Mjg5","avatar_url":"https://avatars.githubusercontent.com/u/51825289?v=4","gravatar_id":"","url":"https://api.github.com/users/theProcrastinatr","html_url":"https://github.com/theProcrastinatr","followers_url":"https://api.github.com/users/theProcrastinatr/followers","following_url":"https://api.github.com/users/theProcrastinatr/following{/other_user}","gists_url":"https://api.github.com/users/theProcrastinatr/gists{/gist_id}","starred_url":"https://api.github.com/users/theProcrastinatr/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/theProcrastinatr/subscriptions","organizations_url":"https://api.github.com/users/theProcrastinatr/orgs","repos_url":"https://api.github.com/users/theProcrastinatr/repos","events_url":"https://api.github.com/users/theProcrastinatr/events{/privacy}","received_events_url":"https://api.github.com/users/theProcrastinatr/received_events","type":"User","site_admin":false},"labels":[],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":1,"created_at":"2022-01-09T19:24:43Z","updated_at":"2022-02-01T06:24:56Z","closed_at":null,"author_association":"NONE","active_lock_reason":null,"body":"Currently, after running the `run_pretraining.py` the model checkpoints get saved. But is there any way I can save the entire model as a `model.pb` file so that in future it could be loaded directly as a `KerasLayer` with `tf_hub`?\n\nOr is there a way of loading `model.ckpt` as a `KerasLayer`?\n\nPlease let me know if there is a way to do either. It'd be very helpful.\n\nThanks.","reactions":{"url":"https://api.github.com/repos/google-research/bert/issues/1289/reactions","total_count":1,"+1":0,"-1":0,"laugh":1,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/google-research/bert/issues/1289/timeline","performed_via_github_app":null},{"url":"https://api.github.com/repos/google-research/bert/issues/1288","repository_url":"https://api.github.com/repos/google-research/bert","labels_url":"https://api.github.com/repos/google-research/bert/issues/1288/labels{/name}","comments_url":"https://api.github.com/repos/google-research/bert/issues/1288/comments","events_url":"https://api.github.com/repos/google-research/bert/issues/1288/events","html_url":"https://github.com/google-research/bert/issues/1288","id":1090858605,"node_id":"I_kwDOCTlCuc5BBS5t","number":1288,"title":"    super(AdamWeightDecayOptimizer, self).__init__(False, name) TypeError: __init__() takes 1 positional argument but 3 were give","user":{"login":"dolphin-Jia","id":33857404,"node_id":"MDQ6VXNlcjMzODU3NDA0","avatar_url":"https://avatars.githubusercontent.com/u/33857404?v=4","gravatar_id":"","url":"https://api.github.com/users/dolphin-Jia","html_url":"https://github.com/dolphin-Jia","followers_url":"https://api.github.com/users/dolphin-Jia/followers","following_url":"https://api.github.com/users/dolphin-Jia/following{/other_user}","gists_url":"https://api.github.com/users/dolphin-Jia/gists{/gist_id}","starred_url":"https://api.github.com/users/dolphin-Jia/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/dolphin-Jia/subscriptions","organizations_url":"https://api.github.com/users/dolphin-Jia/orgs","repos_url":"https://api.github.com/users/dolphin-Jia/repos","events_url":"https://api.github.com/users/dolphin-Jia/events{/privacy}","received_events_url":"https://api.github.com/users/dolphin-Jia/received_events","type":"User","site_admin":false},"labels":[],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":0,"created_at":"2021-12-30T03:54:54Z","updated_at":"2021-12-30T03:54:54Z","closed_at":null,"author_association":"NONE","active_lock_reason":null,"body":"\r\nDeprecated in favor of operator or tf.math.divide.\r\nINFO:tensorflow:Error recorded from training_loop: __init__() takes 1 positional argument but 3 were given\r\nINFO:tensorflow:training_loop marked as finished\r\nWARNING:tensorflow:Reraising captured error\r\nTraceback (most recent call last):\r\n  File \"/tf/bert/run_classifier.py\", line 1054, in <module>\r\n    tf.app.run()\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/platform/app.py\", line 125, in run\r\n    _sys.exit(main(argv))\r\n  File \"/tf/bert/run_classifier.py\", line 953, in main\r\n    estimator.train(input_fn=train_input_fn, max_steps=num_train_steps)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/contrib/tpu/python/tpu/tpu_estimator.py\", line 2457, in train\r\n    rendezvous.raise_errors()\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/contrib/tpu/python/tpu/error_handling.py\", line 128, in raise_errors\r\n    six.reraise(typ, value, traceback)\r\n  File \"/usr/local/lib/python3.5/dist-packages/six.py\", line 693, in reraise\r\n    raise value\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/contrib/tpu/python/tpu/tpu_estimator.py\", line 2452, in train\r\n    saving_listeners=saving_listeners)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow_estimator/python/estimator/estimator.py\", line 358, in train\r\n    loss = self._train_model(input_fn, hooks, saving_listeners)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow_estimator/python/estimator/estimator.py\", line 1124, in _train_model\r\n    return self._train_model_default(input_fn, hooks, saving_listeners)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow_estimator/python/estimator/estimator.py\", line 1154, in _train_model_default\r\n    features, labels, model_fn_lib.ModeKeys.TRAIN, self.config)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/contrib/tpu/python/tpu/tpu_estimator.py\", line 2251, in _call_model_fn\r\n    config)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow_estimator/python/estimator/estimator.py\", line 1112, in _call_model_fn\r\n    model_fn_results = self._model_fn(features=features, **kwargs)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/contrib/tpu/python/tpu/tpu_estimator.py\", line 2534, in _model_fn\r\n    features, labels, is_export_mode=is_export_mode)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/contrib/tpu/python/tpu/tpu_estimator.py\", line 1323, in call_without_tpu\r\n    return self._call_model_fn(features, labels, is_export_mode=is_export_mode)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/contrib/tpu/python/tpu/tpu_estimator.py\", line 1593, in _call_model_fn\r\n    estimator_spec = self._model_fn(features=features, **kwargs)\r\n  File \"/tf/bert/run_classifier.py\", line 747, in model_fn\r\n    total_loss, learning_rate, num_train_steps, num_warmup_steps, use_tpu)\r\n  File \"/tf/bert/optimization.py\", line 65, in create_optimizer\r\n    exclude_from_weight_decay=[\"LayerNorm\", \"layer_norm\", \"bias\"])\r\n  File \"/tf/bert/optimization.py\", line 100, in __init__\r\n    super(AdamWeightDecayOptimizer, self).__init__(False, name)\r\nTypeError: __init__() takes 1 positional argument but 3 were given\r\n","reactions":{"url":"https://api.github.com/repos/google-research/bert/issues/1288/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/google-research/bert/issues/1288/timeline","performed_via_github_app":null},{"url":"https://api.github.com/repos/google-research/bert/issues/1287","repository_url":"https://api.github.com/repos/google-research/bert","labels_url":"https://api.github.com/repos/google-research/bert/issues/1287/labels{/name}","comments_url":"https://api.github.com/repos/google-research/bert/issues/1287/comments","events_url":"https://api.github.com/repos/google-research/bert/issues/1287/events","html_url":"https://github.com/google-research/bert/issues/1287","id":1089625061,"node_id":"I_kwDOCTlCuc5A8lvl","number":1287,"title":"tokenizer.word_index.items() with bert ","user":{"login":"ersamo","id":87552082,"node_id":"MDQ6VXNlcjg3NTUyMDgy","avatar_url":"https://avatars.githubusercontent.com/u/87552082?v=4","gravatar_id":"","url":"https://api.github.com/users/ersamo","html_url":"https://github.com/ersamo","followers_url":"https://api.github.com/users/ersamo/followers","following_url":"https://api.github.com/users/ersamo/following{/other_user}","gists_url":"https://api.github.com/users/ersamo/gists{/gist_id}","starred_url":"https://api.github.com/users/ersamo/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/ersamo/subscriptions","organizations_url":"https://api.github.com/users/ersamo/orgs","repos_url":"https://api.github.com/users/ersamo/repos","events_url":"https://api.github.com/users/ersamo/events{/privacy}","received_events_url":"https://api.github.com/users/ersamo/received_events","type":"User","site_admin":false},"labels":[],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":0,"created_at":"2021-12-28T05:35:23Z","updated_at":"2021-12-28T05:35:23Z","closed_at":null,"author_association":"NONE","active_lock_reason":null,"body":"what is the equivalent for \r\n`tokenizer.word_index.items() `to bert as i got `AttributeError: 'BertTokenizer' object has no attribute 'word_index'`\r\n","reactions":{"url":"https://api.github.com/repos/google-research/bert/issues/1287/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/google-research/bert/issues/1287/timeline","performed_via_github_app":null},{"url":"https://api.github.com/repos/google-research/bert/issues/1286","repository_url":"https://api.github.com/repos/google-research/bert","labels_url":"https://api.github.com/repos/google-research/bert/issues/1286/labels{/name}","comments_url":"https://api.github.com/repos/google-research/bert/issues/1286/comments","events_url":"https://api.github.com/repos/google-research/bert/issues/1286/events","html_url":"https://github.com/google-research/bert/issues/1286","id":1089536096,"node_id":"I_kwDOCTlCuc5A8QBg","number":1286,"title":"How to predict the probability of an empty string using BERT","user":{"login":"brienna","id":9706870,"node_id":"MDQ6VXNlcjk3MDY4NzA=","avatar_url":"https://avatars.githubusercontent.com/u/9706870?v=4","gravatar_id":"","url":"https://api.github.com/users/brienna","html_url":"https://github.com/brienna","followers_url":"https://api.github.com/users/brienna/followers","following_url":"https://api.github.com/users/brienna/following{/other_user}","gists_url":"https://api.github.com/users/brienna/gists{/gist_id}","starred_url":"https://api.github.com/users/brienna/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/brienna/subscriptions","organizations_url":"https://api.github.com/users/brienna/orgs","repos_url":"https://api.github.com/users/brienna/repos","events_url":"https://api.github.com/users/brienna/events{/privacy}","received_events_url":"https://api.github.com/users/brienna/received_events","type":"User","site_admin":false},"labels":[],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":0,"created_at":"2021-12-28T00:30:49Z","updated_at":"2021-12-28T00:38:32Z","closed_at":null,"author_association":"NONE","active_lock_reason":null,"body":"Suppose we have a template sentence like this: \r\n\r\n- \"The ____ house is our meeting place.\"\r\n\r\nand we have a list of adjectives to fill in the blank, e.g.:\r\n- \"yellow\"\r\n- \"large\"\r\n- \"\"\r\n\r\nNote that one of these is an empty string. \r\n\r\nThe goal is to compare the probabilities to select the most likely word to describe \"house\" given the context of the sentence. If it's more likely to have <i>nothing</i>, this should also be taken into consideration. \r\n\r\nWe can predict the probability of each word filling in the blank, but how would we predict the probability that an empty string fills in the blank, i.e. the probability of there being no adjective to describe \"house\"?\r\n\r\nTo predict the probability of a word: \r\n\r\n```python\r\nfrom transformers import BertTokenizer, BertForMaskedLM\r\nimport torch\r\nfrom torch.nn import functional as F\r\n\r\n# Load BERT tokenizer and pre-trained model\r\ntokenizer = BertTokenizer.from_pretrained('bert-large-uncased')\r\nmodel = BertForMaskedLM.from_pretrained('bert-large-uncased', return_dict=True)\r\n\r\ntargets = [\"yellow\", \"large\"]\r\nsentence = \"The [MASK] house is our meeting place.\"\r\n\r\n# Using BERT, compute probability over its entire vocabulary, returning logits\r\ninput = tokenizer.encode_plus(sentence, return_tensors = \"pt\") \r\nmask_index = torch.where(input[\"input_ids\"][0] == tokenizer.mask_token_id)[0] \r\nwith torch.no_grad():\r\n    output = model(**input) \r\n\r\n# Run softmax over the logits to get the probabilities\r\nsoftmax = F.softmax(output.logits[0], dim=-1)\r\n\r\n# Find the words' probabilities in this probability distribution\r\ntarget_probabilities = {t: softmax[mask_index, tokenizer.vocab[t]].numpy()[0] for t in targets}\r\ntarget_probabilities\r\n```\r\n\r\nThis outputs a list of the words and their associated probabilities: \r\n\r\n```python\r\n{'yellow': 0.0061520976, 'large': 0.00071377633}\r\n```\r\n\r\nIf I try to add an empty string to the list, I get the following error: \r\n\r\n```python\r\n---------------------------------------------------------------------------\r\nKeyError                                  Traceback (most recent call last)\r\n<ipython-input-62-6f726220a108> in <module>\r\n     18 \r\n     19 # Find the words' probabilities in this probability distribution\r\n---> 20 target_probabilities = {t: softmax[mask_index, tokenizer.vocab[t]].numpy()[0] for t in targets}\r\n     21 target_probabilities\r\n\r\n<ipython-input-62-6f726220a108> in <dictcomp>(.0)\r\n     18 \r\n     19 # Find the words' probabilities in this probability distribution\r\n---> 20 target_probabilities = {t: softmax[mask_index, tokenizer.vocab[t]].numpy()[0] for t in targets}\r\n     21 target_probabilities\r\n\r\nKeyError: ''\r\n```\r\n\r\nThis is because BERT's vocabulary contains no empty string, so we can't look up the probability of something that doesn't exist in the model. \r\n\r\nHow should we get the probability of there being no word to fill in the blank? Is this possible with the model? Does it make sense to use the empty token `[PAD]` instead of an empty string? (I've only seen `[PAD]` used at the end of sentences, to make a group of sentences the same length.) ","reactions":{"url":"https://api.github.com/repos/google-research/bert/issues/1286/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/google-research/bert/issues/1286/timeline","performed_via_github_app":null},{"url":"https://api.github.com/repos/google-research/bert/issues/1280","repository_url":"https://api.github.com/repos/google-research/bert","labels_url":"https://api.github.com/repos/google-research/bert/issues/1280/labels{/name}","comments_url":"https://api.github.com/repos/google-research/bert/issues/1280/comments","events_url":"https://api.github.com/repos/google-research/bert/issues/1280/events","html_url":"https://github.com/google-research/bert/issues/1280","id":1069197447,"node_id":"I_kwDOCTlCuc4_uqiH","number":1280,"title":"where is the source code of tf.contrib.tpu.RunConfig(),my tensorflow version is 1.11.0","user":{"login":"sweetboxwwy","id":38718118,"node_id":"MDQ6VXNlcjM4NzE4MTE4","avatar_url":"https://avatars.githubusercontent.com/u/38718118?v=4","gravatar_id":"","url":"https://api.github.com/users/sweetboxwwy","html_url":"https://github.com/sweetboxwwy","followers_url":"https://api.github.com/users/sweetboxwwy/followers","following_url":"https://api.github.com/users/sweetboxwwy/following{/other_user}","gists_url":"https://api.github.com/users/sweetboxwwy/gists{/gist_id}","starred_url":"https://api.github.com/users/sweetboxwwy/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/sweetboxwwy/subscriptions","organizations_url":"https://api.github.com/users/sweetboxwwy/orgs","repos_url":"https://api.github.com/users/sweetboxwwy/repos","events_url":"https://api.github.com/users/sweetboxwwy/events{/privacy}","received_events_url":"https://api.github.com/users/sweetboxwwy/received_events","type":"User","site_admin":false},"labels":[],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":1,"created_at":"2021-12-02T07:00:48Z","updated_at":"2022-03-04T19:57:45Z","closed_at":null,"author_association":"NONE","active_lock_reason":null,"body":"    run_config = tf.contrib.tpu.RunConfig(\r\n            cluster=tpu_cluster_resolver,\r\n            master=FLAGS.master,\r\n            model_dir=FLAGS.output_dir,\r\n            save_checkpoints_steps=FLAGS.save_checkpoints_steps,\r\n            tpu_config=tf.contrib.tpu.TPUConfig(\r\n                    iterations_per_loop=FLAGS.iterations_per_loop,\r\n                    num_shards=FLAGS.num_tpu_cores,\r\n                    per_host_input_for_training=is_per_host))\r\n","reactions":{"url":"https://api.github.com/repos/google-research/bert/issues/1280/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/google-research/bert/issues/1280/timeline","performed_via_github_app":null},{"url":"https://api.github.com/repos/google-research/bert/issues/1279","repository_url":"https://api.github.com/repos/google-research/bert","labels_url":"https://api.github.com/repos/google-research/bert/issues/1279/labels{/name}","comments_url":"https://api.github.com/repos/google-research/bert/issues/1279/comments","events_url":"https://api.github.com/repos/google-research/bert/issues/1279/events","html_url":"https://github.com/google-research/bert/issues/1279","id":1065351205,"node_id":"I_kwDOCTlCuc4_f_gl","number":1279,"title":"How can I change head of multi-head attention and the layers of self attention in BERT?","user":{"login":"rafieim74","id":47557033,"node_id":"MDQ6VXNlcjQ3NTU3MDMz","avatar_url":"https://avatars.githubusercontent.com/u/47557033?v=4","gravatar_id":"","url":"https://api.github.com/users/rafieim74","html_url":"https://github.com/rafieim74","followers_url":"https://api.github.com/users/rafieim74/followers","following_url":"https://api.github.com/users/rafieim74/following{/other_user}","gists_url":"https://api.github.com/users/rafieim74/gists{/gist_id}","starred_url":"https://api.github.com/users/rafieim74/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/rafieim74/subscriptions","organizations_url":"https://api.github.com/users/rafieim74/orgs","repos_url":"https://api.github.com/users/rafieim74/repos","events_url":"https://api.github.com/users/rafieim74/events{/privacy}","received_events_url":"https://api.github.com/users/rafieim74/received_events","type":"User","site_admin":false},"labels":[],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":0,"created_at":"2021-11-28T14:49:07Z","updated_at":"2021-11-28T14:49:07Z","closed_at":null,"author_association":"NONE","active_lock_reason":null,"body":null,"reactions":{"url":"https://api.github.com/repos/google-research/bert/issues/1279/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/google-research/bert/issues/1279/timeline","performed_via_github_app":null},{"url":"https://api.github.com/repos/google-research/bert/issues/1278","repository_url":"https://api.github.com/repos/google-research/bert","labels_url":"https://api.github.com/repos/google-research/bert/issues/1278/labels{/name}","comments_url":"https://api.github.com/repos/google-research/bert/issues/1278/comments","events_url":"https://api.github.com/repos/google-research/bert/issues/1278/events","html_url":"https://github.com/google-research/bert/issues/1278","id":1062387988,"node_id":"I_kwDOCTlCuc4_UsEU","number":1278,"title":"OP_REQUIRES failed at save_restore_v2_ops.cc:184 : Not found: Key output_bias not found in checkpoint","user":{"login":"Faker0918","id":92665217,"node_id":"U_kgDOBYX1gQ","avatar_url":"https://avatars.githubusercontent.com/u/92665217?v=4","gravatar_id":"","url":"https://api.github.com/users/Faker0918","html_url":"https://github.com/Faker0918","followers_url":"https://api.github.com/users/Faker0918/followers","following_url":"https://api.github.com/users/Faker0918/following{/other_user}","gists_url":"https://api.github.com/users/Faker0918/gists{/gist_id}","starred_url":"https://api.github.com/users/Faker0918/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/Faker0918/subscriptions","organizations_url":"https://api.github.com/users/Faker0918/orgs","repos_url":"https://api.github.com/users/Faker0918/repos","events_url":"https://api.github.com/users/Faker0918/events{/privacy}","received_events_url":"https://api.github.com/users/Faker0918/received_events","type":"User","site_admin":false},"labels":[],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":0,"created_at":"2021-11-24T12:47:32Z","updated_at":"2021-11-24T12:47:32Z","closed_at":null,"author_association":"NONE","active_lock_reason":null,"body":"I have pre-trained a model based on the bert-large model using my dataset and get the checkpoint successfully, \r\nhowever ,when I use the checkpoint to do predict task I get the **Restoring from checkpoint failed. **error\r\ni just run\r\n****\r\n`python run_classifier.py\r\n--task_name=XNLI\r\n\\\r\n--do_train=true\r\n\\\r\n--do_eval=true\r\n\\\r\n--data_dir=./GLUE_DIR/XNLI\r\n\\\r\n--vocab_file=./wwm_uncased_L-24_H-1024_A-16/vocab.txt\r\n\\\r\n--bert_config_file=./wwm_uncased_L-24_H-1024_A-16/bert_config.json\r\n\\\r\n--init_checkpoint=./tmp/pretraining_output/model.ckpt-20\r\n\\\r\n--max_seq_length=64\r\n\\\r\n--train_batch_size=8\r\n\\\r\n--learning_rate=2e-5\r\n\\\r\n--num_train_epochs=5.0\r\n\\\r\n--output_dir=./tmp/pretraining_output\r\n`\r\n2021-11-24 20:28:22.908571: W tensorflow/core/framework/op_kernel.cc:1651] OP_REQUIRES failed at save_restore_v2_ops.cc:184 : Not found: Key output_bias not found in checkpoint\r\nERROR:tensorflow:Error recorded from training_loop: Restoring from checkpoint failed. This is most likely due to a Variable name or other graph key that is missing from the checkpoint. Please ensure that you have not altered the graph expected based on the checkpoint. Original error:\r\n\r\nKey output_bias not found in checkpoint\r\n\t [[node save/RestoreV2 (defined at /anaconda3/envs/tf_ruan/lib/python3.7/site-packages/tensorflow_core/python/framework/ops.py:1748) ]]\r\n\r\nOriginal stack trace for 'save/RestoreV2':\r\n  File \"/xuweixinxi/run_classifier.py\", line 1007, in <module>\r\n    tf.app.run()\r\n  File \"/anaconda3/envs/tf_ruan/lib/python3.7/site-packages/tensorflow_core/python/platform/app.py\", line 40, in run\r\n    _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef)\r\n  File \"/anaconda3/envs/tf_ruan/lib/python3.7/site-packages/absl/app.py\", line 303, in run\r\n    _run_main(main, args)\r\n  File \"/anaconda3/envs/tf_ruan/lib/python3.7/site-packages/absl/app.py\", line 251, in _run_main\r\n    sys.exit(main(argv))\r\n  File \"/xuweixinxi/run_classifier.py\", line 906, in main\r\n    estimator.train(input_fn=train_input_fn, max_steps=num_train_steps)\r\n  File \"/anaconda3/envs/tf_ruan/lib/python3.7/site-packages/tensorflow_estimator/python/estimator/tpu/tpu_estimator.py\", line 3030, in train\r\n    saving_listeners=saving_listeners)\r\n  File \"/anaconda3/envs/tf_ruan/lib/python3.7/site-packages/tensorflow_estimator/python/estimator/estimator.py\", line 370, in train\r\n    loss = self._train_model(input_fn, hooks, saving_listeners)\r\n  File \"/anaconda3/envs/tf_ruan/lib/python3.7/site-packages/tensorflow_estimator/python/estimator/estimator.py\", line 1161, in _train_model\r\n    return self._train_model_default(input_fn, hooks, saving_listeners)\r\n  File \"/anaconda3/envs/tf_ruan/lib/python3.7/site-packages/tensorflow_estimator/python/estimator/estimator.py\", line 1195, in _train_model_default\r\n    saving_listeners)\r\n  File \"/anaconda3/envs/tf_ruan/lib/python3.7/site-packages/tensorflow_estimator/python/estimator/estimator.py\", line 1490, in _train_with_estimator_spec\r\n    log_step_count_steps=log_step_count_steps) as mon_sess:\r\n  File \"/anaconda3/envs/tf_ruan/lib/python3.7/site-packages/tensorflow_core/python/training/monitored_session.py\", line 584, in MonitoredTrainingSession\r\n    stop_grace_period_secs=stop_grace_period_secs)\r\n  File \"/anaconda3/envs/tf_ruan/lib/python3.7/site-packages/tensorflow_core/python/training/monitored_session.py\", line 1014, in __init__\r\n    stop_grace_period_secs=stop_grace_period_secs)\r\n  File \"/anaconda3/envs/tf_ruan/lib/python3.7/site-packages/tensorflow_core/python/training/monitored_session.py\", line 725, in __init__\r\n    self._sess = _RecoverableSession(self._coordinated_creator)\r\n  File \"/anaconda3/envs/tf_ruan/lib/python3.7/site-packages/tensorflow_core/python/training/monitored_session.py\", line 1207, in __init__\r\n    _WrappedSession.__init__(self, self._create_session())\r\n  File \"/anaconda3/envs/tf_ruan/lib/python3.7/site-packages/tensorflow_core/python/training/monitored_session.py\", line 1212, in _create_session\r\n    return self._sess_creator.create_session()\r\n  File \"/anaconda3/envs/tf_ruan/lib/python3.7/site-packages/tensorflow_core/python/training/monitored_session.py\", line 878, in create_session\r\n    self.tf_sess = self._session_creator.create_session()\r\n  File \"/anaconda3/envs/tf_ruan/lib/python3.7/site-packages/tensorflow_core/python/training/monitored_session.py\", line 638, in create_session\r\n    self._scaffold.finalize()\r\n  File \"/anaconda3/envs/tf_ruan/lib/python3.7/site-packages/tensorflow_core/python/training/monitored_session.py\", line 237, in finalize\r\n    self._saver.build()\r\n  File \"/anaconda3/envs/tf_ruan/lib/python3.7/site-packages/tensorflow_core/python/training/saver.py\", line 840, in build\r\n    self._build(self._filename, build_save=True, build_restore=True)\r\n  File \"/anaconda3/envs/tf_ruan/lib/python3.7/site-packages/tensorflow_core/python/training/saver.py\", line 878, in _build\r\n    build_restore=build_restore)\r\n  File \"/anaconda3/envs/tf_ruan/lib/python3.7/site-packages/tensorflow_core/python/training/saver.py\", line 502, in _build_internal\r\n    restore_sequentially, reshape)\r\n  File \"/anaconda3/envs/tf_ruan/lib/python3.7/site-packages/tensorflow_core/python/training/saver.py\", line 381, in _AddShardedRestoreOps\r\n    name=\"restore_shard\"))\r\n  File \"/anaconda3/envs/tf_ruan/lib/python3.7/site-packages/tensorflow_core/python/training/saver.py\", line 328, in _AddRestoreOps\r\n    restore_sequentially)\r\n  File \"/anaconda3/envs/tf_ruan/lib/python3.7/site-packages/tensorflow_core/python/training/saver.py\", line 575, in bulk_restore\r\n    return io_ops.restore_v2(filename_tensor, names, slices, dtypes)\r\n  File \"/anaconda3/envs/tf_ruan/lib/python3.7/site-packages/tensorflow_core/python/ops/gen_io_ops.py\", line 1696, in restore_v2\r\n    name=name)\r\n  File \"/anaconda3/envs/tf_ruan/lib/python3.7/site-packages/tensorflow_core/python/framework/op_def_library.py\", line 794, in _apply_op_helper\r\n    op_def=op_def)\r\n  File \"/anaconda3/envs/tf_ruan/lib/python3.7/site-packages/tensorflow_core/python/util/deprecation.py\", line 507, in new_func\r\n    return func(*args, **kwargs)\r\n  File \"/anaconda3/envs/tf_ruan/lib/python3.7/site-packages/tensorflow_core/python/framework/ops.py\", line 3357, in create_op\r\n    attrs, op_def, compute_device)\r\n  File \"/anaconda3/envs/tf_ruan/lib/python3.7/site-packages/tensorflow_core/python/framework/ops.py\", line 3426, in _create_op_internal\r\n    op_def=op_def)\r\n  File \"/anaconda3/envs/tf_ruan/lib/python3.7/site-packages/tensorflow_core/python/framework/ops.py\", line 1748, in __init__\r\n    self._traceback = tf_stack.extract_stack()\r\n","reactions":{"url":"https://api.github.com/repos/google-research/bert/issues/1278/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/google-research/bert/issues/1278/timeline","performed_via_github_app":null},{"url":"https://api.github.com/repos/google-research/bert/issues/1277","repository_url":"https://api.github.com/repos/google-research/bert","labels_url":"https://api.github.com/repos/google-research/bert/issues/1277/labels{/name}","comments_url":"https://api.github.com/repos/google-research/bert/issues/1277/comments","events_url":"https://api.github.com/repos/google-research/bert/issues/1277/events","html_url":"https://github.com/google-research/bert/issues/1277","id":1060136687,"node_id":"I_kwDOCTlCuc4_MGbv","number":1277,"title":"Error:   AutoTrackable","user":{"login":"KangChou","id":36963108,"node_id":"MDQ6VXNlcjM2OTYzMTA4","avatar_url":"https://avatars.githubusercontent.com/u/36963108?v=4","gravatar_id":"","url":"https://api.github.com/users/KangChou","html_url":"https://github.com/KangChou","followers_url":"https://api.github.com/users/KangChou/followers","following_url":"https://api.github.com/users/KangChou/following{/other_user}","gists_url":"https://api.github.com/users/KangChou/gists{/gist_id}","starred_url":"https://api.github.com/users/KangChou/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/KangChou/subscriptions","organizations_url":"https://api.github.com/users/KangChou/orgs","repos_url":"https://api.github.com/users/KangChou/repos","events_url":"https://api.github.com/users/KangChou/events{/privacy}","received_events_url":"https://api.github.com/users/KangChou/received_events","type":"User","site_admin":false},"labels":[],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":3,"created_at":"2021-11-22T13:08:31Z","updated_at":"2022-01-19T17:11:10Z","closed_at":null,"author_association":"NONE","active_lock_reason":null,"body":"tf1.x---->tf2.x\r\n\r\npython3.6\r\n\r\n```\r\n\"label_ids\": [feature.label_ids]})\r\nTypeError: 'AutoTrackable' object is not callable\r\n```","reactions":{"url":"https://api.github.com/repos/google-research/bert/issues/1277/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/google-research/bert/issues/1277/timeline","performed_via_github_app":null},{"url":"https://api.github.com/repos/google-research/bert/issues/1276","repository_url":"https://api.github.com/repos/google-research/bert","labels_url":"https://api.github.com/repos/google-research/bert/issues/1276/labels{/name}","comments_url":"https://api.github.com/repos/google-research/bert/issues/1276/comments","events_url":"https://api.github.com/repos/google-research/bert/issues/1276/events","html_url":"https://github.com/google-research/bert/issues/1276","id":1049610878,"node_id":"I_kwDOCTlCuc4-j8p-","number":1276,"title":"How to evaluate the MLM-accuracy of the pre-trained models?","user":{"login":"nikhildurgam95","id":93507020,"node_id":"U_kgDOBZLNzA","avatar_url":"https://avatars.githubusercontent.com/u/93507020?v=4","gravatar_id":"","url":"https://api.github.com/users/nikhildurgam95","html_url":"https://github.com/nikhildurgam95","followers_url":"https://api.github.com/users/nikhildurgam95/followers","following_url":"https://api.github.com/users/nikhildurgam95/following{/other_user}","gists_url":"https://api.github.com/users/nikhildurgam95/gists{/gist_id}","starred_url":"https://api.github.com/users/nikhildurgam95/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/nikhildurgam95/subscriptions","organizations_url":"https://api.github.com/users/nikhildurgam95/orgs","repos_url":"https://api.github.com/users/nikhildurgam95/repos","events_url":"https://api.github.com/users/nikhildurgam95/events{/privacy}","received_events_url":"https://api.github.com/users/nikhildurgam95/received_events","type":"User","site_admin":false},"labels":[],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":0,"created_at":"2021-11-10T09:49:22Z","updated_at":"2021-11-10T09:49:22Z","closed_at":null,"author_association":"NONE","active_lock_reason":null,"body":"Hello all,\r\n\r\nI am curious on what is the MLM-accuracy of my eval-set run on the pre-trained model that google-research provided. Specifically, the bert-large-uncased model. However, when trying to execute the `run_pretraining.py` script to evaluate the model, I encounter the following error:\r\n\r\n```\r\ntensorflow.python.framework.errors_impl.NotFoundError: Restoring from checkpoint failed. This is most likely due to a Variable name or other graph key that is missing from the checkpoint. Please ensure that you have not altered the graph expected based on the checkpoint. Original error:\r\n\r\nKey global_step not found in checkpoint\r\n         [[node save/RestoreV2 (defined at /home/.virtualenvs/ai/lib/python3.6/site-packages/tensorflow_core/python/framework/ops.py:1748) ]]\r\n```\r\n\r\nIt seems that the downloaded google-research model does not have a \"global_step\" Key and so I'm unable to load the model to predict the MLM-accuracy of it. Is there a way I can get the original model with checkpoints weights and all? Is anyone able to evaluate the MLM accuracy of the pre-trained models given by google-research? If so, please let me know how.\r\n","reactions":{"url":"https://api.github.com/repos/google-research/bert/issues/1276/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/google-research/bert/issues/1276/timeline","performed_via_github_app":null},{"url":"https://api.github.com/repos/google-research/bert/issues/1275","repository_url":"https://api.github.com/repos/google-research/bert","labels_url":"https://api.github.com/repos/google-research/bert/issues/1275/labels{/name}","comments_url":"https://api.github.com/repos/google-research/bert/issues/1275/comments","events_url":"https://api.github.com/repos/google-research/bert/issues/1275/events","html_url":"https://github.com/google-research/bert/issues/1275","id":1044261171,"node_id":"I_kwDOCTlCuc4-Pikz","number":1275,"title":"Korean translation","user":{"login":"Sojeong-Kim0915","id":70928408,"node_id":"MDQ6VXNlcjcwOTI4NDA4","avatar_url":"https://avatars.githubusercontent.com/u/70928408?v=4","gravatar_id":"","url":"https://api.github.com/users/Sojeong-Kim0915","html_url":"https://github.com/Sojeong-Kim0915","followers_url":"https://api.github.com/users/Sojeong-Kim0915/followers","following_url":"https://api.github.com/users/Sojeong-Kim0915/following{/other_user}","gists_url":"https://api.github.com/users/Sojeong-Kim0915/gists{/gist_id}","starred_url":"https://api.github.com/users/Sojeong-Kim0915/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/Sojeong-Kim0915/subscriptions","organizations_url":"https://api.github.com/users/Sojeong-Kim0915/orgs","repos_url":"https://api.github.com/users/Sojeong-Kim0915/repos","events_url":"https://api.github.com/users/Sojeong-Kim0915/events{/privacy}","received_events_url":"https://api.github.com/users/Sojeong-Kim0915/received_events","type":"User","site_admin":false},"labels":[],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":1,"created_at":"2021-11-04T01:03:17Z","updated_at":"2021-11-05T06:58:09Z","closed_at":null,"author_association":"NONE","active_lock_reason":null,"body":"Hello\r\nCan I assign <code>README.md</code> to Korean translation?","reactions":{"url":"https://api.github.com/repos/google-research/bert/issues/1275/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/google-research/bert/issues/1275/timeline","performed_via_github_app":null},{"url":"https://api.github.com/repos/google-research/bert/issues/1274","repository_url":"https://api.github.com/repos/google-research/bert","labels_url":"https://api.github.com/repos/google-research/bert/issues/1274/labels{/name}","comments_url":"https://api.github.com/repos/google-research/bert/issues/1274/comments","events_url":"https://api.github.com/repos/google-research/bert/issues/1274/events","html_url":"https://github.com/google-research/bert/issues/1274","id":1042982942,"node_id":"I_kwDOCTlCuc4-Kqge","number":1274,"title":"use tensorflow2.0","user":{"login":"AutumnLeavesCHE","id":30849971,"node_id":"MDQ6VXNlcjMwODQ5OTcx","avatar_url":"https://avatars.githubusercontent.com/u/30849971?v=4","gravatar_id":"","url":"https://api.github.com/users/AutumnLeavesCHE","html_url":"https://github.com/AutumnLeavesCHE","followers_url":"https://api.github.com/users/AutumnLeavesCHE/followers","following_url":"https://api.github.com/users/AutumnLeavesCHE/following{/other_user}","gists_url":"https://api.github.com/users/AutumnLeavesCHE/gists{/gist_id}","starred_url":"https://api.github.com/users/AutumnLeavesCHE/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/AutumnLeavesCHE/subscriptions","organizations_url":"https://api.github.com/users/AutumnLeavesCHE/orgs","repos_url":"https://api.github.com/users/AutumnLeavesCHE/repos","events_url":"https://api.github.com/users/AutumnLeavesCHE/events{/privacy}","received_events_url":"https://api.github.com/users/AutumnLeavesCHE/received_events","type":"User","site_admin":false},"labels":[],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":2,"created_at":"2021-11-03T01:08:53Z","updated_at":"2022-01-04T08:41:13Z","closed_at":null,"author_association":"NONE","active_lock_reason":null,"body":"if I use tensorflow>=2.0 ,the model that have trained  wether still useful?","reactions":{"url":"https://api.github.com/repos/google-research/bert/issues/1274/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/google-research/bert/issues/1274/timeline","performed_via_github_app":null},{"url":"https://api.github.com/repos/google-research/bert/issues/1273","repository_url":"https://api.github.com/repos/google-research/bert","labels_url":"https://api.github.com/repos/google-research/bert/issues/1273/labels{/name}","comments_url":"https://api.github.com/repos/google-research/bert/issues/1273/comments","events_url":"https://api.github.com/repos/google-research/bert/issues/1273/events","html_url":"https://github.com/google-research/bert/issues/1273","id":1039227021,"node_id":"I_kwDOCTlCuc498ViN","number":1273,"title":"My Twitter New still old charm","user":{"login":"IntelOSt","id":23098364,"node_id":"MDQ6VXNlcjIzMDk4MzY0","avatar_url":"https://avatars.githubusercontent.com/u/23098364?v=4","gravatar_id":"","url":"https://api.github.com/users/IntelOSt","html_url":"https://github.com/IntelOSt","followers_url":"https://api.github.com/users/IntelOSt/followers","following_url":"https://api.github.com/users/IntelOSt/following{/other_user}","gists_url":"https://api.github.com/users/IntelOSt/gists{/gist_id}","starred_url":"https://api.github.com/users/IntelOSt/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/IntelOSt/subscriptions","organizations_url":"https://api.github.com/users/IntelOSt/orgs","repos_url":"https://api.github.com/users/IntelOSt/repos","events_url":"https://api.github.com/users/IntelOSt/events{/privacy}","received_events_url":"https://api.github.com/users/IntelOSt/received_events","type":"User","site_admin":false},"labels":[],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":0,"created_at":"2021-10-29T06:50:32Z","updated_at":"2021-10-29T06:50:32Z","closed_at":null,"author_association":"NONE","active_lock_reason":null,"body":"https://twitter.com/William_S_Jr82?t=7Xd7A_JA1C4YJDmefR_uww&s=09","reactions":{"url":"https://api.github.com/repos/google-research/bert/issues/1273/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/google-research/bert/issues/1273/timeline","performed_via_github_app":null},{"url":"https://api.github.com/repos/google-research/bert/issues/1272","repository_url":"https://api.github.com/repos/google-research/bert","labels_url":"https://api.github.com/repos/google-research/bert/issues/1272/labels{/name}","comments_url":"https://api.github.com/repos/google-research/bert/issues/1272/comments","events_url":"https://api.github.com/repos/google-research/bert/issues/1272/events","html_url":"https://github.com/google-research/bert/issues/1272","id":1036928871,"node_id":"I_kwDOCTlCuc49zkdn","number":1272,"title":"run_pretraining.py ignores undefined flags.","user":{"login":"kyle-bong","id":42907231,"node_id":"MDQ6VXNlcjQyOTA3MjMx","avatar_url":"https://avatars.githubusercontent.com/u/42907231?v=4","gravatar_id":"","url":"https://api.github.com/users/kyle-bong","html_url":"https://github.com/kyle-bong","followers_url":"https://api.github.com/users/kyle-bong/followers","following_url":"https://api.github.com/users/kyle-bong/following{/other_user}","gists_url":"https://api.github.com/users/kyle-bong/gists{/gist_id}","starred_url":"https://api.github.com/users/kyle-bong/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/kyle-bong/subscriptions","organizations_url":"https://api.github.com/users/kyle-bong/orgs","repos_url":"https://api.github.com/users/kyle-bong/repos","events_url":"https://api.github.com/users/kyle-bong/events{/privacy}","received_events_url":"https://api.github.com/users/kyle-bong/received_events","type":"User","site_admin":false},"labels":[],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":0,"created_at":"2021-10-27T03:18:12Z","updated_at":"2021-10-27T03:18:42Z","closed_at":null,"author_association":"NONE","active_lock_reason":null,"body":"Hi, I'm trying to make bert model.\r\n\r\nI found that **`run_pretraining.py` ignores undefined flags**.\r\n\r\nfor example, If I do like this,\r\n```\r\npython3 bert/run_pretraining.py \\\r\n--input_file=gs://input_dir/*.tfrecord \\\r\n--output_dir=gs://output_dir/ \\\r\n--do_train=True \\\r\n--do_eval=True \\\r\n--bert_config_file=./bert_config.json \\\r\n--train_batch_size=1024 \\\r\n--max_seq_length=128 \\\r\n--max_predictions_per_seq=20 \\\r\n--num_train_steps=1000000 \\\r\n--num_warmup_steps=10000 \\\r\n--learning_rate=5e-5 \\\r\n--save_checkpoints_steps=10000 \\\r\n--init_checkpoints=340000 \\\r\n--use_tpu=True \\\r\n--tpu_name=tpu2 \\\r\n--tpu_zone=us-central1-f \\\r\n--fake=undifined_flags\r\n--gcp_project=my_project \\\r\n--num_tpu_cores=8\r\n```\r\n\r\n**I included an undefined arg** **`--fake=undifined_flags`**\r\nI think It should throw out an error, but It doesn't.\r\nIt train well. (Maybe) There is no problem in progress.\r\n\r\nTPU was connected normally, and ckpt was made normally too.\r\nAnd fine tuning result of ckpt is not bad. \r\n\r\nWhy it doesn't throw out error? Why it works?\r\n\r\n\r\n\r\n","reactions":{"url":"https://api.github.com/repos/google-research/bert/issues/1272/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/google-research/bert/issues/1272/timeline","performed_via_github_app":null},{"url":"https://api.github.com/repos/google-research/bert/issues/1270","repository_url":"https://api.github.com/repos/google-research/bert","labels_url":"https://api.github.com/repos/google-research/bert/issues/1270/labels{/name}","comments_url":"https://api.github.com/repos/google-research/bert/issues/1270/comments","events_url":"https://api.github.com/repos/google-research/bert/issues/1270/events","html_url":"https://github.com/google-research/bert/issues/1270","id":1032869752,"node_id":"I_kwDOCTlCuc49kFd4","number":1270,"title":"Creating_pretrainingdata is getting killed with bookcorpus dataset","user":{"login":"sairamgajavelli","id":89247834,"node_id":"MDQ6VXNlcjg5MjQ3ODM0","avatar_url":"https://avatars.githubusercontent.com/u/89247834?v=4","gravatar_id":"","url":"https://api.github.com/users/sairamgajavelli","html_url":"https://github.com/sairamgajavelli","followers_url":"https://api.github.com/users/sairamgajavelli/followers","following_url":"https://api.github.com/users/sairamgajavelli/following{/other_user}","gists_url":"https://api.github.com/users/sairamgajavelli/gists{/gist_id}","starred_url":"https://api.github.com/users/sairamgajavelli/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/sairamgajavelli/subscriptions","organizations_url":"https://api.github.com/users/sairamgajavelli/orgs","repos_url":"https://api.github.com/users/sairamgajavelli/repos","events_url":"https://api.github.com/users/sairamgajavelli/events{/privacy}","received_events_url":"https://api.github.com/users/sairamgajavelli/received_events","type":"User","site_admin":false},"labels":[],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":1,"created_at":"2021-10-21T19:32:23Z","updated_at":"2021-11-08T09:18:21Z","closed_at":null,"author_association":"NONE","active_lock_reason":null,"body":"The dataset I am using is Book Corpus having 18000 books \r\nThe system i am training on is having 64GB of RAM\r\nWhen I am trying to generate the pretraining data using create_pretraining_data.py it is getting killed in between and also only a single core is getting used.\r\nPlease give me a solution to this.","reactions":{"url":"https://api.github.com/repos/google-research/bert/issues/1270/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/google-research/bert/issues/1270/timeline","performed_via_github_app":null},{"url":"https://api.github.com/repos/google-research/bert/issues/1269","repository_url":"https://api.github.com/repos/google-research/bert","labels_url":"https://api.github.com/repos/google-research/bert/issues/1269/labels{/name}","comments_url":"https://api.github.com/repos/google-research/bert/issues/1269/comments","events_url":"https://api.github.com/repos/google-research/bert/issues/1269/events","html_url":"https://github.com/google-research/bert/issues/1269","id":1026867087,"node_id":"I_kwDOCTlCuc49NL-P","number":1269,"title":"gfile reading with tf 2.6 no longer works ","user":{"login":"aaita92","id":31774156,"node_id":"MDQ6VXNlcjMxNzc0MTU2","avatar_url":"https://avatars.githubusercontent.com/u/31774156?v=4","gravatar_id":"","url":"https://api.github.com/users/aaita92","html_url":"https://github.com/aaita92","followers_url":"https://api.github.com/users/aaita92/followers","following_url":"https://api.github.com/users/aaita92/following{/other_user}","gists_url":"https://api.github.com/users/aaita92/gists{/gist_id}","starred_url":"https://api.github.com/users/aaita92/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/aaita92/subscriptions","organizations_url":"https://api.github.com/users/aaita92/orgs","repos_url":"https://api.github.com/users/aaita92/repos","events_url":"https://api.github.com/users/aaita92/events{/privacy}","received_events_url":"https://api.github.com/users/aaita92/received_events","type":"User","site_admin":false},"labels":[],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":0,"created_at":"2021-10-14T22:13:42Z","updated_at":"2021-10-14T22:13:42Z","closed_at":null,"author_association":"NONE","active_lock_reason":null,"body":"this line: \r\n\r\nhttps://github.com/google-research/bert/blob/eedf5716ce1268e56f0a50264a88cafad334ac61/tokenization.py#L125\r\n\r\nno longer works with the tensorflow version grater then 2.6, and considering that for python versions from 3.6-3.9 are available tf versione higher then 2.5 this package seems to be obsolete.\r\n\r\nIn order to improve the code substitute the  tf.gfile.GFile(...) with tf.io.gfileGfile(...)","reactions":{"url":"https://api.github.com/repos/google-research/bert/issues/1269/reactions","total_count":1,"+1":1,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/google-research/bert/issues/1269/timeline","performed_via_github_app":null},{"url":"https://api.github.com/repos/google-research/bert/issues/1268","repository_url":"https://api.github.com/repos/google-research/bert","labels_url":"https://api.github.com/repos/google-research/bert/issues/1268/labels{/name}","comments_url":"https://api.github.com/repos/google-research/bert/issues/1268/comments","events_url":"https://api.github.com/repos/google-research/bert/issues/1268/events","html_url":"https://github.com/google-research/bert/issues/1268","id":1021583984,"node_id":"I_kwDOCTlCuc485CJw","number":1268,"title":"how to train BERT in bfloat16 mode on TPUs? ","user":{"login":"fangli80","id":9782948,"node_id":"MDQ6VXNlcjk3ODI5NDg=","avatar_url":"https://avatars.githubusercontent.com/u/9782948?v=4","gravatar_id":"","url":"https://api.github.com/users/fangli80","html_url":"https://github.com/fangli80","followers_url":"https://api.github.com/users/fangli80/followers","following_url":"https://api.github.com/users/fangli80/following{/other_user}","gists_url":"https://api.github.com/users/fangli80/gists{/gist_id}","starred_url":"https://api.github.com/users/fangli80/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/fangli80/subscriptions","organizations_url":"https://api.github.com/users/fangli80/orgs","repos_url":"https://api.github.com/users/fangli80/repos","events_url":"https://api.github.com/users/fangli80/events{/privacy}","received_events_url":"https://api.github.com/users/fangli80/received_events","type":"User","site_admin":false},"labels":[],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":0,"created_at":"2021-10-09T03:17:43Z","updated_at":"2021-10-09T03:17:43Z","closed_at":null,"author_association":"NONE","active_lock_reason":null,"body":"Hello, \r\nHas anyone tried to train a BERT model using bfloat16?\r\nThanks, \r\nLi","reactions":{"url":"https://api.github.com/repos/google-research/bert/issues/1268/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/google-research/bert/issues/1268/timeline","performed_via_github_app":null},{"url":"https://api.github.com/repos/google-research/bert/issues/1267","repository_url":"https://api.github.com/repos/google-research/bert","labels_url":"https://api.github.com/repos/google-research/bert/issues/1267/labels{/name}","comments_url":"https://api.github.com/repos/google-research/bert/issues/1267/comments","events_url":"https://api.github.com/repos/google-research/bert/issues/1267/events","html_url":"https://github.com/google-research/bert/issues/1267","id":1020027512,"node_id":"I_kwDOCTlCuc48zGJ4","number":1267,"title":"Specify language when executing run_pretraining.py","user":{"login":"LivingDeadCloud","id":22834605,"node_id":"MDQ6VXNlcjIyODM0NjA1","avatar_url":"https://avatars.githubusercontent.com/u/22834605?v=4","gravatar_id":"","url":"https://api.github.com/users/LivingDeadCloud","html_url":"https://github.com/LivingDeadCloud","followers_url":"https://api.github.com/users/LivingDeadCloud/followers","following_url":"https://api.github.com/users/LivingDeadCloud/following{/other_user}","gists_url":"https://api.github.com/users/LivingDeadCloud/gists{/gist_id}","starred_url":"https://api.github.com/users/LivingDeadCloud/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/LivingDeadCloud/subscriptions","organizations_url":"https://api.github.com/users/LivingDeadCloud/orgs","repos_url":"https://api.github.com/users/LivingDeadCloud/repos","events_url":"https://api.github.com/users/LivingDeadCloud/events{/privacy}","received_events_url":"https://api.github.com/users/LivingDeadCloud/received_events","type":"User","site_admin":false},"labels":[],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":0,"created_at":"2021-10-07T13:10:18Z","updated_at":"2021-10-07T13:10:18Z","closed_at":null,"author_association":"NONE","active_lock_reason":null,"body":"I'm doing some preliminary test to pre-train a BERT model for italian language.\r\n\r\nMaybe it's a trivial question, but how can I specify the language when executing `run_pretraining.py`?\r\nAs initial checkpoint I am using the model `BERT-Base, Multilingual Cased`. All the other parameters are the ones specified [here](https://github.com/google-research/bert#pre-training-with-bert)","reactions":{"url":"https://api.github.com/repos/google-research/bert/issues/1267/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/google-research/bert/issues/1267/timeline","performed_via_github_app":null},{"url":"https://api.github.com/repos/google-research/bert/issues/1266","repository_url":"https://api.github.com/repos/google-research/bert","labels_url":"https://api.github.com/repos/google-research/bert/issues/1266/labels{/name}","comments_url":"https://api.github.com/repos/google-research/bert/issues/1266/comments","events_url":"https://api.github.com/repos/google-research/bert/issues/1266/events","html_url":"https://github.com/google-research/bert/issues/1266","id":1016403391,"node_id":"I_kwDOCTlCuc48lRW_","number":1266,"title":"Can BERT recognize if a sentence fragment follows from a preceding one?","user":{"login":"julkhami","id":90245630,"node_id":"MDQ6VXNlcjkwMjQ1NjMw","avatar_url":"https://avatars.githubusercontent.com/u/90245630?v=4","gravatar_id":"","url":"https://api.github.com/users/julkhami","html_url":"https://github.com/julkhami","followers_url":"https://api.github.com/users/julkhami/followers","following_url":"https://api.github.com/users/julkhami/following{/other_user}","gists_url":"https://api.github.com/users/julkhami/gists{/gist_id}","starred_url":"https://api.github.com/users/julkhami/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/julkhami/subscriptions","organizations_url":"https://api.github.com/users/julkhami/orgs","repos_url":"https://api.github.com/users/julkhami/repos","events_url":"https://api.github.com/users/julkhami/events{/privacy}","received_events_url":"https://api.github.com/users/julkhami/received_events","type":"User","site_admin":false},"labels":[],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":0,"created_at":"2021-10-05T14:30:48Z","updated_at":"2021-10-05T14:30:48Z","closed_at":null,"author_association":"NONE","active_lock_reason":null,"body":"A few introductory questions about BERT:\r\n\r\nIs it like GPT-3 where you must query it in natural language or does BERT have some kind of built-in methods?\r\n\r\nIs BERT an algorithm that can be trained on any data or is it a pre-trained model?\r\n\r\nMost importantly, I read that BERT can assess with high accuracy if a sentence follows from a previous one. Could it do the same with sentence fragments, i.e. if a text is broken up into lines, could it detect if line x+1 is a continuation of the sentence begun in line x, vs. being a separate entity of some kind, like a chapter title following by an author name?\r\n\r\nThank you very much.","reactions":{"url":"https://api.github.com/repos/google-research/bert/issues/1266/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/google-research/bert/issues/1266/timeline","performed_via_github_app":null},{"url":"https://api.github.com/repos/google-research/bert/issues/1264","repository_url":"https://api.github.com/repos/google-research/bert","labels_url":"https://api.github.com/repos/google-research/bert/issues/1264/labels{/name}","comments_url":"https://api.github.com/repos/google-research/bert/issues/1264/comments","events_url":"https://api.github.com/repos/google-research/bert/issues/1264/events","html_url":"https://github.com/google-research/bert/issues/1264","id":996396259,"node_id":"I_kwDOCTlCuc47Y8zj","number":1264,"title":"You missed Ro","user":{"login":"Mkfish","id":10230994,"node_id":"MDQ6VXNlcjEwMjMwOTk0","avatar_url":"https://avatars.githubusercontent.com/u/10230994?v=4","gravatar_id":"","url":"https://api.github.com/users/Mkfish","html_url":"https://github.com/Mkfish","followers_url":"https://api.github.com/users/Mkfish/followers","following_url":"https://api.github.com/users/Mkfish/following{/other_user}","gists_url":"https://api.github.com/users/Mkfish/gists{/gist_id}","starred_url":"https://api.github.com/users/Mkfish/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/Mkfish/subscriptions","organizations_url":"https://api.github.com/users/Mkfish/orgs","repos_url":"https://api.github.com/users/Mkfish/repos","events_url":"https://api.github.com/users/Mkfish/events{/privacy}","received_events_url":"https://api.github.com/users/Mkfish/received_events","type":"User","site_admin":false},"labels":[],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":0,"created_at":"2021-09-14T20:16:23Z","updated_at":"2021-09-14T20:16:23Z","closed_at":null,"author_association":"NONE","active_lock_reason":null,"body":"Love you all ","reactions":{"url":"https://api.github.com/repos/google-research/bert/issues/1264/reactions","total_count":3,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":3,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/google-research/bert/issues/1264/timeline","performed_via_github_app":null},{"url":"https://api.github.com/repos/google-research/bert/issues/1259","repository_url":"https://api.github.com/repos/google-research/bert","labels_url":"https://api.github.com/repos/google-research/bert/issues/1259/labels{/name}","comments_url":"https://api.github.com/repos/google-research/bert/issues/1259/comments","events_url":"https://api.github.com/repos/google-research/bert/issues/1259/events","html_url":"https://github.com/google-research/bert/issues/1259","id":981244217,"node_id":"MDU6SXNzdWU5ODEyNDQyMTc=","number":1259,"title":"BasicTokenizer process question(tokenization.py)","user":{"login":"herosunly","id":24822076,"node_id":"MDQ6VXNlcjI0ODIyMDc2","avatar_url":"https://avatars.githubusercontent.com/u/24822076?v=4","gravatar_id":"","url":"https://api.github.com/users/herosunly","html_url":"https://github.com/herosunly","followers_url":"https://api.github.com/users/herosunly/followers","following_url":"https://api.github.com/users/herosunly/following{/other_user}","gists_url":"https://api.github.com/users/herosunly/gists{/gist_id}","starred_url":"https://api.github.com/users/herosunly/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/herosunly/subscriptions","organizations_url":"https://api.github.com/users/herosunly/orgs","repos_url":"https://api.github.com/users/herosunly/repos","events_url":"https://api.github.com/users/herosunly/events{/privacy}","received_events_url":"https://api.github.com/users/herosunly/received_events","type":"User","site_admin":false},"labels":[],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":0,"created_at":"2021-08-27T13:19:26Z","updated_at":"2021-08-27T13:19:26Z","closed_at":null,"author_association":"NONE","active_lock_reason":null,"body":"The class method of  tokenize in class BasicTokenizer, why use below code\r\n`output_tokens = whitespace_tokenize(\" \".join(split_tokens))\r\n`\r\nIn my test code, split_tokens is equal to output_tokens. So I think it's possible to remove the line code, but I am not sure.I am looking forward to your reply.","reactions":{"url":"https://api.github.com/repos/google-research/bert/issues/1259/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/google-research/bert/issues/1259/timeline","performed_via_github_app":null},{"url":"https://api.github.com/repos/google-research/bert/issues/1258","repository_url":"https://api.github.com/repos/google-research/bert","labels_url":"https://api.github.com/repos/google-research/bert/issues/1258/labels{/name}","comments_url":"https://api.github.com/repos/google-research/bert/issues/1258/comments","events_url":"https://api.github.com/repos/google-research/bert/issues/1258/events","html_url":"https://github.com/google-research/bert/issues/1258","id":979729436,"node_id":"MDU6SXNzdWU5Nzk3Mjk0MzY=","number":1258,"title":"Why just show global_step/sec&examples/sec by pretrain?","user":{"login":"franklee24","id":18497603,"node_id":"MDQ6VXNlcjE4NDk3NjAz","avatar_url":"https://avatars.githubusercontent.com/u/18497603?v=4","gravatar_id":"","url":"https://api.github.com/users/franklee24","html_url":"https://github.com/franklee24","followers_url":"https://api.github.com/users/franklee24/followers","following_url":"https://api.github.com/users/franklee24/following{/other_user}","gists_url":"https://api.github.com/users/franklee24/gists{/gist_id}","starred_url":"https://api.github.com/users/franklee24/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/franklee24/subscriptions","organizations_url":"https://api.github.com/users/franklee24/orgs","repos_url":"https://api.github.com/users/franklee24/repos","events_url":"https://api.github.com/users/franklee24/events{/privacy}","received_events_url":"https://api.github.com/users/franklee24/received_events","type":"User","site_admin":false},"labels":[],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":0,"created_at":"2021-08-26T01:01:08Z","updated_at":"2021-08-26T01:01:08Z","closed_at":null,"author_association":"NONE","active_lock_reason":null,"body":"\r\n\r\nI used google colab to pretrain my own data by BERT,but it confuse me that it just show global_step/sec&examples/sec,no loss or others.How can i change my code to fix it?``\r\n`W0825 13:34:11.809314 140652181628800 basic_session_run_hooks.py:724] It seems that global step (tf.train.get_global_step) has not been increased. Current value (could be stable): 12 vs previous value: 12. You could increase the global step by passing tf.train.get_global_step() to Optimizer.apply_gradients or Optimizer.minimize\r\n\r\nI0826 00:45:28.850609 140375389116288 session_manager.py:500] Running local_init_op.\r\nINFO:tensorflow:Done running local_init_op.\r\nI0826 00:45:28.996295 140375389116288 session_manager.py:502] Done running local_init_op.\r\nINFO:tensorflow:Saving checkpoints for 0 into /content/drive/MyDrive/colab/bertmaster/bert-master/tmp/pretraining_output/model.ckpt.\r\nI0826 00:45:36.375832 140375389116288 basic_session_run_hooks.py:606] Saving checkpoints for 0 into /content/drive/MyDrive/colab/bertmaster/bert-master/tmp/pretraining_output/model.ckpt.\r\n2021-08-26 00:45:49.021796: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcublas.so.10.0\r\nINFO:tensorflow:global_step/sec: 0.129691\r\nI0826 00:45:58.006054 140375389116288 tpu_estimator.py:2159] global_step/sec: 0.129691\r\nINFO:tensorflow:examples/sec: 4.15011\r\nI0826 00:45:58.006821 140375389116288 tpu_estimator.py:2160] examples/sec: 4.15011\r\nINFO:tensorflow:global_step/sec: 2.06314\r\nI0826 00:45:58.490685 140375389116288 tpu_estimator.py:2159] global_step/sec: 2.06314\r\nINFO:tensorflow:examples/sec: 66.0206\r\nI0826 00:45:58.491152 140375389116288 tpu_estimator.py:2160] examples/sec: 66.0206\r\nINFO:tensorflow:global_step/sec: 2.06575\r\nI0826 00:45:58.974765 140375389116288 tpu_estimator.py:2159] global_step/sec: 2.06575\r\nINFO:tensorflow:examples/sec: 66.1041\r\nI0826 00:45:58.975437 140375389116288 tpu_estimator.py:2160] examples/sec: 66.1041\r\nINFO:tensorflow:global_step/sec: 2.0621\r\nI0826 00:45:59.459806 140375389116288 tpu_estimator.py:2159] global_step/sec: 2.0621\r\nINFO:tensorflow:examples/sec: 65.9873\r\nI0826 00:45:59.460407 140375389116288 tpu_estimator.py:2160] examples/sec: 65.9873\r\nINFO:tensorflow:global_step/sec: 2.06622\r\nI0826 00:45:59.943793 140375389116288 tpu_estimator.py:2159] global_step/sec: 2.06622\r\nINFO:tensorflow:examples/sec: 66.1189\r\nI0826 00:45:59.944195 140375389116288 tpu_estimator.py:2160] examples/sec: 66.1189\r\nINFO:tensorflow:global_step/sec: 2.06073\r\nI0826 00:46:00.429032 140375389116288 tpu_estimator.py:2159] global_step/sec: 2.06073\r\nINFO:tensorflow:examples/sec: 65.9434\r\nI0826 00:46:00.429662 140375389116288 tpu_estimator.py:2160] examples/sec: 65.9434\r\nINFO:tensorflow:global_step/sec: 2.06349\r\nI0826 00:46:00.913643 140375389116288 tpu_estimator.py:2159] global_step/sec: 2.06349\r\nINFO:tensorflow:examples/sec: 66.0316\r\nI0826 00:46:00.913931 140375389116288 tpu_estimator.py:2160] examples/sec: 66.0316\r\nINFO:tensorflow:global_step/sec: 2.06284\r\nI0826 00:46:01.398393 140375389116288 tpu_estimator.py:2159] global_step/sec: 2.06284\r\nINFO:tensorflow:examples/sec: 66.011\r\nI0826 00:46:01.398838 140375389116288 tpu_estimator.py:2160] examples/sec: 66.011\r\n`\r\n<img width=\"709\" alt=\"0d67ceca1a33ab993200f4bc4f3399b\" src=\"https://user-images.githubusercontent.com/18497603/130883373-e6766abe-8fb4-4eb3-8f3d-75ddfb7debb4.png\">\r\n","reactions":{"url":"https://api.github.com/repos/google-research/bert/issues/1258/reactions","total_count":1,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":1},"timeline_url":"https://api.github.com/repos/google-research/bert/issues/1258/timeline","performed_via_github_app":null}]